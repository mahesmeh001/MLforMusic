{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365d89d443f65455",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TASK 1\n",
    "About .7 perc acc w/ MLP based model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.735352Z",
     "start_time": "2025-05-20T19:34:44.521648Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a81a8214c46e4e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.742536Z",
     "start_time": "2025-05-20T19:34:45.736886Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy1(groundtruth, predictions):\n",
    "    correct = 0\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        if predictions[k] == groundtruth[k]:\n",
    "            correct += 1\n",
    "    return correct / len(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f7725253a006ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.743180Z",
     "start_time": "2025-05-20T19:34:45.738839Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataroot1 = \"data/student_files/task1_composer_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75259b79043412a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.745921Z",
     "start_time": "2025-05-20T19:34:45.744138Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class model1():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def features(self, path):\n",
    "        midi_obj = miditoolkit.midi.parser.MidiFile(dataroot1 + '/' + path)\n",
    "        notes = midi_obj.instruments[0].notes\n",
    "        num_notes = len(notes)\n",
    "        average_pitch = sum([note.pitch for note in notes]) / num_notes\n",
    "        average_duration = sum([note.end - note.start for note in notes]) / num_notes\n",
    "        features = [average_pitch, average_duration]\n",
    "        return features\n",
    "    \n",
    "    def predict(self, path, outpath=None):\n",
    "        d = eval(open(path, 'r').read())\n",
    "        predictions = {}\n",
    "        for k in d:\n",
    "            x = self.features(k)\n",
    "            pred = self.model.predict([x])\n",
    "            predictions[k] = str(pred[0])\n",
    "        if outpath:\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        return predictions\n",
    "\n",
    "    # Train your model. Note that this function will not be called from the autograder:\n",
    "    # instead you should upload your saved model using save()\n",
    "    def train(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            train_json = eval(f.read())\n",
    "        X_train = [self.features(k) for k in train_json]\n",
    "        y_train = [train_json[k] for k in train_json]\n",
    "        \n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37995c2bbe95759b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.753745Z",
     "start_time": "2025-05-20T19:34:45.746784Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run1():\n",
    "    model = model1()\n",
    "    model.train(dataroot1 + \"/train.json\")\n",
    "    train_preds = model.predict(dataroot1 + \"/train.json\")\n",
    "    test_preds = model.predict(dataroot1 + \"/test.json\", \"predictions1.json\")\n",
    "    \n",
    "    train_labels = eval(open(dataroot1 + \"/train.json\").read())\n",
    "    acc1 = accuracy1(train_labels, train_preds)\n",
    "    print(\"Task 1 training accuracy = \" + str(acc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ae1a2b10f10d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Custom Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb051a850ec9ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.763076Z",
     "start_time": "2025-05-20T19:34:45.749307Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def create_artist_mapping(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        midi_to_artist = ast.literal_eval(f.read())\n",
    "        \n",
    "\n",
    "    unique_artists = sorted(set(midi_to_artist.values()))\n",
    "    id_to_artist = {i: artist for i, artist in enumerate(unique_artists)}\n",
    "\n",
    "    \n",
    "    return id_to_artist\n",
    "idToArtist = create_artist_mapping(\"data/student_files/task1_composer_classification/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8021ccd014c5168b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.764033Z",
     "start_time": "2025-05-20T19:34:45.757357Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "{0: 'Bach', 1: 'Beethoven', 2: 'Chopin', 3: 'Haydn', 4: 'Liszt', 5: 'Mozart', 6: 'Schubert', 7: 'Schumann'}\n",
      "{'Bach': 0, 'Beethoven': 1, 'Chopin': 2, 'Haydn': 3, 'Liszt': 4, 'Mozart': 5, 'Schubert': 6, 'Schumann': 7}\n"
     ]
    }
   ],
   "source": [
    "print(len(idToArtist))\n",
    "print(idToArtist)\n",
    "\n",
    "artistToId={}\n",
    "for key,value in idToArtist.items():\n",
    "    artistToId[value] = key\n",
    "print(artistToId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f51285f1e304fee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:34:45.811552Z",
     "start_time": "2025-05-20T19:34:45.763494Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model, filepath='sol_1.pt'):\n",
    "    \"\"\"Save a PyTorch model to a file\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model_class, filepath='sol_1.pt', *args, **kwargs):\n",
    "    \"\"\"Load a PyTorch model from a file\"\"\"\n",
    "    model = model_class(*args, **kwargs)  # instantiate the model\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # optional: sets dropout/batchnorm to eval mode\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c4c5c466e04515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:35:38.407880Z",
     "start_time": "2025-05-20T19:34:45.774401Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-255.9119,  140.4222,  -41.8253,  ...,   89.0000,   39.0000,\n",
      "          62.8079])\n",
      "tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9272/2094906300.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n"
     ]
    }
   ],
   "source": [
    "from mido import MidiFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import islice\n",
    "import fluidsynth\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# create train loader \n",
    "\n",
    "\n",
    "def get_lowest_pitch(file_path):\n",
    "    # Initialize lowest_note to a high value (since MIDI notes are from 0 to 127)\n",
    "    lowest_note = 128  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note < lowest_note:\n",
    "                    lowest_note = msg.note\n",
    "    \n",
    "    # Return None if no note is found\n",
    "    return lowest_note if lowest_note != 128 else None\n",
    "\n",
    "def get_highest_pitch(file_path):\n",
    "    # Initialize highest_note to a low value (since MIDI notes are from 0 to 127)\n",
    "    highest_note = -1  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note > highest_note:\n",
    "                    highest_note = msg.note\n",
    "                    \n",
    "    # Return None if no note is found\n",
    "    return highest_note if highest_note != -1 else None\n",
    "\n",
    "def get_unique_pitch_num(file_path):\n",
    "    mid = MidiFile(file_path)\n",
    "    notes = set()\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.add(msg.note)\n",
    "    \n",
    "    return len(notes)\n",
    "\n",
    "def get_average_pitch_value(file_path):\n",
    "    #Q8: Your code goes here\n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    notes = []\n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.append(msg.note)\n",
    "    \n",
    "    if notes:\n",
    "        return sum(notes) / len(notes)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_waveform(path):\n",
    "    # Your code here\n",
    "    wave, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    return wave \n",
    "\n",
    "def extract_q(w):\n",
    "    # Your code here\n",
    "    result = librosa.cqt(y=w, sr=SAMPLE_RATE)\n",
    "    result = librosa.amplitude_to_db(np.abs(result))\n",
    "    q =torch.FloatTensor(result)\n",
    "    \n",
    "    mean = q.mean(dim=1)  # shape: (84,)\n",
    "    std = q.std(dim=1)    # shape: (84,)\n",
    "    return torch.cat([mean, std], dim=0)  # shape: (168,)\n",
    "\n",
    "def extract_mfcc(w):\n",
    "    # Your code here:\n",
    "    # load using librosa.feature.mfcc\n",
    "    mfcc = librosa.feature.mfcc(y=w, sr=SAMPLE_RATE, n_mfcc = 64)\n",
    "    # extract mean and \n",
    "    means = np.mean(mfcc, axis=1)\n",
    "    \n",
    "    stds = np.std(mfcc, axis=1)\n",
    "    # concatenate\n",
    "    features = np.concatenate([means, stds])\n",
    "    \n",
    "    return torch.FloatTensor(features)\n",
    "\n",
    "def extract_spec(w):\n",
    "    # Your code here\n",
    "    # load\n",
    "    stft = librosa.stft(y=w)\n",
    "    # take squared absolute values\n",
    "    spec = np.abs(stft) ** 2\n",
    "    \n",
    "    spec = torch.FloatTensor(spec)\n",
    "    \n",
    "    mean = spec.mean(dim=1)  # shape (128,)\n",
    "    std = spec.std(dim=1)    # shape (128,)\n",
    "    return torch.cat([mean, std], dim=0)  # shape (256,)\n",
    "\n",
    "\n",
    "import torch\n",
    "import pretty_midi\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def features(path):\n",
    "    full_path = dataroot1 + '/' + path\n",
    "    try:\n",
    "        midi_obj = pretty_midi.PrettyMIDI(full_path)\n",
    "        w = midi_obj.fluidsynth(fs=SAMPLE_RATE)\n",
    "\n",
    "        if w is None or len(w) < SAMPLE_RATE // 10:  # e.g. less than 0.1s\n",
    "            raise ValueError(\"Waveform too short or empty\")\n",
    "\n",
    "        mfcc = extract_mfcc(w)\n",
    "        spec = extract_spec(w)\n",
    "        q = extract_q(w)\n",
    "\n",
    "        extra = torch.tensor([\n",
    "            get_lowest_pitch(full_path) or 0,\n",
    "            get_highest_pitch(full_path) or 0,\n",
    "            get_unique_pitch_num(full_path),\n",
    "            get_average_pitch_value(full_path) or 0\n",
    "        ], dtype=torch.float64)\n",
    "        \n",
    "        features = torch.cat(\n",
    "            [\n",
    "                mfcc, \n",
    "                spec, \n",
    "                q, \n",
    "                extra\n",
    "            ]) \n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {full_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def create_train_features(size=None, val_split=0.1, n_jobs=4):\n",
    "    # Load data\n",
    "    with open(dataroot1 + \"/train.json\", 'r') as f:\n",
    "        train_json = eval(f.read())\n",
    "    \n",
    "    # Limit size if specified\n",
    "    if size is not None:\n",
    "        train_json = dict(list(train_json.items())[:size])\n",
    "    \n",
    "    # Parallel feature extraction\n",
    "    keys = list(train_json.keys())\n",
    "    values = list(train_json.values())\n",
    "\n",
    "    X = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(features)(key) for key in keys\n",
    "    )\n",
    "    Y = [artistToId[value] for value in values]\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
    "    Y = torch.tensor(Y, dtype=torch.int64)\n",
    "    \n",
    "    # Return all data if no validation split needed\n",
    "    if val_split <= 0:\n",
    "        return X, Y\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X, Y, test_size=val_split, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "\n",
    "    \n",
    "X_train, y_train, X_val, y_val = create_train_features()\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd6b0de3dcdd3be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:35:38.411195Z",
     "start_time": "2025-05-20T19:35:38.405708Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# midi_obj = pretty_midi.PrettyMIDI(\"data/student_files/task1_composer_classification/midis/0.mid\")\n",
    "# w = midi_obj.fluidsynth()  # This converts MIDI to waveform\n",
    "# print(w.shape)\n",
    "# extract_q(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e33a0cb603977009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:35:38.414075Z",
     "start_time": "2025-05-20T19:35:38.407083Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset splits to: task1_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace with your actual file path\n",
    "save_path = \"task1_data.pkl\"\n",
    "\n",
    "data = {\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_val\": X_val,\n",
    "    \"y_val\": y_val\n",
    "}\n",
    "\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(f\"Saved dataset splits to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d34bd676d4b4bef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:35:38.415719Z",
     "start_time": "2025-05-20T19:35:38.413625Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]))\n",
    "feature_size = (len(X_train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27edf1843d4b8595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:37:44.848373Z",
     "start_time": "2025-05-20T19:37:44.836859Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53207266d5e6acff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:37:50.519260Z",
     "start_time": "2025-05-20T19:37:50.513861Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=feature_size, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.__init_args__ = (input_dim,)\n",
    "        self.__init_kwargs__ = {'num_classes': num_classes}\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Feature processing network\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for feature weighting\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Linear(512, 256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process features\n",
    "        features = self.feature_net(x)\n",
    "        \n",
    "        # Apply attention (optional branch)\n",
    "        attention_weights = torch.sigmoid(self.attention(features))\n",
    "        weighted_features = features * attention_weights\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(weighted_features)\n",
    "        return logits\n",
    "        \n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Return intermediate feature representation\"\"\"\n",
    "        return self.feature_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5df887fd03f94ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:40:23.486328Z",
     "start_time": "2025-05-20T19:40:23.468935Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class model2():\n",
    "    def __init__(self):\n",
    "        self.model = MLPClassifier()\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        return\n",
    "    \n",
    "    def predict(self, path, outpath=None):\n",
    "        d = eval(open(path, 'r').read())\n",
    "        predictions = {}\n",
    "    \n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "    \n",
    "        for k in d:\n",
    "            x = torch.tensor(features(k), dtype=torch.float32)\n",
    "            x = x.unsqueeze(0).to(device)  # (1, feature_dim) or (1, C, H, W) depending on model\n",
    "            with torch.no_grad():\n",
    "                output = self.model(x)\n",
    "                _, pred = torch.max(output, 1)\n",
    "                predictions[k] = str(idToArtist[pred.item()])  # get scalar int from tensor\n",
    "    \n",
    "        if outpath:\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "    \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    # Train your model. Note that this function will not be called from the autograder:\n",
    "    # instead you should upload your saved model using save()\n",
    "    import torch\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=5):\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "        model = self.model\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()  \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.9, patience=5)\n",
    "        \n",
    "    \n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        \n",
    "        # Early stopping params\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        patience= 10\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += batch_y.size(0)\n",
    "                correct_train += (predicted == batch_y).sum().item()\n",
    "    \n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_acc.append(train_accuracy)\n",
    "    \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    batch_y = batch_y.to(device)\n",
    "                    outputs = model(batch_x)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += batch_y.size(0)\n",
    "                    correct_val += (predicted == batch_y).sum().item()\n",
    "    \n",
    "            val_accuracy = correct_val / total_val\n",
    "            val_acc.append(val_accuracy)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "            # Step the LR scheduler\n",
    "            scheduler.step(val_accuracy)\n",
    "    \n",
    "            # Early Stopping\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict()  # Save best model\n",
    "                save_model(model, 'sol1_MLP_2.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "    \n",
    "        # Load best model state\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "        self.train_acc = train_acc\n",
    "        self.val_acc = val_acc\n",
    "        self.model = model\n",
    "                \n",
    "    def get_train_acc(self):\n",
    "        return self.train_acc, self.val_acc\n",
    "    \n",
    "    def _get_model_copy(self, model):\n",
    "        \"\"\"Create a deep copy of the model.\"\"\"\n",
    "        model_copy = type(model)(*model.__init_args__, **model.__init_kwargs__)\n",
    "        model_copy.load_state_dict(model.state_dict())\n",
    "        return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c179b4e57e8ca203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:40:47.653437Z",
     "start_time": "2025-05-20T19:40:23.641397Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Train Accuracy: 0.4182, Validation Accuracy: 0.5104\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [2/300], Train Accuracy: 0.5368, Validation Accuracy: 0.5833\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [3/300], Train Accuracy: 0.5754, Validation Accuracy: 0.6042\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [4/300], Train Accuracy: 0.5708, Validation Accuracy: 0.5312\n",
      "Epoch [5/300], Train Accuracy: 0.6029, Validation Accuracy: 0.6250\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [6/300], Train Accuracy: 0.6186, Validation Accuracy: 0.5729\n",
      "Epoch [7/300], Train Accuracy: 0.6278, Validation Accuracy: 0.5729\n",
      "Epoch [8/300], Train Accuracy: 0.6305, Validation Accuracy: 0.6042\n",
      "Epoch [9/300], Train Accuracy: 0.6443, Validation Accuracy: 0.6146\n",
      "Epoch [10/300], Train Accuracy: 0.6167, Validation Accuracy: 0.5625\n",
      "Epoch [11/300], Train Accuracy: 0.6599, Validation Accuracy: 0.6042\n",
      "Epoch [12/300], Train Accuracy: 0.6452, Validation Accuracy: 0.5312\n",
      "Epoch [13/300], Train Accuracy: 0.6544, Validation Accuracy: 0.6979\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [14/300], Train Accuracy: 0.6691, Validation Accuracy: 0.5208\n",
      "Epoch [15/300], Train Accuracy: 0.6857, Validation Accuracy: 0.5938\n",
      "Epoch [16/300], Train Accuracy: 0.6719, Validation Accuracy: 0.5729\n",
      "Epoch [17/300], Train Accuracy: 0.6728, Validation Accuracy: 0.6146\n",
      "Epoch [18/300], Train Accuracy: 0.7059, Validation Accuracy: 0.6146\n",
      "Epoch [19/300], Train Accuracy: 0.7151, Validation Accuracy: 0.6667\n",
      "Epoch [20/300], Train Accuracy: 0.7013, Validation Accuracy: 0.5625\n",
      "Epoch [21/300], Train Accuracy: 0.7142, Validation Accuracy: 0.6354\n",
      "Epoch [22/300], Train Accuracy: 0.7316, Validation Accuracy: 0.6250\n",
      "Epoch [23/300], Train Accuracy: 0.6939, Validation Accuracy: 0.6562\n",
      "Epoch [24/300], Train Accuracy: 0.7077, Validation Accuracy: 0.6979\n",
      "Epoch [25/300], Train Accuracy: 0.7160, Validation Accuracy: 0.6458\n",
      "Epoch [26/300], Train Accuracy: 0.7325, Validation Accuracy: 0.6458\n",
      "Epoch [27/300], Train Accuracy: 0.7509, Validation Accuracy: 0.6562\n",
      "Epoch [28/300], Train Accuracy: 0.7289, Validation Accuracy: 0.6667\n",
      "Epoch [29/300], Train Accuracy: 0.7399, Validation Accuracy: 0.5729\n",
      "Epoch [30/300], Train Accuracy: 0.7629, Validation Accuracy: 0.6771\n",
      "Epoch [31/300], Train Accuracy: 0.7555, Validation Accuracy: 0.5625\n",
      "Epoch [32/300], Train Accuracy: 0.7629, Validation Accuracy: 0.6562\n",
      "Epoch [33/300], Train Accuracy: 0.7665, Validation Accuracy: 0.6458\n",
      "Epoch [34/300], Train Accuracy: 0.7555, Validation Accuracy: 0.6354\n",
      "Epoch [35/300], Train Accuracy: 0.7629, Validation Accuracy: 0.6562\n",
      "Epoch [36/300], Train Accuracy: 0.7721, Validation Accuracy: 0.6771\n",
      "Epoch [37/300], Train Accuracy: 0.7914, Validation Accuracy: 0.6562\n",
      "Epoch [38/300], Train Accuracy: 0.7767, Validation Accuracy: 0.6458\n",
      "Epoch [39/300], Train Accuracy: 0.7895, Validation Accuracy: 0.6354\n",
      "Epoch [40/300], Train Accuracy: 0.7858, Validation Accuracy: 0.6458\n",
      "Epoch [41/300], Train Accuracy: 0.8061, Validation Accuracy: 0.7292\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [42/300], Train Accuracy: 0.8107, Validation Accuracy: 0.6979\n",
      "Epoch [43/300], Train Accuracy: 0.7858, Validation Accuracy: 0.6667\n",
      "Epoch [44/300], Train Accuracy: 0.7941, Validation Accuracy: 0.6771\n",
      "Epoch [45/300], Train Accuracy: 0.7950, Validation Accuracy: 0.6562\n",
      "Epoch [46/300], Train Accuracy: 0.8051, Validation Accuracy: 0.6458\n",
      "Epoch [47/300], Train Accuracy: 0.7923, Validation Accuracy: 0.6875\n",
      "Epoch [48/300], Train Accuracy: 0.8116, Validation Accuracy: 0.6146\n",
      "Epoch [49/300], Train Accuracy: 0.8226, Validation Accuracy: 0.6979\n",
      "Epoch [50/300], Train Accuracy: 0.8061, Validation Accuracy: 0.6771\n",
      "Epoch [51/300], Train Accuracy: 0.8244, Validation Accuracy: 0.6667\n",
      "Epoch [52/300], Train Accuracy: 0.8217, Validation Accuracy: 0.6146\n",
      "Epoch [53/300], Train Accuracy: 0.8355, Validation Accuracy: 0.7292\n",
      "Epoch [54/300], Train Accuracy: 0.8208, Validation Accuracy: 0.6875\n",
      "Epoch [55/300], Train Accuracy: 0.8290, Validation Accuracy: 0.6250\n",
      "Epoch [56/300], Train Accuracy: 0.8318, Validation Accuracy: 0.7083\n",
      "Epoch [57/300], Train Accuracy: 0.8382, Validation Accuracy: 0.6458\n",
      "Epoch [58/300], Train Accuracy: 0.8456, Validation Accuracy: 0.6771\n",
      "Epoch [59/300], Train Accuracy: 0.8309, Validation Accuracy: 0.6458\n",
      "Epoch [60/300], Train Accuracy: 0.8300, Validation Accuracy: 0.6771\n",
      "Epoch [61/300], Train Accuracy: 0.8438, Validation Accuracy: 0.6875\n",
      "Epoch [62/300], Train Accuracy: 0.8346, Validation Accuracy: 0.6875\n",
      "Epoch [63/300], Train Accuracy: 0.8419, Validation Accuracy: 0.6771\n",
      "Epoch [64/300], Train Accuracy: 0.8447, Validation Accuracy: 0.6979\n",
      "Epoch [65/300], Train Accuracy: 0.8318, Validation Accuracy: 0.6875\n",
      "Epoch [66/300], Train Accuracy: 0.8373, Validation Accuracy: 0.6562\n",
      "Epoch [67/300], Train Accuracy: 0.8520, Validation Accuracy: 0.7083\n",
      "Epoch [68/300], Train Accuracy: 0.8483, Validation Accuracy: 0.7292\n",
      "Epoch [69/300], Train Accuracy: 0.8502, Validation Accuracy: 0.6562\n",
      "Epoch [70/300], Train Accuracy: 0.8465, Validation Accuracy: 0.6771\n",
      "Epoch [71/300], Train Accuracy: 0.8667, Validation Accuracy: 0.6979\n",
      "Epoch [72/300], Train Accuracy: 0.8667, Validation Accuracy: 0.6771\n",
      "Epoch [73/300], Train Accuracy: 0.8695, Validation Accuracy: 0.6667\n",
      "Epoch [74/300], Train Accuracy: 0.8686, Validation Accuracy: 0.6667\n",
      "Epoch [75/300], Train Accuracy: 0.8575, Validation Accuracy: 0.6979\n",
      "Epoch [76/300], Train Accuracy: 0.8768, Validation Accuracy: 0.6771\n",
      "Epoch [77/300], Train Accuracy: 0.8741, Validation Accuracy: 0.6354\n",
      "Epoch [78/300], Train Accuracy: 0.8603, Validation Accuracy: 0.6875\n",
      "Epoch [79/300], Train Accuracy: 0.8511, Validation Accuracy: 0.7500\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [80/300], Train Accuracy: 0.8658, Validation Accuracy: 0.7083\n",
      "Epoch [81/300], Train Accuracy: 0.8759, Validation Accuracy: 0.7083\n",
      "Epoch [82/300], Train Accuracy: 0.8768, Validation Accuracy: 0.6771\n",
      "Epoch [83/300], Train Accuracy: 0.8621, Validation Accuracy: 0.7292\n",
      "Epoch [84/300], Train Accuracy: 0.8658, Validation Accuracy: 0.6771\n",
      "Epoch [85/300], Train Accuracy: 0.8759, Validation Accuracy: 0.6667\n",
      "Epoch [86/300], Train Accuracy: 0.8704, Validation Accuracy: 0.6667\n",
      "Epoch [87/300], Train Accuracy: 0.8879, Validation Accuracy: 0.6979\n",
      "Epoch [88/300], Train Accuracy: 0.8704, Validation Accuracy: 0.6562\n",
      "Epoch [89/300], Train Accuracy: 0.8759, Validation Accuracy: 0.7083\n",
      "Epoch [90/300], Train Accuracy: 0.8805, Validation Accuracy: 0.6771\n",
      "Epoch [91/300], Train Accuracy: 0.8722, Validation Accuracy: 0.7083\n",
      "Epoch [92/300], Train Accuracy: 0.8814, Validation Accuracy: 0.7188\n",
      "Epoch [93/300], Train Accuracy: 0.8934, Validation Accuracy: 0.7292\n",
      "Epoch [94/300], Train Accuracy: 0.8759, Validation Accuracy: 0.6979\n",
      "Epoch [95/300], Train Accuracy: 0.8915, Validation Accuracy: 0.6667\n",
      "Epoch [96/300], Train Accuracy: 0.8768, Validation Accuracy: 0.6771\n",
      "Epoch [97/300], Train Accuracy: 0.8842, Validation Accuracy: 0.7083\n",
      "Epoch [98/300], Train Accuracy: 0.8768, Validation Accuracy: 0.6771\n",
      "Epoch [99/300], Train Accuracy: 0.8897, Validation Accuracy: 0.7604\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [100/300], Train Accuracy: 0.8879, Validation Accuracy: 0.6979\n",
      "Epoch [101/300], Train Accuracy: 0.8814, Validation Accuracy: 0.7083\n",
      "Epoch [102/300], Train Accuracy: 0.8971, Validation Accuracy: 0.6667\n",
      "Epoch [103/300], Train Accuracy: 0.8833, Validation Accuracy: 0.6771\n",
      "Epoch [104/300], Train Accuracy: 0.9035, Validation Accuracy: 0.6771\n",
      "Epoch [105/300], Train Accuracy: 0.8998, Validation Accuracy: 0.6458\n",
      "Epoch [106/300], Train Accuracy: 0.9099, Validation Accuracy: 0.6979\n",
      "Epoch [107/300], Train Accuracy: 0.9035, Validation Accuracy: 0.7500\n",
      "Epoch [108/300], Train Accuracy: 0.9053, Validation Accuracy: 0.6771\n",
      "Epoch [109/300], Train Accuracy: 0.9007, Validation Accuracy: 0.7396\n",
      "Epoch [110/300], Train Accuracy: 0.8934, Validation Accuracy: 0.7083\n",
      "Epoch [111/300], Train Accuracy: 0.9136, Validation Accuracy: 0.7292\n",
      "Epoch [112/300], Train Accuracy: 0.9044, Validation Accuracy: 0.7292\n",
      "Epoch [113/300], Train Accuracy: 0.9118, Validation Accuracy: 0.7188\n",
      "Epoch [114/300], Train Accuracy: 0.9228, Validation Accuracy: 0.7083\n",
      "Epoch [115/300], Train Accuracy: 0.9026, Validation Accuracy: 0.7292\n",
      "Epoch [116/300], Train Accuracy: 0.9062, Validation Accuracy: 0.7292\n",
      "Epoch [117/300], Train Accuracy: 0.9062, Validation Accuracy: 0.7292\n",
      "Epoch [118/300], Train Accuracy: 0.9044, Validation Accuracy: 0.6979\n",
      "Epoch [119/300], Train Accuracy: 0.8934, Validation Accuracy: 0.7708\n",
      "Model saved to sol1_MLP_2.pt\n",
      "Epoch [120/300], Train Accuracy: 0.9210, Validation Accuracy: 0.6667\n",
      "Epoch [121/300], Train Accuracy: 0.8971, Validation Accuracy: 0.6771\n",
      "Epoch [122/300], Train Accuracy: 0.8879, Validation Accuracy: 0.6875\n",
      "Epoch [123/300], Train Accuracy: 0.9026, Validation Accuracy: 0.7188\n",
      "Epoch [124/300], Train Accuracy: 0.9044, Validation Accuracy: 0.7083\n",
      "Epoch [125/300], Train Accuracy: 0.8934, Validation Accuracy: 0.7500\n",
      "Epoch [126/300], Train Accuracy: 0.9072, Validation Accuracy: 0.7188\n",
      "Epoch [127/300], Train Accuracy: 0.9191, Validation Accuracy: 0.6979\n",
      "Epoch [128/300], Train Accuracy: 0.9118, Validation Accuracy: 0.7188\n",
      "Epoch [129/300], Train Accuracy: 0.9228, Validation Accuracy: 0.7708\n",
      "Epoch [130/300], Train Accuracy: 0.9072, Validation Accuracy: 0.7292\n",
      "Epoch [131/300], Train Accuracy: 0.9072, Validation Accuracy: 0.7188\n",
      "Epoch [132/300], Train Accuracy: 0.9182, Validation Accuracy: 0.7396\n",
      "Epoch [133/300], Train Accuracy: 0.9081, Validation Accuracy: 0.7396\n",
      "Epoch [134/300], Train Accuracy: 0.9173, Validation Accuracy: 0.7708\n",
      "Epoch [135/300], Train Accuracy: 0.9127, Validation Accuracy: 0.7083\n",
      "Epoch [136/300], Train Accuracy: 0.9145, Validation Accuracy: 0.7396\n",
      "Epoch [137/300], Train Accuracy: 0.9035, Validation Accuracy: 0.7396\n",
      "Epoch [138/300], Train Accuracy: 0.9173, Validation Accuracy: 0.7188\n",
      "Epoch [139/300], Train Accuracy: 0.9118, Validation Accuracy: 0.7083\n",
      "Epoch [140/300], Train Accuracy: 0.9228, Validation Accuracy: 0.7292\n",
      "Epoch [141/300], Train Accuracy: 0.9099, Validation Accuracy: 0.7396\n",
      "Epoch [142/300], Train Accuracy: 0.9283, Validation Accuracy: 0.7500\n",
      "Epoch [143/300], Train Accuracy: 0.9173, Validation Accuracy: 0.7083\n",
      "Epoch [144/300], Train Accuracy: 0.9081, Validation Accuracy: 0.7188\n",
      "Epoch [145/300], Train Accuracy: 0.9099, Validation Accuracy: 0.7083\n",
      "Epoch [146/300], Train Accuracy: 0.9164, Validation Accuracy: 0.7500\n",
      "Epoch [147/300], Train Accuracy: 0.9338, Validation Accuracy: 0.6667\n",
      "Epoch [148/300], Train Accuracy: 0.9283, Validation Accuracy: 0.6875\n",
      "Epoch [149/300], Train Accuracy: 0.9219, Validation Accuracy: 0.6771\n",
      "Epoch [150/300], Train Accuracy: 0.9256, Validation Accuracy: 0.6979\n",
      "Epoch [151/300], Train Accuracy: 0.9182, Validation Accuracy: 0.7292\n",
      "Epoch [152/300], Train Accuracy: 0.9219, Validation Accuracy: 0.6667\n",
      "Epoch [153/300], Train Accuracy: 0.9108, Validation Accuracy: 0.7188\n",
      "Epoch [154/300], Train Accuracy: 0.9145, Validation Accuracy: 0.7292\n",
      "Epoch [155/300], Train Accuracy: 0.9292, Validation Accuracy: 0.6771\n",
      "Epoch [156/300], Train Accuracy: 0.9283, Validation Accuracy: 0.6979\n",
      "Epoch [157/300], Train Accuracy: 0.9200, Validation Accuracy: 0.7604\n",
      "Epoch [158/300], Train Accuracy: 0.9301, Validation Accuracy: 0.6875\n",
      "Epoch [159/300], Train Accuracy: 0.9228, Validation Accuracy: 0.6771\n",
      "Epoch [160/300], Train Accuracy: 0.9154, Validation Accuracy: 0.6667\n",
      "Epoch [161/300], Train Accuracy: 0.9237, Validation Accuracy: 0.6771\n",
      "Epoch [162/300], Train Accuracy: 0.9265, Validation Accuracy: 0.7292\n",
      "Epoch [163/300], Train Accuracy: 0.9256, Validation Accuracy: 0.6875\n",
      "Epoch [164/300], Train Accuracy: 0.9164, Validation Accuracy: 0.6875\n",
      "Epoch [165/300], Train Accuracy: 0.9458, Validation Accuracy: 0.7396\n",
      "Epoch [166/300], Train Accuracy: 0.9191, Validation Accuracy: 0.6979\n",
      "Epoch [167/300], Train Accuracy: 0.9283, Validation Accuracy: 0.7188\n",
      "Epoch [168/300], Train Accuracy: 0.9292, Validation Accuracy: 0.7396\n",
      "Epoch [169/300], Train Accuracy: 0.9173, Validation Accuracy: 0.7188\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 300\n",
    "model = model2()\n",
    "model.train(train_loader, val_loader, EPOCHS)\n",
    "\n",
    "\n",
    "# train_preds = model.predict(dataroot1 + \"/train.json\")\n",
    "# test_preds = model.predict(dataroot1 + \"/test.json\", \"predictions1.json\")\n",
    "# \n",
    "# train_labels = eval(open(dataroot1 + \"/train.json\").read())\n",
    "# acc1 = accuracy1(train_labels, train_preds)\n",
    "# print(\"Task 1 training accuracy = \" + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f5934c6ae1de768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:40:58.224023Z",
     "start_time": "2025-05-20T19:40:58.113552Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc, val_acc = model.get_train_acc()\n",
    "\n",
    "\n",
    "\n",
    "x = range(1, len(train_acc) + 1)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(x, train_acc, label='Training Accuracy', color='blue', linestyle='-', marker='o')\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(x, val_acc, label='Validation Accuracy', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy per Epoch')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c6a974b5c41f249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:40:58.922189Z",
     "start_time": "2025-05-20T19:40:58.902969Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('fullTrainData.pkl', 'wb') as f:\n",
    "    data = {'train_loader': train_loader, 'val_loader':val_loader, 'train_dataset': train_dataset, 'val_dataset':val_dataset}\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b673452e35da44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ebda317a7583bf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:41:00.129196Z",
     "start_time": "2025-05-20T19:41:00.123469Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = model2()\n",
    "# MLP = MLPClassifier()\n",
    "# MLP = load_model(MLPClassifier, \"sol1_MLP.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "921a8fd2cd536cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:41:00.435606Z",
     "start_time": "2025-05-20T19:41:00.430287Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.model = MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d01ed86b17d905dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:41:56.424140Z",
     "start_time": "2025-05-20T19:41:00.902983Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9272/3284981968.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(features(k), dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'midis/1210.mid': 'Bach',\n",
       " 'midis/1211.mid': 'Beethoven',\n",
       " 'midis/1212.mid': 'Beethoven',\n",
       " 'midis/1213.mid': 'Chopin',\n",
       " 'midis/1214.mid': 'Beethoven',\n",
       " 'midis/1215.mid': 'Bach',\n",
       " 'midis/1216.mid': 'Schumann',\n",
       " 'midis/1217.mid': 'Haydn',\n",
       " 'midis/1218.mid': 'Bach',\n",
       " 'midis/1219.mid': 'Beethoven',\n",
       " 'midis/1220.mid': 'Haydn',\n",
       " 'midis/1221.mid': 'Chopin',\n",
       " 'midis/1222.mid': 'Beethoven',\n",
       " 'midis/1223.mid': 'Beethoven',\n",
       " 'midis/1224.mid': 'Beethoven',\n",
       " 'midis/1225.mid': 'Bach',\n",
       " 'midis/1226.mid': 'Beethoven',\n",
       " 'midis/1227.mid': 'Chopin',\n",
       " 'midis/1228.mid': 'Schumann',\n",
       " 'midis/1229.mid': 'Beethoven',\n",
       " 'midis/1230.mid': 'Liszt',\n",
       " 'midis/1231.mid': 'Beethoven',\n",
       " 'midis/1232.mid': 'Chopin',\n",
       " 'midis/1233.mid': 'Liszt',\n",
       " 'midis/1234.mid': 'Schubert',\n",
       " 'midis/1235.mid': 'Schubert',\n",
       " 'midis/1236.mid': 'Liszt',\n",
       " 'midis/1237.mid': 'Schubert',\n",
       " 'midis/1238.mid': 'Schubert',\n",
       " 'midis/1239.mid': 'Beethoven',\n",
       " 'midis/1240.mid': 'Beethoven',\n",
       " 'midis/1241.mid': 'Beethoven',\n",
       " 'midis/1242.mid': 'Beethoven',\n",
       " 'midis/1243.mid': 'Mozart',\n",
       " 'midis/1244.mid': 'Schubert',\n",
       " 'midis/1245.mid': 'Chopin',\n",
       " 'midis/1246.mid': 'Beethoven',\n",
       " 'midis/1247.mid': 'Bach',\n",
       " 'midis/1248.mid': 'Liszt',\n",
       " 'midis/1249.mid': 'Beethoven',\n",
       " 'midis/1250.mid': 'Beethoven',\n",
       " 'midis/1251.mid': 'Chopin',\n",
       " 'midis/1252.mid': 'Beethoven',\n",
       " 'midis/1253.mid': 'Schumann',\n",
       " 'midis/1254.mid': 'Liszt',\n",
       " 'midis/1255.mid': 'Liszt',\n",
       " 'midis/1256.mid': 'Liszt',\n",
       " 'midis/1257.mid': 'Schubert',\n",
       " 'midis/1258.mid': 'Beethoven',\n",
       " 'midis/1259.mid': 'Beethoven',\n",
       " 'midis/1260.mid': 'Beethoven',\n",
       " 'midis/1261.mid': 'Chopin',\n",
       " 'midis/1262.mid': 'Haydn',\n",
       " 'midis/1263.mid': 'Beethoven',\n",
       " 'midis/1264.mid': 'Chopin',\n",
       " 'midis/1265.mid': 'Bach',\n",
       " 'midis/1266.mid': 'Beethoven',\n",
       " 'midis/1267.mid': 'Beethoven',\n",
       " 'midis/1268.mid': 'Mozart',\n",
       " 'midis/1269.mid': 'Schubert',\n",
       " 'midis/1270.mid': 'Schubert',\n",
       " 'midis/1271.mid': 'Beethoven',\n",
       " 'midis/1272.mid': 'Chopin',\n",
       " 'midis/1273.mid': 'Beethoven',\n",
       " 'midis/1274.mid': 'Beethoven',\n",
       " 'midis/1275.mid': 'Bach',\n",
       " 'midis/1276.mid': 'Liszt',\n",
       " 'midis/1277.mid': 'Mozart',\n",
       " 'midis/1278.mid': 'Bach',\n",
       " 'midis/1279.mid': 'Beethoven',\n",
       " 'midis/1280.mid': 'Beethoven',\n",
       " 'midis/1281.mid': 'Chopin',\n",
       " 'midis/1282.mid': 'Liszt',\n",
       " 'midis/1283.mid': 'Beethoven',\n",
       " 'midis/1284.mid': 'Chopin',\n",
       " 'midis/1285.mid': 'Chopin',\n",
       " 'midis/1286.mid': 'Beethoven',\n",
       " 'midis/1287.mid': 'Bach',\n",
       " 'midis/1288.mid': 'Mozart',\n",
       " 'midis/1289.mid': 'Liszt',\n",
       " 'midis/1290.mid': 'Chopin',\n",
       " 'midis/1291.mid': 'Haydn',\n",
       " 'midis/1292.mid': 'Beethoven',\n",
       " 'midis/1293.mid': 'Beethoven',\n",
       " 'midis/1294.mid': 'Beethoven',\n",
       " 'midis/1295.mid': 'Beethoven',\n",
       " 'midis/1296.mid': 'Bach',\n",
       " 'midis/1297.mid': 'Chopin',\n",
       " 'midis/1298.mid': 'Mozart',\n",
       " 'midis/1299.mid': 'Beethoven',\n",
       " 'midis/1300.mid': 'Schumann',\n",
       " 'midis/1301.mid': 'Liszt',\n",
       " 'midis/1302.mid': 'Beethoven',\n",
       " 'midis/1303.mid': 'Chopin',\n",
       " 'midis/1304.mid': 'Mozart',\n",
       " 'midis/1305.mid': 'Schubert',\n",
       " 'midis/1306.mid': 'Liszt',\n",
       " 'midis/1307.mid': 'Beethoven',\n",
       " 'midis/1308.mid': 'Beethoven',\n",
       " 'midis/1309.mid': 'Bach',\n",
       " 'midis/1310.mid': 'Liszt',\n",
       " 'midis/1311.mid': 'Schubert',\n",
       " 'midis/1312.mid': 'Liszt',\n",
       " 'midis/1313.mid': 'Liszt',\n",
       " 'midis/1314.mid': 'Chopin',\n",
       " 'midis/1315.mid': 'Beethoven',\n",
       " 'midis/1316.mid': 'Beethoven',\n",
       " 'midis/1317.mid': 'Beethoven',\n",
       " 'midis/1318.mid': 'Bach',\n",
       " 'midis/1319.mid': 'Chopin',\n",
       " 'midis/1320.mid': 'Schubert',\n",
       " 'midis/1321.mid': 'Beethoven',\n",
       " 'midis/1322.mid': 'Chopin',\n",
       " 'midis/1323.mid': 'Bach',\n",
       " 'midis/1324.mid': 'Schumann',\n",
       " 'midis/1325.mid': 'Liszt',\n",
       " 'midis/1326.mid': 'Beethoven',\n",
       " 'midis/1327.mid': 'Schumann',\n",
       " 'midis/1328.mid': 'Liszt',\n",
       " 'midis/1329.mid': 'Schumann',\n",
       " 'midis/1330.mid': 'Bach',\n",
       " 'midis/1331.mid': 'Chopin',\n",
       " 'midis/1332.mid': 'Beethoven',\n",
       " 'midis/1333.mid': 'Bach',\n",
       " 'midis/1334.mid': 'Schumann',\n",
       " 'midis/1335.mid': 'Bach',\n",
       " 'midis/1336.mid': 'Beethoven',\n",
       " 'midis/1337.mid': 'Chopin',\n",
       " 'midis/1338.mid': 'Chopin',\n",
       " 'midis/1339.mid': 'Schumann',\n",
       " 'midis/1340.mid': 'Liszt',\n",
       " 'midis/1341.mid': 'Bach',\n",
       " 'midis/1342.mid': 'Chopin',\n",
       " 'midis/1343.mid': 'Haydn',\n",
       " 'midis/1344.mid': 'Beethoven',\n",
       " 'midis/1345.mid': 'Schubert',\n",
       " 'midis/1346.mid': 'Chopin',\n",
       " 'midis/1347.mid': 'Bach',\n",
       " 'midis/1348.mid': 'Beethoven',\n",
       " 'midis/1349.mid': 'Haydn',\n",
       " 'midis/1350.mid': 'Beethoven',\n",
       " 'midis/1351.mid': 'Beethoven',\n",
       " 'midis/1352.mid': 'Beethoven',\n",
       " 'midis/1353.mid': 'Liszt',\n",
       " 'midis/1354.mid': 'Liszt',\n",
       " 'midis/1355.mid': 'Liszt',\n",
       " 'midis/1356.mid': 'Beethoven',\n",
       " 'midis/1357.mid': 'Beethoven',\n",
       " 'midis/1358.mid': 'Beethoven',\n",
       " 'midis/1359.mid': 'Chopin',\n",
       " 'midis/1360.mid': 'Chopin',\n",
       " 'midis/1361.mid': 'Beethoven',\n",
       " 'midis/1362.mid': 'Beethoven',\n",
       " 'midis/1363.mid': 'Chopin',\n",
       " 'midis/1364.mid': 'Schubert',\n",
       " 'midis/1365.mid': 'Haydn',\n",
       " 'midis/1366.mid': 'Chopin',\n",
       " 'midis/1367.mid': 'Beethoven',\n",
       " 'midis/1368.mid': 'Beethoven',\n",
       " 'midis/1369.mid': 'Schubert',\n",
       " 'midis/1370.mid': 'Chopin',\n",
       " 'midis/1371.mid': 'Beethoven',\n",
       " 'midis/1372.mid': 'Bach',\n",
       " 'midis/1373.mid': 'Chopin',\n",
       " 'midis/1374.mid': 'Liszt',\n",
       " 'midis/1375.mid': 'Schumann',\n",
       " 'midis/1376.mid': 'Haydn',\n",
       " 'midis/1377.mid': 'Bach',\n",
       " 'midis/1378.mid': 'Bach',\n",
       " 'midis/1379.mid': 'Liszt',\n",
       " 'midis/1380.mid': 'Beethoven',\n",
       " 'midis/1381.mid': 'Beethoven',\n",
       " 'midis/1382.mid': 'Bach',\n",
       " 'midis/1383.mid': 'Haydn',\n",
       " 'midis/1384.mid': 'Liszt',\n",
       " 'midis/1385.mid': 'Beethoven',\n",
       " 'midis/1386.mid': 'Liszt',\n",
       " 'midis/1387.mid': 'Haydn',\n",
       " 'midis/1388.mid': 'Beethoven',\n",
       " 'midis/1389.mid': 'Schumann',\n",
       " 'midis/1390.mid': 'Beethoven',\n",
       " 'midis/1391.mid': 'Beethoven',\n",
       " 'midis/1392.mid': 'Chopin',\n",
       " 'midis/1393.mid': 'Beethoven',\n",
       " 'midis/1394.mid': 'Liszt',\n",
       " 'midis/1395.mid': 'Chopin',\n",
       " 'midis/1396.mid': 'Beethoven',\n",
       " 'midis/1397.mid': 'Chopin',\n",
       " 'midis/1398.mid': 'Beethoven',\n",
       " 'midis/1399.mid': 'Chopin',\n",
       " 'midis/1400.mid': 'Schumann',\n",
       " 'midis/1401.mid': 'Haydn',\n",
       " 'midis/1402.mid': 'Beethoven',\n",
       " 'midis/1403.mid': 'Bach',\n",
       " 'midis/1404.mid': 'Haydn',\n",
       " 'midis/1405.mid': 'Chopin',\n",
       " 'midis/1406.mid': 'Chopin',\n",
       " 'midis/1407.mid': 'Haydn',\n",
       " 'midis/1408.mid': 'Beethoven',\n",
       " 'midis/1409.mid': 'Schumann',\n",
       " 'midis/1410.mid': 'Liszt',\n",
       " 'midis/1411.mid': 'Chopin',\n",
       " 'midis/1412.mid': 'Schubert',\n",
       " 'midis/1413.mid': 'Liszt',\n",
       " 'midis/1414.mid': 'Chopin',\n",
       " 'midis/1415.mid': 'Haydn',\n",
       " 'midis/1416.mid': 'Liszt',\n",
       " 'midis/1417.mid': 'Bach',\n",
       " 'midis/1418.mid': 'Chopin',\n",
       " 'midis/1419.mid': 'Chopin',\n",
       " 'midis/1420.mid': 'Bach',\n",
       " 'midis/1421.mid': 'Bach',\n",
       " 'midis/1422.mid': 'Schumann',\n",
       " 'midis/1423.mid': 'Beethoven',\n",
       " 'midis/1424.mid': 'Bach',\n",
       " 'midis/1425.mid': 'Beethoven',\n",
       " 'midis/1426.mid': 'Haydn',\n",
       " 'midis/1427.mid': 'Chopin',\n",
       " 'midis/1428.mid': 'Liszt',\n",
       " 'midis/1429.mid': 'Bach',\n",
       " 'midis/1430.mid': 'Chopin',\n",
       " 'midis/1431.mid': 'Bach',\n",
       " 'midis/1432.mid': 'Beethoven',\n",
       " 'midis/1433.mid': 'Schumann',\n",
       " 'midis/1434.mid': 'Beethoven',\n",
       " 'midis/1435.mid': 'Beethoven',\n",
       " 'midis/1436.mid': 'Schubert',\n",
       " 'midis/1437.mid': 'Chopin',\n",
       " 'midis/1438.mid': 'Liszt',\n",
       " 'midis/1439.mid': 'Beethoven',\n",
       " 'midis/1440.mid': 'Mozart',\n",
       " 'midis/1441.mid': 'Beethoven',\n",
       " 'midis/1442.mid': 'Schubert',\n",
       " 'midis/1443.mid': 'Beethoven',\n",
       " 'midis/1444.mid': 'Chopin',\n",
       " 'midis/1445.mid': 'Haydn',\n",
       " 'midis/1446.mid': 'Chopin',\n",
       " 'midis/1447.mid': 'Beethoven',\n",
       " 'midis/1448.mid': 'Beethoven',\n",
       " 'midis/1449.mid': 'Chopin',\n",
       " 'midis/1450.mid': 'Schubert',\n",
       " 'midis/1451.mid': 'Beethoven',\n",
       " 'midis/1452.mid': 'Beethoven',\n",
       " 'midis/1453.mid': 'Beethoven',\n",
       " 'midis/1454.mid': 'Beethoven',\n",
       " 'midis/1455.mid': 'Beethoven',\n",
       " 'midis/1456.mid': 'Liszt',\n",
       " 'midis/1457.mid': 'Beethoven',\n",
       " 'midis/1458.mid': 'Haydn',\n",
       " 'midis/1459.mid': 'Liszt',\n",
       " 'midis/1460.mid': 'Chopin',\n",
       " 'midis/1461.mid': 'Beethoven',\n",
       " 'midis/1462.mid': 'Liszt',\n",
       " 'midis/1463.mid': 'Liszt',\n",
       " 'midis/1464.mid': 'Liszt',\n",
       " 'midis/1465.mid': 'Chopin',\n",
       " 'midis/1466.mid': 'Schubert',\n",
       " 'midis/1467.mid': 'Schumann',\n",
       " 'midis/1468.mid': 'Beethoven',\n",
       " 'midis/1469.mid': 'Schubert',\n",
       " 'midis/1470.mid': 'Beethoven',\n",
       " 'midis/1471.mid': 'Bach',\n",
       " 'midis/1472.mid': 'Beethoven',\n",
       " 'midis/1473.mid': 'Bach',\n",
       " 'midis/1474.mid': 'Beethoven',\n",
       " 'midis/1475.mid': 'Chopin',\n",
       " 'midis/1476.mid': 'Bach',\n",
       " 'midis/1477.mid': 'Beethoven',\n",
       " 'midis/1478.mid': 'Beethoven',\n",
       " 'midis/1479.mid': 'Schubert',\n",
       " 'midis/1480.mid': 'Schumann',\n",
       " 'midis/1481.mid': 'Bach',\n",
       " 'midis/1482.mid': 'Beethoven',\n",
       " 'midis/1483.mid': 'Haydn',\n",
       " 'midis/1484.mid': 'Haydn',\n",
       " 'midis/1485.mid': 'Bach',\n",
       " 'midis/1486.mid': 'Liszt',\n",
       " 'midis/1487.mid': 'Haydn',\n",
       " 'midis/1488.mid': 'Beethoven',\n",
       " 'midis/1489.mid': 'Chopin',\n",
       " 'midis/1490.mid': 'Chopin',\n",
       " 'midis/1491.mid': 'Haydn',\n",
       " 'midis/1492.mid': 'Schubert',\n",
       " 'midis/1493.mid': 'Schumann',\n",
       " 'midis/1494.mid': 'Schumann',\n",
       " 'midis/1495.mid': 'Haydn',\n",
       " 'midis/1496.mid': 'Schubert',\n",
       " 'midis/1497.mid': 'Liszt',\n",
       " 'midis/1498.mid': 'Haydn',\n",
       " 'midis/1499.mid': 'Chopin',\n",
       " 'midis/1500.mid': 'Chopin',\n",
       " 'midis/1501.mid': 'Chopin',\n",
       " 'midis/1502.mid': 'Beethoven',\n",
       " 'midis/1503.mid': 'Bach',\n",
       " 'midis/1504.mid': 'Liszt',\n",
       " 'midis/1505.mid': 'Chopin',\n",
       " 'midis/1506.mid': 'Liszt',\n",
       " 'midis/1507.mid': 'Beethoven',\n",
       " 'midis/1508.mid': 'Beethoven',\n",
       " 'midis/1509.mid': 'Bach',\n",
       " 'midis/1510.mid': 'Chopin',\n",
       " 'midis/1511.mid': 'Schubert',\n",
       " 'midis/1512.mid': 'Chopin',\n",
       " 'midis/1513.mid': 'Beethoven',\n",
       " 'midis/1514.mid': 'Liszt',\n",
       " 'midis/1515.mid': 'Chopin',\n",
       " 'midis/1516.mid': 'Liszt',\n",
       " 'midis/1517.mid': 'Schumann',\n",
       " 'midis/1518.mid': 'Chopin',\n",
       " 'midis/1519.mid': 'Beethoven',\n",
       " 'midis/1520.mid': 'Schumann',\n",
       " 'midis/1521.mid': 'Liszt',\n",
       " 'midis/1522.mid': 'Beethoven',\n",
       " 'midis/1523.mid': 'Chopin',\n",
       " 'midis/1524.mid': 'Beethoven',\n",
       " 'midis/1525.mid': 'Beethoven',\n",
       " 'midis/1526.mid': 'Chopin',\n",
       " 'midis/1527.mid': 'Chopin',\n",
       " 'midis/1528.mid': 'Bach',\n",
       " 'midis/1529.mid': 'Liszt',\n",
       " 'midis/1530.mid': 'Chopin',\n",
       " 'midis/1531.mid': 'Schumann',\n",
       " 'midis/1532.mid': 'Mozart',\n",
       " 'midis/1533.mid': 'Liszt',\n",
       " 'midis/1534.mid': 'Beethoven',\n",
       " 'midis/1535.mid': 'Bach',\n",
       " 'midis/1536.mid': 'Bach',\n",
       " 'midis/1537.mid': 'Beethoven',\n",
       " 'midis/1538.mid': 'Beethoven',\n",
       " 'midis/1539.mid': 'Chopin',\n",
       " 'midis/1540.mid': 'Liszt',\n",
       " 'midis/1541.mid': 'Liszt',\n",
       " 'midis/1542.mid': 'Chopin',\n",
       " 'midis/1543.mid': 'Chopin',\n",
       " 'midis/1544.mid': 'Beethoven',\n",
       " 'midis/1545.mid': 'Chopin',\n",
       " 'midis/1546.mid': 'Chopin',\n",
       " 'midis/1547.mid': 'Liszt',\n",
       " 'midis/1548.mid': 'Beethoven',\n",
       " 'midis/1549.mid': 'Bach',\n",
       " 'midis/1550.mid': 'Schumann',\n",
       " 'midis/1551.mid': 'Bach',\n",
       " 'midis/1552.mid': 'Chopin',\n",
       " 'midis/1553.mid': 'Chopin',\n",
       " 'midis/1554.mid': 'Beethoven',\n",
       " 'midis/1555.mid': 'Beethoven',\n",
       " 'midis/1556.mid': 'Schumann',\n",
       " 'midis/1557.mid': 'Schumann',\n",
       " 'midis/1558.mid': 'Beethoven',\n",
       " 'midis/1559.mid': 'Schumann',\n",
       " 'midis/1560.mid': 'Beethoven',\n",
       " 'midis/1561.mid': 'Beethoven',\n",
       " 'midis/1562.mid': 'Beethoven',\n",
       " 'midis/1563.mid': 'Schumann',\n",
       " 'midis/1564.mid': 'Beethoven',\n",
       " 'midis/1565.mid': 'Bach',\n",
       " 'midis/1566.mid': 'Schumann',\n",
       " 'midis/1567.mid': 'Chopin',\n",
       " 'midis/1568.mid': 'Bach',\n",
       " 'midis/1569.mid': 'Mozart',\n",
       " 'midis/1570.mid': 'Chopin',\n",
       " 'midis/1571.mid': 'Schumann',\n",
       " 'midis/1572.mid': 'Liszt',\n",
       " 'midis/1573.mid': 'Beethoven',\n",
       " 'midis/1574.mid': 'Beethoven',\n",
       " 'midis/1575.mid': 'Bach',\n",
       " 'midis/1576.mid': 'Bach',\n",
       " 'midis/1577.mid': 'Beethoven',\n",
       " 'midis/1578.mid': 'Bach',\n",
       " 'midis/1579.mid': 'Beethoven',\n",
       " 'midis/1580.mid': 'Beethoven',\n",
       " 'midis/1581.mid': 'Schumann',\n",
       " 'midis/1582.mid': 'Bach',\n",
       " 'midis/1583.mid': 'Bach',\n",
       " 'midis/1584.mid': 'Beethoven',\n",
       " 'midis/1585.mid': 'Liszt',\n",
       " 'midis/1586.mid': 'Beethoven',\n",
       " 'midis/1587.mid': 'Schubert',\n",
       " 'midis/1588.mid': 'Schumann',\n",
       " 'midis/1589.mid': 'Beethoven',\n",
       " 'midis/1590.mid': 'Schumann',\n",
       " 'midis/1591.mid': 'Beethoven',\n",
       " 'midis/1592.mid': 'Beethoven',\n",
       " 'midis/1593.mid': 'Beethoven',\n",
       " 'midis/1594.mid': 'Mozart',\n",
       " 'midis/1595.mid': 'Schumann',\n",
       " 'midis/1596.mid': 'Bach',\n",
       " 'midis/1597.mid': 'Chopin',\n",
       " 'midis/1598.mid': 'Mozart'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dataroot1+\"/test.json\", outpath=\"predictions1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "624f5fb400aa6386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:04:42.316850Z",
     "start_time": "2025-05-20T22:04:12.273672Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8.1018  | val_accuracy: 0.08264 |  0:00:02s\n",
      "epoch 1  | loss: 5.53447 | val_accuracy: 0.1405  |  0:00:03s\n",
      "epoch 2  | loss: 4.7496  | val_accuracy: 0.28926 |  0:00:04s\n",
      "epoch 3  | loss: 3.77169 | val_accuracy: 0.39669 |  0:00:05s\n",
      "epoch 4  | loss: 3.30748 | val_accuracy: 0.39669 |  0:00:07s\n",
      "epoch 5  | loss: 3.26651 | val_accuracy: 0.36364 |  0:00:08s\n",
      "epoch 6  | loss: 3.09013 | val_accuracy: 0.33884 |  0:00:09s\n",
      "epoch 7  | loss: 3.08457 | val_accuracy: 0.33058 |  0:00:10s\n",
      "epoch 8  | loss: 2.99407 | val_accuracy: 0.33058 |  0:00:11s\n",
      "epoch 9  | loss: 2.90112 | val_accuracy: 0.31405 |  0:00:12s\n",
      "epoch 10 | loss: 2.72629 | val_accuracy: 0.32231 |  0:00:14s\n",
      "epoch 11 | loss: 2.6929  | val_accuracy: 0.30579 |  0:00:15s\n",
      "epoch 12 | loss: 2.64355 | val_accuracy: 0.32231 |  0:00:16s\n",
      "epoch 13 | loss: 2.66028 | val_accuracy: 0.33884 |  0:00:17s\n",
      "epoch 14 | loss: 2.62915 | val_accuracy: 0.3719  |  0:00:18s\n",
      "epoch 15 | loss: 2.47657 | val_accuracy: 0.36364 |  0:00:19s\n",
      "epoch 16 | loss: 2.51599 | val_accuracy: 0.3719  |  0:00:20s\n",
      "epoch 17 | loss: 2.47655 | val_accuracy: 0.36364 |  0:00:22s\n",
      "epoch 18 | loss: 2.38286 | val_accuracy: 0.36364 |  0:00:23s\n",
      "epoch 19 | loss: 2.70766 | val_accuracy: 0.35537 |  0:00:24s\n",
      "epoch 20 | loss: 2.56976 | val_accuracy: 0.36364 |  0:00:25s\n",
      "epoch 21 | loss: 2.37135 | val_accuracy: 0.35537 |  0:00:26s\n",
      "epoch 22 | loss: 2.43106 | val_accuracy: 0.3719  |  0:00:27s\n",
      "epoch 23 | loss: 2.26624 | val_accuracy: 0.38017 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_accuracy = 0.39669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "tab = TabNetClassifier(\n",
    "    n_d=1024,                    # Width of decision prediction layer\n",
    "    n_a=128,                    # Width of attention embedding\n",
    "    # n_steps=5,                 # Number of decision steps\n",
    "    # gamma=1.5,                 # Relaxation parameter in sparsemax\n",
    "    # lambda_sparse=1e-4,        # Sparsity regularization\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.001),\n",
    "    scheduler_params={\"step_size\": 10, \"gamma\": 0.9},  # Learning rate scheduler\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',        # Try 'entmax' (or fallback to 'sparsemax')\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tab.fit(\n",
    "    X_train=X_train.numpy(), \n",
    "    y_train=y_train.numpy(),\n",
    "    eval_set=[(X_val.numpy(), y_val.numpy())],\n",
    "    eval_name=[\"val\"],\n",
    "    eval_metric=[\"accuracy\"],   # Try: \"accuracy\", \"logloss\", \"auc\", \"balanced_accuracy\"\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "201611d2bc804f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:52:58.978191Z",
     "start_time": "2025-05-20T21:52:58.969269Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(path, outpath=None):\n",
    "    # Load your input dictionary\n",
    "    d = eval(open(path, 'r').read())\n",
    "    predictions = {}\n",
    "\n",
    "    # Prepare batch inputs\n",
    "    keys = list(d)\n",
    "    X = np.array([features(k) for k in keys], dtype=np.float32)  # shape: (N, feature_dim)\n",
    "\n",
    "    # Predict using TabNet\n",
    "    preds = tab.predict(X)  # shape: (N,)\n",
    "    \n",
    "    # Map predictions back to composer names\n",
    "    for key, pred in zip(keys, preds):\n",
    "        predictions[key] = str(idToArtist[int(pred)])\n",
    "\n",
    "    # Optionally save results\n",
    "    if outpath:\n",
    "        with open(outpath, \"w\") as z:\n",
    "            z.write(str(predictions) + '\\n')\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e188f4c26d0fd98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:54:07.478342Z",
     "start_time": "2025-05-20T21:52:59.874205Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'midis/1210.mid': 'Schumann',\n",
       " 'midis/1211.mid': 'Beethoven',\n",
       " 'midis/1212.mid': 'Beethoven',\n",
       " 'midis/1213.mid': 'Beethoven',\n",
       " 'midis/1214.mid': 'Beethoven',\n",
       " 'midis/1215.mid': 'Schumann',\n",
       " 'midis/1216.mid': 'Beethoven',\n",
       " 'midis/1217.mid': 'Beethoven',\n",
       " 'midis/1218.mid': 'Beethoven',\n",
       " 'midis/1219.mid': 'Beethoven',\n",
       " 'midis/1220.mid': 'Beethoven',\n",
       " 'midis/1221.mid': 'Beethoven',\n",
       " 'midis/1222.mid': 'Chopin',\n",
       " 'midis/1223.mid': 'Beethoven',\n",
       " 'midis/1224.mid': 'Beethoven',\n",
       " 'midis/1225.mid': 'Beethoven',\n",
       " 'midis/1226.mid': 'Beethoven',\n",
       " 'midis/1227.mid': 'Beethoven',\n",
       " 'midis/1228.mid': 'Beethoven',\n",
       " 'midis/1229.mid': 'Beethoven',\n",
       " 'midis/1230.mid': 'Beethoven',\n",
       " 'midis/1231.mid': 'Beethoven',\n",
       " 'midis/1232.mid': 'Beethoven',\n",
       " 'midis/1233.mid': 'Beethoven',\n",
       " 'midis/1234.mid': 'Beethoven',\n",
       " 'midis/1235.mid': 'Chopin',\n",
       " 'midis/1236.mid': 'Beethoven',\n",
       " 'midis/1237.mid': 'Beethoven',\n",
       " 'midis/1238.mid': 'Beethoven',\n",
       " 'midis/1239.mid': 'Beethoven',\n",
       " 'midis/1240.mid': 'Beethoven',\n",
       " 'midis/1241.mid': 'Beethoven',\n",
       " 'midis/1242.mid': 'Beethoven',\n",
       " 'midis/1243.mid': 'Beethoven',\n",
       " 'midis/1244.mid': 'Beethoven',\n",
       " 'midis/1245.mid': 'Beethoven',\n",
       " 'midis/1246.mid': 'Beethoven',\n",
       " 'midis/1247.mid': 'Beethoven',\n",
       " 'midis/1248.mid': 'Beethoven',\n",
       " 'midis/1249.mid': 'Beethoven',\n",
       " 'midis/1250.mid': 'Beethoven',\n",
       " 'midis/1251.mid': 'Beethoven',\n",
       " 'midis/1252.mid': 'Beethoven',\n",
       " 'midis/1253.mid': 'Beethoven',\n",
       " 'midis/1254.mid': 'Beethoven',\n",
       " 'midis/1255.mid': 'Beethoven',\n",
       " 'midis/1256.mid': 'Chopin',\n",
       " 'midis/1257.mid': 'Beethoven',\n",
       " 'midis/1258.mid': 'Beethoven',\n",
       " 'midis/1259.mid': 'Beethoven',\n",
       " 'midis/1260.mid': 'Beethoven',\n",
       " 'midis/1261.mid': 'Beethoven',\n",
       " 'midis/1262.mid': 'Bach',\n",
       " 'midis/1263.mid': 'Beethoven',\n",
       " 'midis/1264.mid': 'Beethoven',\n",
       " 'midis/1265.mid': 'Beethoven',\n",
       " 'midis/1266.mid': 'Beethoven',\n",
       " 'midis/1267.mid': 'Beethoven',\n",
       " 'midis/1268.mid': 'Beethoven',\n",
       " 'midis/1269.mid': 'Beethoven',\n",
       " 'midis/1270.mid': 'Beethoven',\n",
       " 'midis/1271.mid': 'Beethoven',\n",
       " 'midis/1272.mid': 'Beethoven',\n",
       " 'midis/1273.mid': 'Beethoven',\n",
       " 'midis/1274.mid': 'Beethoven',\n",
       " 'midis/1275.mid': 'Beethoven',\n",
       " 'midis/1276.mid': 'Beethoven',\n",
       " 'midis/1277.mid': 'Beethoven',\n",
       " 'midis/1278.mid': 'Bach',\n",
       " 'midis/1279.mid': 'Beethoven',\n",
       " 'midis/1280.mid': 'Beethoven',\n",
       " 'midis/1281.mid': 'Beethoven',\n",
       " 'midis/1282.mid': 'Beethoven',\n",
       " 'midis/1283.mid': 'Beethoven',\n",
       " 'midis/1284.mid': 'Beethoven',\n",
       " 'midis/1285.mid': 'Liszt',\n",
       " 'midis/1286.mid': 'Beethoven',\n",
       " 'midis/1287.mid': 'Beethoven',\n",
       " 'midis/1288.mid': 'Beethoven',\n",
       " 'midis/1289.mid': 'Beethoven',\n",
       " 'midis/1290.mid': 'Beethoven',\n",
       " 'midis/1291.mid': 'Beethoven',\n",
       " 'midis/1292.mid': 'Beethoven',\n",
       " 'midis/1293.mid': 'Beethoven',\n",
       " 'midis/1294.mid': 'Beethoven',\n",
       " 'midis/1295.mid': 'Beethoven',\n",
       " 'midis/1296.mid': 'Bach',\n",
       " 'midis/1297.mid': 'Beethoven',\n",
       " 'midis/1298.mid': 'Beethoven',\n",
       " 'midis/1299.mid': 'Chopin',\n",
       " 'midis/1300.mid': 'Beethoven',\n",
       " 'midis/1301.mid': 'Chopin',\n",
       " 'midis/1302.mid': 'Beethoven',\n",
       " 'midis/1303.mid': 'Beethoven',\n",
       " 'midis/1304.mid': 'Beethoven',\n",
       " 'midis/1305.mid': 'Beethoven',\n",
       " 'midis/1306.mid': 'Beethoven',\n",
       " 'midis/1307.mid': 'Beethoven',\n",
       " 'midis/1308.mid': 'Beethoven',\n",
       " 'midis/1309.mid': 'Schumann',\n",
       " 'midis/1310.mid': 'Beethoven',\n",
       " 'midis/1311.mid': 'Beethoven',\n",
       " 'midis/1312.mid': 'Beethoven',\n",
       " 'midis/1313.mid': 'Beethoven',\n",
       " 'midis/1314.mid': 'Beethoven',\n",
       " 'midis/1315.mid': 'Beethoven',\n",
       " 'midis/1316.mid': 'Beethoven',\n",
       " 'midis/1317.mid': 'Beethoven',\n",
       " 'midis/1318.mid': 'Bach',\n",
       " 'midis/1319.mid': 'Chopin',\n",
       " 'midis/1320.mid': 'Beethoven',\n",
       " 'midis/1321.mid': 'Beethoven',\n",
       " 'midis/1322.mid': 'Beethoven',\n",
       " 'midis/1323.mid': 'Bach',\n",
       " 'midis/1324.mid': 'Beethoven',\n",
       " 'midis/1325.mid': 'Beethoven',\n",
       " 'midis/1326.mid': 'Beethoven',\n",
       " 'midis/1327.mid': 'Beethoven',\n",
       " 'midis/1328.mid': 'Beethoven',\n",
       " 'midis/1329.mid': 'Beethoven',\n",
       " 'midis/1330.mid': 'Beethoven',\n",
       " 'midis/1331.mid': 'Bach',\n",
       " 'midis/1332.mid': 'Beethoven',\n",
       " 'midis/1333.mid': 'Beethoven',\n",
       " 'midis/1334.mid': 'Beethoven',\n",
       " 'midis/1335.mid': 'Beethoven',\n",
       " 'midis/1336.mid': 'Beethoven',\n",
       " 'midis/1337.mid': 'Bach',\n",
       " 'midis/1338.mid': 'Beethoven',\n",
       " 'midis/1339.mid': 'Beethoven',\n",
       " 'midis/1340.mid': 'Beethoven',\n",
       " 'midis/1341.mid': 'Schumann',\n",
       " 'midis/1342.mid': 'Beethoven',\n",
       " 'midis/1343.mid': 'Beethoven',\n",
       " 'midis/1344.mid': 'Beethoven',\n",
       " 'midis/1345.mid': 'Beethoven',\n",
       " 'midis/1346.mid': 'Beethoven',\n",
       " 'midis/1347.mid': 'Bach',\n",
       " 'midis/1348.mid': 'Beethoven',\n",
       " 'midis/1349.mid': 'Beethoven',\n",
       " 'midis/1350.mid': 'Beethoven',\n",
       " 'midis/1351.mid': 'Beethoven',\n",
       " 'midis/1352.mid': 'Beethoven',\n",
       " 'midis/1353.mid': 'Schubert',\n",
       " 'midis/1354.mid': 'Beethoven',\n",
       " 'midis/1355.mid': 'Beethoven',\n",
       " 'midis/1356.mid': 'Beethoven',\n",
       " 'midis/1357.mid': 'Beethoven',\n",
       " 'midis/1358.mid': 'Beethoven',\n",
       " 'midis/1359.mid': 'Beethoven',\n",
       " 'midis/1360.mid': 'Chopin',\n",
       " 'midis/1361.mid': 'Beethoven',\n",
       " 'midis/1362.mid': 'Beethoven',\n",
       " 'midis/1363.mid': 'Beethoven',\n",
       " 'midis/1364.mid': 'Beethoven',\n",
       " 'midis/1365.mid': 'Beethoven',\n",
       " 'midis/1366.mid': 'Beethoven',\n",
       " 'midis/1367.mid': 'Beethoven',\n",
       " 'midis/1368.mid': 'Beethoven',\n",
       " 'midis/1369.mid': 'Beethoven',\n",
       " 'midis/1370.mid': 'Beethoven',\n",
       " 'midis/1371.mid': 'Beethoven',\n",
       " 'midis/1372.mid': 'Bach',\n",
       " 'midis/1373.mid': 'Beethoven',\n",
       " 'midis/1374.mid': 'Liszt',\n",
       " 'midis/1375.mid': 'Beethoven',\n",
       " 'midis/1376.mid': 'Beethoven',\n",
       " 'midis/1377.mid': 'Bach',\n",
       " 'midis/1378.mid': 'Beethoven',\n",
       " 'midis/1379.mid': 'Beethoven',\n",
       " 'midis/1380.mid': 'Beethoven',\n",
       " 'midis/1381.mid': 'Beethoven',\n",
       " 'midis/1382.mid': 'Beethoven',\n",
       " 'midis/1383.mid': 'Beethoven',\n",
       " 'midis/1384.mid': 'Beethoven',\n",
       " 'midis/1385.mid': 'Beethoven',\n",
       " 'midis/1386.mid': 'Beethoven',\n",
       " 'midis/1387.mid': 'Beethoven',\n",
       " 'midis/1388.mid': 'Beethoven',\n",
       " 'midis/1389.mid': 'Beethoven',\n",
       " 'midis/1390.mid': 'Beethoven',\n",
       " 'midis/1391.mid': 'Beethoven',\n",
       " 'midis/1392.mid': 'Beethoven',\n",
       " 'midis/1393.mid': 'Beethoven',\n",
       " 'midis/1394.mid': 'Beethoven',\n",
       " 'midis/1395.mid': 'Beethoven',\n",
       " 'midis/1396.mid': 'Beethoven',\n",
       " 'midis/1397.mid': 'Beethoven',\n",
       " 'midis/1398.mid': 'Beethoven',\n",
       " 'midis/1399.mid': 'Beethoven',\n",
       " 'midis/1400.mid': 'Beethoven',\n",
       " 'midis/1401.mid': 'Beethoven',\n",
       " 'midis/1402.mid': 'Beethoven',\n",
       " 'midis/1403.mid': 'Beethoven',\n",
       " 'midis/1404.mid': 'Beethoven',\n",
       " 'midis/1405.mid': 'Beethoven',\n",
       " 'midis/1406.mid': 'Beethoven',\n",
       " 'midis/1407.mid': 'Beethoven',\n",
       " 'midis/1408.mid': 'Beethoven',\n",
       " 'midis/1409.mid': 'Bach',\n",
       " 'midis/1410.mid': 'Beethoven',\n",
       " 'midis/1411.mid': 'Beethoven',\n",
       " 'midis/1412.mid': 'Beethoven',\n",
       " 'midis/1413.mid': 'Beethoven',\n",
       " 'midis/1414.mid': 'Beethoven',\n",
       " 'midis/1415.mid': 'Beethoven',\n",
       " 'midis/1416.mid': 'Beethoven',\n",
       " 'midis/1417.mid': 'Bach',\n",
       " 'midis/1418.mid': 'Chopin',\n",
       " 'midis/1419.mid': 'Beethoven',\n",
       " 'midis/1420.mid': 'Bach',\n",
       " 'midis/1421.mid': 'Bach',\n",
       " 'midis/1422.mid': 'Beethoven',\n",
       " 'midis/1423.mid': 'Beethoven',\n",
       " 'midis/1424.mid': 'Bach',\n",
       " 'midis/1425.mid': 'Beethoven',\n",
       " 'midis/1426.mid': 'Beethoven',\n",
       " 'midis/1427.mid': 'Liszt',\n",
       " 'midis/1428.mid': 'Beethoven',\n",
       " 'midis/1429.mid': 'Bach',\n",
       " 'midis/1430.mid': 'Beethoven',\n",
       " 'midis/1431.mid': 'Bach',\n",
       " 'midis/1432.mid': 'Beethoven',\n",
       " 'midis/1433.mid': 'Beethoven',\n",
       " 'midis/1434.mid': 'Bach',\n",
       " 'midis/1435.mid': 'Beethoven',\n",
       " 'midis/1436.mid': 'Beethoven',\n",
       " 'midis/1437.mid': 'Bach',\n",
       " 'midis/1438.mid': 'Chopin',\n",
       " 'midis/1439.mid': 'Beethoven',\n",
       " 'midis/1440.mid': 'Beethoven',\n",
       " 'midis/1441.mid': 'Beethoven',\n",
       " 'midis/1442.mid': 'Bach',\n",
       " 'midis/1443.mid': 'Beethoven',\n",
       " 'midis/1444.mid': 'Chopin',\n",
       " 'midis/1445.mid': 'Beethoven',\n",
       " 'midis/1446.mid': 'Beethoven',\n",
       " 'midis/1447.mid': 'Beethoven',\n",
       " 'midis/1448.mid': 'Beethoven',\n",
       " 'midis/1449.mid': 'Beethoven',\n",
       " 'midis/1450.mid': 'Beethoven',\n",
       " 'midis/1451.mid': 'Schumann',\n",
       " 'midis/1452.mid': 'Beethoven',\n",
       " 'midis/1453.mid': 'Beethoven',\n",
       " 'midis/1454.mid': 'Beethoven',\n",
       " 'midis/1455.mid': 'Beethoven',\n",
       " 'midis/1456.mid': 'Beethoven',\n",
       " 'midis/1457.mid': 'Beethoven',\n",
       " 'midis/1458.mid': 'Beethoven',\n",
       " 'midis/1459.mid': 'Beethoven',\n",
       " 'midis/1460.mid': 'Chopin',\n",
       " 'midis/1461.mid': 'Beethoven',\n",
       " 'midis/1462.mid': 'Beethoven',\n",
       " 'midis/1463.mid': 'Beethoven',\n",
       " 'midis/1464.mid': 'Liszt',\n",
       " 'midis/1465.mid': 'Beethoven',\n",
       " 'midis/1466.mid': 'Beethoven',\n",
       " 'midis/1467.mid': 'Beethoven',\n",
       " 'midis/1468.mid': 'Beethoven',\n",
       " 'midis/1469.mid': 'Beethoven',\n",
       " 'midis/1470.mid': 'Beethoven',\n",
       " 'midis/1471.mid': 'Bach',\n",
       " 'midis/1472.mid': 'Beethoven',\n",
       " 'midis/1473.mid': 'Beethoven',\n",
       " 'midis/1474.mid': 'Beethoven',\n",
       " 'midis/1475.mid': 'Beethoven',\n",
       " 'midis/1476.mid': 'Bach',\n",
       " 'midis/1477.mid': 'Beethoven',\n",
       " 'midis/1478.mid': 'Beethoven',\n",
       " 'midis/1479.mid': 'Beethoven',\n",
       " 'midis/1480.mid': 'Beethoven',\n",
       " 'midis/1481.mid': 'Beethoven',\n",
       " 'midis/1482.mid': 'Beethoven',\n",
       " 'midis/1483.mid': 'Beethoven',\n",
       " 'midis/1484.mid': 'Beethoven',\n",
       " 'midis/1485.mid': 'Bach',\n",
       " 'midis/1486.mid': 'Beethoven',\n",
       " 'midis/1487.mid': 'Beethoven',\n",
       " 'midis/1488.mid': 'Beethoven',\n",
       " 'midis/1489.mid': 'Beethoven',\n",
       " 'midis/1490.mid': 'Beethoven',\n",
       " 'midis/1491.mid': 'Beethoven',\n",
       " 'midis/1492.mid': 'Beethoven',\n",
       " 'midis/1493.mid': 'Beethoven',\n",
       " 'midis/1494.mid': 'Bach',\n",
       " 'midis/1495.mid': 'Beethoven',\n",
       " 'midis/1496.mid': 'Beethoven',\n",
       " 'midis/1497.mid': 'Beethoven',\n",
       " 'midis/1498.mid': 'Beethoven',\n",
       " 'midis/1499.mid': 'Beethoven',\n",
       " 'midis/1500.mid': 'Beethoven',\n",
       " 'midis/1501.mid': 'Beethoven',\n",
       " 'midis/1502.mid': 'Beethoven',\n",
       " 'midis/1503.mid': 'Beethoven',\n",
       " 'midis/1504.mid': 'Beethoven',\n",
       " 'midis/1505.mid': 'Beethoven',\n",
       " 'midis/1506.mid': 'Beethoven',\n",
       " 'midis/1507.mid': 'Beethoven',\n",
       " 'midis/1508.mid': 'Beethoven',\n",
       " 'midis/1509.mid': 'Schumann',\n",
       " 'midis/1510.mid': 'Bach',\n",
       " 'midis/1511.mid': 'Beethoven',\n",
       " 'midis/1512.mid': 'Beethoven',\n",
       " 'midis/1513.mid': 'Beethoven',\n",
       " 'midis/1514.mid': 'Liszt',\n",
       " 'midis/1515.mid': 'Beethoven',\n",
       " 'midis/1516.mid': 'Beethoven',\n",
       " 'midis/1517.mid': 'Beethoven',\n",
       " 'midis/1518.mid': 'Liszt',\n",
       " 'midis/1519.mid': 'Beethoven',\n",
       " 'midis/1520.mid': 'Beethoven',\n",
       " 'midis/1521.mid': 'Beethoven',\n",
       " 'midis/1522.mid': 'Beethoven',\n",
       " 'midis/1523.mid': 'Beethoven',\n",
       " 'midis/1524.mid': 'Liszt',\n",
       " 'midis/1525.mid': 'Beethoven',\n",
       " 'midis/1526.mid': 'Chopin',\n",
       " 'midis/1527.mid': 'Beethoven',\n",
       " 'midis/1528.mid': 'Beethoven',\n",
       " 'midis/1529.mid': 'Beethoven',\n",
       " 'midis/1530.mid': 'Beethoven',\n",
       " 'midis/1531.mid': 'Beethoven',\n",
       " 'midis/1532.mid': 'Beethoven',\n",
       " 'midis/1533.mid': 'Beethoven',\n",
       " 'midis/1534.mid': 'Beethoven',\n",
       " 'midis/1535.mid': 'Bach',\n",
       " 'midis/1536.mid': 'Beethoven',\n",
       " 'midis/1537.mid': 'Beethoven',\n",
       " 'midis/1538.mid': 'Beethoven',\n",
       " 'midis/1539.mid': 'Beethoven',\n",
       " 'midis/1540.mid': 'Beethoven',\n",
       " 'midis/1541.mid': 'Beethoven',\n",
       " 'midis/1542.mid': 'Beethoven',\n",
       " 'midis/1543.mid': 'Beethoven',\n",
       " 'midis/1544.mid': 'Beethoven',\n",
       " 'midis/1545.mid': 'Beethoven',\n",
       " 'midis/1546.mid': 'Beethoven',\n",
       " 'midis/1547.mid': 'Beethoven',\n",
       " 'midis/1548.mid': 'Beethoven',\n",
       " 'midis/1549.mid': 'Beethoven',\n",
       " 'midis/1550.mid': 'Beethoven',\n",
       " 'midis/1551.mid': 'Beethoven',\n",
       " 'midis/1552.mid': 'Beethoven',\n",
       " 'midis/1553.mid': 'Chopin',\n",
       " 'midis/1554.mid': 'Beethoven',\n",
       " 'midis/1555.mid': 'Beethoven',\n",
       " 'midis/1556.mid': 'Beethoven',\n",
       " 'midis/1557.mid': 'Beethoven',\n",
       " 'midis/1558.mid': 'Beethoven',\n",
       " 'midis/1559.mid': 'Beethoven',\n",
       " 'midis/1560.mid': 'Beethoven',\n",
       " 'midis/1561.mid': 'Bach',\n",
       " 'midis/1562.mid': 'Bach',\n",
       " 'midis/1563.mid': 'Beethoven',\n",
       " 'midis/1564.mid': 'Beethoven',\n",
       " 'midis/1565.mid': 'Chopin',\n",
       " 'midis/1566.mid': 'Beethoven',\n",
       " 'midis/1567.mid': 'Chopin',\n",
       " 'midis/1568.mid': 'Beethoven',\n",
       " 'midis/1569.mid': 'Beethoven',\n",
       " 'midis/1570.mid': 'Beethoven',\n",
       " 'midis/1571.mid': 'Beethoven',\n",
       " 'midis/1572.mid': 'Beethoven',\n",
       " 'midis/1573.mid': 'Beethoven',\n",
       " 'midis/1574.mid': 'Beethoven',\n",
       " 'midis/1575.mid': 'Beethoven',\n",
       " 'midis/1576.mid': 'Bach',\n",
       " 'midis/1577.mid': 'Beethoven',\n",
       " 'midis/1578.mid': 'Beethoven',\n",
       " 'midis/1579.mid': 'Beethoven',\n",
       " 'midis/1580.mid': 'Beethoven',\n",
       " 'midis/1581.mid': 'Beethoven',\n",
       " 'midis/1582.mid': 'Bach',\n",
       " 'midis/1583.mid': 'Bach',\n",
       " 'midis/1584.mid': 'Beethoven',\n",
       " 'midis/1585.mid': 'Beethoven',\n",
       " 'midis/1586.mid': 'Beethoven',\n",
       " 'midis/1587.mid': 'Beethoven',\n",
       " 'midis/1588.mid': 'Beethoven',\n",
       " 'midis/1589.mid': 'Beethoven',\n",
       " 'midis/1590.mid': 'Beethoven',\n",
       " 'midis/1591.mid': 'Beethoven',\n",
       " 'midis/1592.mid': 'Beethoven',\n",
       " 'midis/1593.mid': 'Beethoven',\n",
       " 'midis/1594.mid': 'Beethoven',\n",
       " 'midis/1595.mid': 'Beethoven',\n",
       " 'midis/1596.mid': 'Bach',\n",
       " 'midis/1597.mid': 'Beethoven',\n",
       " 'midis/1598.mid': 'Beethoven'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(dataroot1+\"/test.json\", \"predictions1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "d6bf72f2e4f60b5c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b1eaf0dbac4c10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Task: next sequence prediction. \n",
    "Would the next midi file follow this one? \n",
    "\n",
    "baseline acc is about .6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67ad0b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:37.441244Z",
     "start_time": "2025-05-13T22:18:37.389922Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random\n",
    "import pretty_midi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d276a06fe6de94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:18:20.620368Z",
     "start_time": "2025-05-13T03:18:20.616978Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy2(groundtruth, predictions):\n",
    "    correct = 0\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        if predictions[k] == groundtruth[k]:\n",
    "            correct += 1\n",
    "    return correct / len(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244f761bd04ed087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:01.623395Z",
     "start_time": "2025-05-13T22:18:01.610438Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataroot2 = \"data/student_files/task2_next_sequence_prediction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5a30de1001f24f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:18:20.625404Z",
     "start_time": "2025-05-13T03:18:20.623702Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class model2():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def features(self, path):\n",
    "        midi_obj = miditoolkit.midi.parser.MidiFile(dataroot2 + '/' + path)\n",
    "        notes = midi_obj.instruments[0].notes\n",
    "        num_notes = len(notes)\n",
    "        average_pitch = sum([note.pitch for note in notes]) / num_notes\n",
    "        features = [average_pitch]\n",
    "        return features\n",
    "    \n",
    "    def train(self, path):\n",
    "        # This baseline doesn't use any model (it just measures feature similarity)\n",
    "        # You can use this approach but *probably* you'll want to implement a model\n",
    "        pass\n",
    "\n",
    "    def predict(self, path, outpath=None):\n",
    "        d = eval(open(path, 'r').read())\n",
    "        predictions = {}\n",
    "        for k in d:\n",
    "            path1,path2 = k # Keys are pairs of paths\n",
    "            x1 = self.features(path1)\n",
    "            x2 = self.features(path2)\n",
    "            # Note: hardcoded difference between features\n",
    "            if abs(x1[0] - x2[0]) < 5:\n",
    "                predictions[k] = True\n",
    "            else:\n",
    "                predictions[k] = False\n",
    "        if outpath:\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb78dc3ab23dee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:18:20.627904Z",
     "start_time": "2025-05-13T03:18:20.626104Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run2():\n",
    "    model = model2()\n",
    "    model.train(dataroot2 + \"/train.json\")\n",
    "    train_preds = model.predict(dataroot2 + \"/train.json\")\n",
    "    test_preds = model.predict(dataroot2 + \"/test.json\", \"predictions2.json\")\n",
    "    \n",
    "    train_labels = eval(open(dataroot2 + \"/train.json\").read())\n",
    "    acc2 = accuracy2(train_labels, train_preds)\n",
    "    print(\"Task 2 training accuracy = \" + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b478197681eaa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:18:20.635106Z",
     "start_time": "2025-05-13T03:18:20.627920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab284f47700d1c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "CUSTOM MODEL\n",
    "\n",
    "note: even basic feature similarity seems to be doing pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177d95f45251d0c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, load in data. \n",
    "Create a small dataset for overfitting, and an actual large one\n",
    "\n",
    "data is coming in as {(file1,file2):T/F,....}\n",
    "convert to X = [similarity scores, raw features1, raw features2] and y=t/f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31626f47cc79a7cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:39.893966Z",
     "start_time": "2025-05-13T22:18:39.870961Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mido import MidiFile\n",
    "\n",
    "SAMPLE_RATE = 25000\n",
    "\n",
    "# create train loader \n",
    "\n",
    "\n",
    "def get_lowest_pitch(file_path):\n",
    "    # Initialize lowest_note to a high value (since MIDI notes are from 0 to 127)\n",
    "    lowest_note = 128  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note < lowest_note:\n",
    "                    lowest_note = msg.note\n",
    "    \n",
    "    # Return None if no note is found\n",
    "    return lowest_note if lowest_note != 128 else None\n",
    "\n",
    "def get_highest_pitch(file_path):\n",
    "    # Initialize highest_note to a low value (since MIDI notes are from 0 to 127)\n",
    "    highest_note = -1  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note > highest_note:\n",
    "                    highest_note = msg.note\n",
    "                    \n",
    "    # Return None if no note is found\n",
    "    return highest_note if highest_note != -1 else None\n",
    "\n",
    "def get_unique_pitch_num(file_path):\n",
    "    mid = MidiFile(file_path)\n",
    "    notes = set()\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.add(msg.note)\n",
    "    \n",
    "    return len(notes)\n",
    "\n",
    "def get_average_pitch_value(file_path):\n",
    "    #Q8: Your code goes here\n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    notes = []\n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.append(msg.note)\n",
    "    \n",
    "    if notes:\n",
    "        return sum(notes) / len(notes)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_waveform(path):\n",
    "    # Your code here\n",
    "    wave, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    return wave \n",
    "\n",
    "def extract_q(w):\n",
    "    # Your code here\n",
    "    result = librosa.cqt(y=w, sr=SAMPLE_RATE)\n",
    "    result = librosa.amplitude_to_db(np.abs(result))\n",
    "    \n",
    "    return torch.FloatTensor(result)\n",
    "\n",
    "def extract_mfcc(w):\n",
    "    # Your code here:\n",
    "    # load using librosa.feature.mfcc\n",
    "    mfcc = librosa.feature.mfcc(y=w, sr=SAMPLE_RATE, n_mfcc = 13)\n",
    "    # extract mean and \n",
    "    means = np.mean(mfcc, axis=1)\n",
    "    \n",
    "    stds = np.std(mfcc, axis=1)\n",
    "    # concatenate\n",
    "    features = np.concatenate([means, stds])\n",
    "    \n",
    "    return torch.FloatTensor(features)\n",
    "\n",
    "def extract_spec(w):\n",
    "    # Your code here\n",
    "    # load\n",
    "    stft = librosa.stft(y=w)\n",
    "    # take squared absolute values\n",
    "    spec = np.abs(stft) ** 2\n",
    "    \n",
    "    return torch.FloatTensor(spec)\n",
    "\n",
    "\n",
    "def pad_or_truncate(spec, max_time=500):\n",
    "    freq_bins, time_bins = spec.shape\n",
    "    if time_bins > max_time:\n",
    "        return spec[:, :max_time]\n",
    "    elif time_bins < max_time:\n",
    "        pad_width = max_time - time_bins\n",
    "        return F.pad(spec, (0, pad_width), mode='constant', value=0)\n",
    "    return spec\n",
    "\n",
    "from mido import MidiFile\n",
    "\n",
    "def pitch_class_profile(path):\n",
    "    # Initialize an array to count occurrences of each pitch class (12 semitones)\n",
    "    pitch_class_counts = [0] * 12\n",
    "    \n",
    "    # Load the MIDI file\n",
    "    mid = MidiFile(path)\n",
    "    \n",
    "    # Iterate through each track in the MIDI file\n",
    "    for track in mid.tracks:\n",
    "        # Track MIDI events\n",
    "        time = 0\n",
    "        for msg in track:\n",
    "            time += msg.time  # accumulate time (not used directly here, but part of the MIDI structure)\n",
    "            \n",
    "            # Process note-on and note-off events\n",
    "            if msg.type in ['note_on', 'note_off']:\n",
    "                # We care about note_on events to track pitches\n",
    "                if msg.type == 'note_on' and msg.velocity > 0:  # Only count note_on with non-zero velocity\n",
    "                    pitch = msg.note  # MIDI note number (e.g., 60 is Middle C)\n",
    "                    # Convert to pitch class (0-11)\n",
    "                    pitch_class = pitch % 12\n",
    "                    pitch_class_counts[pitch_class] += 1\n",
    "    \n",
    "    # Normalize the pitch class counts to get a distribution\n",
    "    total_notes = sum(pitch_class_counts)\n",
    "    if total_notes > 0:\n",
    "        pitch_class_profile = [count / total_notes for count in pitch_class_counts]\n",
    "    else:\n",
    "        pitch_class_profile = [0] * 12  # If no notes are found, return an all-zero vector\n",
    "    \n",
    "    \n",
    "    return torch.tensor(pitch_class_profile)\n",
    "\n",
    "\n",
    "def get_key_signature(path):\n",
    "    # Load the MIDI file\n",
    "    midi = MidiFile(path)\n",
    "    \n",
    "    # Iterate through all tracks in the MIDI file\n",
    "    for track in midi.tracks:\n",
    "        for msg in track:\n",
    "            # Look for meta events of type 'key_signature'\n",
    "            if msg.type == 'key_signature':\n",
    "                # Extract the key and mode from the key_signature meta message\n",
    "                key = msg.key\n",
    "                \n",
    "                return key  # Return key and mode\n",
    "    \n",
    "    # Return None if no key_signature meta message was found\n",
    "    print('no key found')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e8a100d11755964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:40.672732Z",
     "start_time": "2025-05-13T22:18:40.663134Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features(path):\n",
    "    full_path = dataroot2 + '/' + path\n",
    "    try:\n",
    "        midi_obj = pretty_midi.PrettyMIDI(full_path)\n",
    "        w = midi_obj.fluidsynth()\n",
    "\n",
    "        mfcc = extract_mfcc(w)\n",
    "        spec = extract_spec(w).mean(dim=1)\n",
    "        q = extract_q(w).mean(dim=1)\n",
    "        pitch_distribution = pitch_class_profile(full_path)\n",
    "        key_signature = get_key_signature(full_path)\n",
    "\n",
    "        \n",
    "        # want things like... \n",
    "        # key signature\n",
    "        # pitch interval analysis -> average size of intervals between notes\n",
    "        \n",
    "        \n",
    "        extra = torch.tensor([\n",
    "            get_lowest_pitch(full_path) or 0,\n",
    "            get_highest_pitch(full_path) or 0,\n",
    "            get_unique_pitch_num(full_path),\n",
    "            get_average_pitch_value(full_path) or 0\n",
    "        ], dtype=torch.float64)\n",
    "\n",
    "        # return torch.cat([mfcc, spec, q, extra])\n",
    "        return mfcc, spec, q, pitch_distribution, key_signature, extra\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {full_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab5a792a3b1e769b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:41.264335Z",
     "start_time": "2025-05-13T22:18:41.255493Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_features():\n",
    "    path = 'midis/0.mid'\n",
    "    feats = features(path)\n",
    "    \n",
    "    if feats is None:\n",
    "        print(\"Feature extraction failed.\")\n",
    "        return\n",
    "\n",
    "    mfcc, spec, q, pitch_dist, key_sig, extra = feats\n",
    "\n",
    "    print(\"MFCC shape:\", mfcc.shape)\n",
    "    print(\"Spectrogram shape:\", spec.shape)\n",
    "    print(\"Q-Transform shape:\", q.shape)\n",
    "    print(\"Pitch distribution shape:\", pitch_dist.shape)\n",
    "    print(\"Key signature:\", key_sig)\n",
    "    print(\"Extra shape:\", extra.shape)\n",
    "\n",
    "    # Basic sanity checks (can be modified to assert instead of print if desired)\n",
    "    assert isinstance(mfcc, torch.Tensor)\n",
    "    assert isinstance(spec, torch.Tensor)\n",
    "    assert isinstance(q, torch.Tensor)\n",
    "    assert isinstance(pitch_dist, torch.Tensor)\n",
    "    assert isinstance(extra, torch.Tensor)\n",
    "    assert extra.shape[0] == 4\n",
    "\n",
    "    print(\"All feature checks passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed23e427b9e289e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:43.477610Z",
     "start_time": "2025-05-13T22:18:42.019126Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape: torch.Size([26])\n",
      "Spectrogram shape: torch.Size([1025])\n",
      "Q-Transform shape: torch.Size([84])\n",
      "Pitch distribution shape: torch.Size([12])\n",
      "Key signature: F#\n",
      "Extra shape: torch.Size([4])\n",
      "All feature checks passed.\n"
     ]
    }
   ],
   "source": [
    "test_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c8bfce6fb8513b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:47.909475Z",
     "start_time": "2025-05-13T22:18:47.866039Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.spatial.distance import euclidean\n",
    "import ast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import wasserstein_distance\n",
    "from fastdtw import fastdtw\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "\n",
    "\n",
    "X_data_small, Y_data_small = [],[]\n",
    "X_data, Y_data = [], []\n",
    "\n",
    "\n",
    "def process_pair_predict(tuple):\n",
    "    \"\"\"Process a single pair of files and compute similarity metrics\"\"\"\n",
    "    file1, file2 = str(tuple[0]), str(tuple[1])\n",
    "    \n",
    "    # Extract features for each file\n",
    "    mfcc1, spec1, q1, pitch1, key1, extra1 = features(file1)\n",
    "    mfcc2, spec2, q2, pitch2, key2, extra2 = features(file2)\n",
    "    \n",
    "    # Calculate similarity metrics\n",
    "    sim_mfcc = euclidean(mfcc1, mfcc2)\n",
    "    sim_spec = cosine_similarity(mfcc1.reshape(1, -1), mfcc2.reshape(1, -1))[0][0]\n",
    "    sim_pitch = wasserstein_distance(pitch1, pitch2)\n",
    "    sim_key = 1 if str(key1) == str(key2) else 0\n",
    "    sim_extra = cosine_similarity(extra1.reshape(1, -1), extra2.reshape(1, -1))[0][0]\n",
    "    \n",
    "    # Create feature vector\n",
    "    features_vector = [sim_mfcc, sim_spec, sim_pitch, sim_key, sim_extra]\n",
    "    \n",
    "    # Add statistical features\n",
    "    features_vector.extend([mfcc1.mean(), mfcc2.mean(), mfcc1.std(), mfcc2.std(),\n",
    "                           spec1.mean(), spec2.mean(), spec1.std(), spec2.std()])\n",
    "    \n",
    "    # Add extra features\n",
    "    features_vector.extend(extra1.tolist())\n",
    "    features_vector.extend(extra2.tolist())\n",
    "    \n",
    "    return features_vector\n",
    "\n",
    "def process_pair(key_value_pair, features_func):\n",
    "    \"\"\"Process a single pair of files and compute similarity metrics\"\"\"\n",
    "    key, value = key_value_pair\n",
    "    file1, file2 = str(key[0]), str(key[1])\n",
    "    y = value\n",
    "    \n",
    "    # Extract features for each file\n",
    "    mfcc1, spec1, q1, pitch1, key1, extra1 = features_func(file1)\n",
    "    mfcc2, spec2, q2, pitch2, key2, extra2 = features_func(file2)\n",
    "    \n",
    "    # Calculate similarity metrics\n",
    "    sim_mfcc = euclidean(mfcc1, mfcc2)\n",
    "    sim_spec = cosine_similarity(mfcc1.reshape(1, -1), mfcc2.reshape(1, -1))[0][0]\n",
    "    sim_pitch = wasserstein_distance(pitch1, pitch2)\n",
    "    sim_key = 1 if str(key1) == str(key2) else 0\n",
    "    sim_extra = cosine_similarity(extra1.reshape(1, -1), extra2.reshape(1, -1))[0][0]\n",
    "    \n",
    "    # Create feature vector\n",
    "    features_vector = [sim_mfcc, sim_spec, sim_pitch, sim_key, sim_extra]\n",
    "    \n",
    "    # Add statistical features\n",
    "    features_vector.extend([mfcc1.mean(), mfcc2.mean(), mfcc1.std(), mfcc2.std(),\n",
    "                           spec1.mean(), spec2.mean(), spec1.std(), spec2.std()])\n",
    "    \n",
    "    # Add extra features\n",
    "    features_vector.extend(extra1.tolist())\n",
    "    features_vector.extend(extra2.tolist())\n",
    "    \n",
    "    return features_vector, y\n",
    "\n",
    "def create_datasets(size=None, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Create datasets with parallelized processing\n",
    "    \n",
    "    Args:\n",
    "        size: Optional limit on number of samples to process\n",
    "        n_jobs: Number of parallel jobs to run\n",
    "        \n",
    "    Returns:\n",
    "        X_data, Y_data: Features and labels\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    with open(dataroot2 + \"/train.json\", 'r') as f:\n",
    "        data = ast.literal_eval(f.read())\n",
    "    \n",
    "    # Shuffle and crop data if size limit\n",
    "    if size:\n",
    "        items = list(data.items())\n",
    "        random.shuffle(items)\n",
    "        items = items[:size]\n",
    "        data = dict(items)\n",
    "    \n",
    "    # Create list of key-value pairs for parallel processing\n",
    "    items = list(data.items())\n",
    "    \n",
    "    # Create a partial function with the features function\n",
    "    process_func = partial(process_pair, features_func=features)\n",
    "    \n",
    "    # Process pairs in parallel\n",
    "    # Process pairs in parallel with progress bar\n",
    "    print(f\"Processing {len(items)} file pairs using {n_jobs} parallel jobs...\")\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_func)(item) for item in tqdm.tqdm(items, desc=\"Processing file pairs\")\n",
    "    )\n",
    "    \n",
    "    # Count and filter failed results\n",
    "    num_failed = sum(r is None for r in results)\n",
    "    results = [r for r in results if r is not None]\n",
    "    \n",
    "    print(f\"{num_failed} items failed to process and were skipped.\")\n",
    "    \n",
    "    # Split results into X and Y\n",
    "    X_data, Y_data = zip(*results) if results else ([], [])\n",
    "    \n",
    "    # Print class distribution\n",
    "    positive_count = sum(Y_data)\n",
    "    negative_count = len(Y_data) - positive_count\n",
    "    print(f\"Positive examples: {positive_count}, Negative examples: {negative_count}\")\n",
    "    \n",
    "    return list(X_data), list(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cacf02827f53e2cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:18:55.580161Z",
     "start_time": "2025-05-13T22:18:52.494558Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"task2_train_data.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_data = data['x']\n",
    "y_data = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9d4e99dbaedb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:26:49.344437Z",
     "start_time": "2025-05-13T03:18:20.770870Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9558 file pairs using 4 parallel jobs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8bff0ada8c41f8b59ca9a8f57d6278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file pairs:   0%|          | 0/9558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 items failed to process and were skipped.\n",
      "Positive examples: 4779, Negative examples: 4779\n",
      "Feature size: 21\n",
      "Data size: 9558\n"
     ]
    }
   ],
   "source": [
    "# X_data, y_data = create_datasets()  # uncomment to re-run features on training data\n",
    "feature_size = len(X_data[0])\n",
    "print(f\"Feature size: {feature_size}\")\n",
    "print(f\"Data size: {len(X_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2923dcc5516702e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T21:31:34.404175Z",
     "start_time": "2025-05-13T21:31:31.523827Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "import pickle\n",
    "\n",
    "train_data_dict = {'x': X_data, 'y': y_data}\n",
    "\n",
    "with open(\"task2_train_data.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_data_dict, file)\n",
    "\n",
    "# check if pickle file exists. if not, run create_datasets and save to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76208fb91a062ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:19:46.602978Z",
     "start_time": "2025-05-13T22:19:46.585805Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_size = len(X_data[0])\n",
    "feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac44ab9e0738a295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:19:06.525894Z",
     "start_time": "2025-05-13T22:19:06.517946Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation distribution split: 0.49215481171548114\n",
      "training distribution split: 0.5019618100967826\n"
     ]
    }
   ],
   "source": [
    "# Zip and shuffle together to maintain correspondence\n",
    "combined = list(zip(X_data, y_data))\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Split index for 80/20\n",
    "split_idx = int(0.8 * len(combined))\n",
    "\n",
    "# Unzip into train and val sets\n",
    "train_data = combined[:split_idx]\n",
    "val_data = combined[split_idx:]\n",
    "\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_val, y_val = zip(*val_data)\n",
    "\n",
    "# Optionally convert back to lists\n",
    "X_train, y_train = list(X_train), list(y_train)\n",
    "X_val, y_val = list(X_val), list(y_val)\n",
    "    \n",
    "# check that the distribution is still 50-50 between true and false \n",
    "print('validation distribution split:', sum(y_val) / len(y_val))\n",
    "print('training distribution split:', sum(y_train) / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90c814270e85ccfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:19:08.388953Z",
     "start_time": "2025-05-13T22:19:08.309427Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int)  # or float if doing regression\n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d1e903841e42c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:19:09.543388Z",
     "start_time": "2025-05-13T22:19:09.507127Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931f234a9cc5fbb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model Business\n",
    "Create model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea3c63fca9c93299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:19:49.512564Z",
     "start_time": "2025-05-13T22:19:49.477754Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=feature_size, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.__init_args__ = (input_dim,)\n",
    "        self.__init_kwargs__ = {'num_classes': num_classes}\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Feature processing network\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for feature weighting\n",
    "        # self.attention = nn.Sequential(\n",
    "        #     nn.Linear(128, 256),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(256, 1)\n",
    "        # )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Linear(512, 256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process features\n",
    "        features = self.feature_net(x)\n",
    "        \n",
    "        # Apply attention (optional branch)\n",
    "        # attention_weights = torch.sigmoid(self.attention(features))\n",
    "        # weighted_features = features * attention_weights\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "        \n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Return intermediate feature representation\"\"\"\n",
    "        return self.feature_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faecdbd673ea422b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:19:53.292323Z",
     "start_time": "2025-05-13T22:19:53.257723Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model, filepath='sol_2.pt'):\n",
    "    \"\"\"Save a PyTorch model to a file\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model_class, filepath='sol_2.pt', *args, **kwargs):\n",
    "    \"\"\"Load a PyTorch model from a file\"\"\"\n",
    "    model = model_class(*args, **kwargs)  # instantiate the model\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # optional: sets dropout/batchnorm to eval mode\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b096fb7cae4838d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Process Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "814b9756787bcb15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T21:48:51.951319Z",
     "start_time": "2025-05-13T21:48:51.903459Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to process test inputs\n",
    "def process_test_data(path, n_jobs=4):\n",
    "    print(\"Loading test data...\")\n",
    "    with open(path, 'r') as f:\n",
    "        data = ast.literal_eval(f.read())\n",
    "    \n",
    "    print(f\"Total test samples: {len(data)}\")\n",
    "\n",
    "    # Parallelize feature processing\n",
    "    print(f\"Extracting features in parallel using {n_jobs} workers...\")\n",
    "    process_func = partial(process_pair_predict)\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_func)(data) for data in tqdm.tqdm(data, desc=\"Processing test pairs\")\n",
    "    )\n",
    "    \n",
    "    # Filter out failed results\n",
    "    results = [r for r in results if r is not None]\n",
    "    if len(results) < len(data):\n",
    "        print(f\"Warning: {len(data) - len(results)} samples failed during feature extraction.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2120237a35f117a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T21:52:14.938797Z",
     "start_time": "2025-05-13T21:49:01.972682Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Total test samples: 3070\n",
      "Extracting features in parallel using 4 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test pairs:   0%|          | 0/3070 [00:00<?, ?it/s]\u001B[A\n",
      "Processing test pairs:   0%|          | 8/3070 [00:03<22:54,  2.23it/s]\u001B[A\n",
      "Processing test pairs:   0%|          | 12/3070 [00:03<14:54,  3.42it/s]\u001B[A\n",
      "Processing test pairs:   1%|          | 16/3070 [00:04<10:22,  4.91it/s]\u001B[A\n",
      "Processing test pairs:   1%|          | 20/3070 [00:04<08:01,  6.34it/s]\u001B[A\n",
      "Processing test pairs:   1%|          | 24/3070 [00:04<06:05,  8.33it/s]\u001B[A\n",
      "Processing test pairs:   1%|          | 28/3070 [00:04<05:18,  9.56it/s]\u001B[A\n",
      "Processing test pairs:   1%|          | 32/3070 [00:05<05:17,  9.58it/s]\u001B[A\n",
      "Processing test pairs:   1%|          | 36/3070 [00:05<04:41, 10.79it/s]\u001B[A\n",
      "Processing test pairs:   1%|▏         | 40/3070 [00:05<04:18, 11.72it/s]\u001B[A\n",
      "Processing test pairs:   1%|▏         | 44/3070 [00:06<04:04, 12.36it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 48/3070 [00:06<03:54, 12.89it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 52/3070 [00:06<03:31, 14.25it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 56/3070 [00:06<03:08, 16.02it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 60/3070 [00:06<02:59, 16.81it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 64/3070 [00:07<03:06, 16.09it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 68/3070 [00:07<02:55, 17.13it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 72/3070 [00:07<02:55, 17.04it/s]\u001B[A\n",
      "Processing test pairs:   2%|▏         | 76/3070 [00:07<02:42, 18.47it/s]\u001B[A\n",
      "Processing test pairs:   3%|▎         | 80/3070 [00:08<02:36, 19.12it/s]\u001B[A\n",
      "Processing test pairs:   3%|▎         | 84/3070 [00:08<02:28, 20.15it/s]\u001B[A\n",
      "Processing test pairs:   3%|▎         | 88/3070 [00:08<02:38, 18.78it/s]\u001B[A\n",
      "Processing test pairs:   3%|▎         | 92/3070 [00:08<03:00, 16.52it/s]\u001B[A\n",
      "Processing test pairs:   3%|▎         | 96/3070 [00:08<02:36, 18.99it/s]\u001B[A\n",
      "Processing test pairs:   3%|▎         | 100/3070 [00:09<02:48, 17.59it/s]\u001B[A\n",
      "Processing test pairs:   3%|▎         | 104/3070 [00:09<03:22, 14.67it/s]\u001B[A\n",
      "Processing test pairs:   4%|▎         | 108/3070 [00:10<04:00, 12.29it/s]\u001B[A\n",
      "Processing test pairs:   4%|▎         | 112/3070 [00:10<04:25, 11.15it/s]\u001B[A\n",
      "Processing test pairs:   4%|▍         | 116/3070 [00:10<04:24, 11.16it/s]\u001B[A\n",
      "Processing test pairs:   4%|▍         | 120/3070 [00:11<05:07,  9.60it/s]\u001B[A\n",
      "Processing test pairs:   4%|▍         | 124/3070 [00:11<04:53, 10.05it/s]\u001B[A\n",
      "Processing test pairs:   4%|▍         | 128/3070 [00:11<04:06, 11.92it/s]\u001B[A\n",
      "Processing test pairs:   4%|▍         | 132/3070 [00:12<03:29, 13.99it/s]\u001B[A\n",
      "Processing test pairs:   4%|▍         | 136/3070 [00:12<03:09, 15.49it/s]\u001B[A\n",
      "Processing test pairs:   5%|▍         | 140/3070 [00:12<02:45, 17.74it/s]\u001B[A\n",
      "Processing test pairs:   5%|▍         | 144/3070 [00:12<02:46, 17.59it/s]\u001B[A\n",
      "Processing test pairs:   5%|▍         | 148/3070 [00:12<02:42, 18.02it/s]\u001B[A\n",
      "Processing test pairs:   5%|▍         | 152/3070 [00:13<02:40, 18.20it/s]\u001B[A\n",
      "Processing test pairs:   5%|▌         | 156/3070 [00:13<02:39, 18.23it/s]\u001B[A\n",
      "Processing test pairs:   5%|▌         | 160/3070 [00:13<02:28, 19.56it/s]\u001B[A\n",
      "Processing test pairs:   5%|▌         | 164/3070 [00:13<02:47, 17.37it/s]\u001B[A\n",
      "Processing test pairs:   5%|▌         | 168/3070 [00:13<02:45, 17.58it/s]\u001B[A\n",
      "Processing test pairs:   6%|▌         | 172/3070 [00:14<03:09, 15.32it/s]\u001B[A\n",
      "Processing test pairs:   6%|▌         | 176/3070 [00:14<02:58, 16.24it/s]\u001B[A\n",
      "Processing test pairs:   6%|▌         | 180/3070 [00:14<02:53, 16.63it/s]\u001B[A\n",
      "Processing test pairs:   6%|▌         | 184/3070 [00:14<02:49, 16.98it/s]\u001B[A\n",
      "Processing test pairs:   6%|▌         | 188/3070 [00:15<02:42, 17.73it/s]\u001B[A\n",
      "Processing test pairs:   6%|▋         | 192/3070 [00:15<02:27, 19.56it/s]\u001B[A\n",
      "Processing test pairs:   6%|▋         | 196/3070 [00:15<02:36, 18.34it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 200/3070 [00:15<02:42, 17.65it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 204/3070 [00:16<02:50, 16.83it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 208/3070 [00:16<02:46, 17.16it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 212/3070 [00:16<02:44, 17.42it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 216/3070 [00:16<02:48, 16.95it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 220/3070 [00:16<02:40, 17.79it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 224/3070 [00:17<02:41, 17.59it/s]\u001B[A\n",
      "Processing test pairs:   7%|▋         | 228/3070 [00:17<02:39, 17.85it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 232/3070 [00:17<02:23, 19.74it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 236/3070 [00:17<02:21, 20.09it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 240/3070 [00:18<02:25, 19.44it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 244/3070 [00:18<02:07, 22.19it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 248/3070 [00:18<02:28, 18.96it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 252/3070 [00:18<02:28, 19.01it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 256/3070 [00:18<02:23, 19.56it/s]\u001B[A\n",
      "Processing test pairs:   8%|▊         | 260/3070 [00:19<02:32, 18.41it/s]\u001B[A\n",
      "Processing test pairs:   9%|▊         | 264/3070 [00:19<02:37, 17.82it/s]\u001B[A\n",
      "Processing test pairs:   9%|▊         | 268/3070 [00:19<02:38, 17.72it/s]\u001B[A\n",
      "Processing test pairs:   9%|▉         | 272/3070 [00:19<02:32, 18.38it/s]\u001B[A\n",
      "Processing test pairs:   9%|▉         | 276/3070 [00:19<02:19, 20.07it/s]\u001B[A\n",
      "Processing test pairs:   9%|▉         | 280/3070 [00:20<02:29, 18.66it/s]\u001B[A\n",
      "Processing test pairs:   9%|▉         | 284/3070 [00:20<02:27, 18.94it/s]\u001B[A\n",
      "Processing test pairs:   9%|▉         | 288/3070 [00:20<02:18, 20.14it/s]\u001B[A\n",
      "Processing test pairs:  10%|▉         | 292/3070 [00:20<02:26, 18.97it/s]\u001B[A\n",
      "Processing test pairs:  10%|▉         | 296/3070 [00:21<02:47, 16.52it/s]\u001B[A\n",
      "Processing test pairs:  10%|▉         | 300/3070 [00:21<02:51, 16.12it/s]\u001B[A\n",
      "Processing test pairs:  10%|▉         | 304/3070 [00:21<02:50, 16.23it/s]\u001B[A\n",
      "Processing test pairs:  10%|█         | 308/3070 [00:21<02:51, 16.12it/s]\u001B[A\n",
      "Processing test pairs:  10%|█         | 312/3070 [00:22<02:45, 16.63it/s]\u001B[A\n",
      "Processing test pairs:  10%|█         | 316/3070 [00:22<02:34, 17.82it/s]\u001B[A\n",
      "Processing test pairs:  10%|█         | 320/3070 [00:22<02:29, 18.41it/s]\u001B[A\n",
      "Processing test pairs:  11%|█         | 324/3070 [00:22<02:32, 18.05it/s]\u001B[A\n",
      "Processing test pairs:  11%|█         | 328/3070 [00:22<02:28, 18.52it/s]\u001B[A\n",
      "Processing test pairs:  11%|█         | 332/3070 [00:23<02:33, 17.87it/s]\u001B[A\n",
      "Processing test pairs:  11%|█         | 336/3070 [00:23<02:28, 18.45it/s]\u001B[A\n",
      "Processing test pairs:  11%|█         | 340/3070 [00:23<02:24, 18.88it/s]\u001B[A\n",
      "Processing test pairs:  11%|█         | 344/3070 [00:23<02:34, 17.70it/s]\u001B[A\n",
      "Processing test pairs:  11%|█▏        | 348/3070 [00:23<02:29, 18.25it/s]\u001B[A\n",
      "Processing test pairs:  11%|█▏        | 352/3070 [00:24<02:27, 18.46it/s]\u001B[A\n",
      "Processing test pairs:  12%|█▏        | 356/3070 [00:24<02:22, 19.02it/s]\u001B[A\n",
      "Processing test pairs:  12%|█▏        | 360/3070 [00:24<02:20, 19.27it/s]\u001B[A\n",
      "Processing test pairs:  12%|█▏        | 364/3070 [00:24<02:12, 20.39it/s]\u001B[A\n",
      "Processing test pairs:  12%|█▏        | 368/3070 [00:24<02:15, 19.95it/s]\u001B[A\n",
      "Processing test pairs:  12%|█▏        | 372/3070 [00:25<02:10, 20.60it/s]\u001B[A\n",
      "Processing test pairs:  12%|█▏        | 376/3070 [00:25<02:09, 20.74it/s]\u001B[A\n",
      "Processing test pairs:  12%|█▏        | 380/3070 [00:25<02:11, 20.51it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 384/3070 [00:25<02:18, 19.34it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 388/3070 [00:25<02:14, 19.90it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 392/3070 [00:26<02:17, 19.46it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 396/3070 [00:26<03:00, 14.82it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 400/3070 [00:26<02:48, 15.85it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 404/3070 [00:27<02:53, 15.36it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 408/3070 [00:27<02:36, 17.03it/s]\u001B[A\n",
      "Processing test pairs:  13%|█▎        | 412/3070 [00:27<02:38, 16.75it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▎        | 416/3070 [00:27<02:20, 18.95it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▎        | 420/3070 [00:27<02:21, 18.78it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▍        | 424/3070 [00:28<02:29, 17.68it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▍        | 428/3070 [00:28<02:36, 16.87it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▍        | 432/3070 [00:28<02:47, 15.73it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▍        | 436/3070 [00:28<02:54, 15.06it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▍        | 440/3070 [00:29<02:50, 15.40it/s]\u001B[A\n",
      "Processing test pairs:  14%|█▍        | 444/3070 [00:29<02:55, 14.97it/s]\u001B[A\n",
      "Processing test pairs:  15%|█▍        | 448/3070 [00:29<02:59, 14.59it/s]\u001B[A\n",
      "Processing test pairs:  15%|█▍        | 452/3070 [00:29<02:41, 16.17it/s]\u001B[A\n",
      "Processing test pairs:  15%|█▍        | 456/3070 [00:30<02:49, 15.43it/s]\u001B[A\n",
      "Processing test pairs:  15%|█▍        | 460/3070 [00:30<02:49, 15.38it/s]\u001B[A\n",
      "Processing test pairs:  15%|█▌        | 464/3070 [00:30<02:41, 16.13it/s]\u001B[A\n",
      "Processing test pairs:  15%|█▌        | 468/3070 [00:30<02:39, 16.36it/s]\u001B[A\n",
      "Processing test pairs:  15%|█▌        | 472/3070 [00:31<02:41, 16.11it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▌        | 476/3070 [00:31<02:56, 14.67it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▌        | 480/3070 [00:31<02:47, 15.50it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▌        | 484/3070 [00:31<02:34, 16.69it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▌        | 488/3070 [00:32<02:56, 14.66it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▌        | 492/3070 [00:32<02:40, 16.03it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▌        | 496/3070 [00:32<02:38, 16.20it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▋        | 500/3070 [00:33<02:53, 14.84it/s]\u001B[A\n",
      "Processing test pairs:  16%|█▋        | 504/3070 [00:33<02:36, 16.38it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 508/3070 [00:33<02:31, 16.87it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 512/3070 [00:33<02:25, 17.55it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 516/3070 [00:34<02:58, 14.28it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 520/3070 [00:34<03:08, 13.50it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 524/3070 [00:34<02:56, 14.42it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 528/3070 [00:34<02:59, 14.17it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 532/3070 [00:35<03:00, 14.07it/s]\u001B[A\n",
      "Processing test pairs:  17%|█▋        | 536/3070 [00:35<02:43, 15.51it/s]\u001B[A\n",
      "Processing test pairs:  18%|█▊        | 540/3070 [00:35<02:34, 16.41it/s]\u001B[A\n",
      "Processing test pairs:  18%|█▊        | 544/3070 [00:35<02:26, 17.28it/s]\u001B[A\n",
      "Processing test pairs:  18%|█▊        | 548/3070 [00:36<02:25, 17.38it/s]\u001B[A\n",
      "Processing test pairs:  18%|█▊        | 552/3070 [00:36<02:19, 18.01it/s]\u001B[A\n",
      "Processing test pairs:  18%|█▊        | 556/3070 [00:36<02:14, 18.66it/s]\u001B[A\n",
      "Processing test pairs:  18%|█▊        | 560/3070 [00:36<02:08, 19.47it/s]\u001B[A\n",
      "Processing test pairs:  18%|█▊        | 564/3070 [00:36<01:54, 21.80it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▊        | 568/3070 [00:37<02:05, 19.88it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▊        | 572/3070 [00:37<02:14, 18.59it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▉        | 576/3070 [00:37<02:25, 17.15it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▉        | 580/3070 [00:37<02:26, 16.94it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▉        | 584/3070 [00:38<02:40, 15.51it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▉        | 588/3070 [00:38<02:44, 15.06it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▉        | 592/3070 [00:38<02:35, 15.93it/s]\u001B[A\n",
      "Processing test pairs:  19%|█▉        | 596/3070 [00:38<02:43, 15.13it/s]\u001B[A\n",
      "Processing test pairs:  20%|█▉        | 600/3070 [00:39<02:35, 15.89it/s]\u001B[A\n",
      "Processing test pairs:  20%|█▉        | 604/3070 [00:39<02:36, 15.71it/s]\u001B[A\n",
      "Processing test pairs:  20%|█▉        | 608/3070 [00:39<02:35, 15.85it/s]\u001B[A\n",
      "Processing test pairs:  20%|█▉        | 612/3070 [00:39<02:34, 15.95it/s]\u001B[A\n",
      "Processing test pairs:  20%|██        | 616/3070 [00:40<02:35, 15.83it/s]\u001B[A\n",
      "Processing test pairs:  20%|██        | 620/3070 [00:40<02:40, 15.22it/s]\u001B[A\n",
      "Processing test pairs:  20%|██        | 624/3070 [00:40<02:36, 15.67it/s]\u001B[A\n",
      "Processing test pairs:  20%|██        | 628/3070 [00:40<02:44, 14.88it/s]\u001B[A\n",
      "Processing test pairs:  21%|██        | 632/3070 [00:41<02:44, 14.82it/s]\u001B[A\n",
      "Processing test pairs:  21%|██        | 636/3070 [00:41<02:37, 15.46it/s]\u001B[A\n",
      "Processing test pairs:  21%|██        | 640/3070 [00:41<02:31, 15.99it/s]\u001B[A\n",
      "Processing test pairs:  21%|██        | 644/3070 [00:41<02:09, 18.81it/s]\u001B[A\n",
      "Processing test pairs:  21%|██        | 648/3070 [00:42<02:27, 16.45it/s]\u001B[A\n",
      "Processing test pairs:  21%|██        | 652/3070 [00:42<02:22, 16.93it/s]\u001B[A\n",
      "Processing test pairs:  21%|██▏       | 656/3070 [00:42<02:10, 18.48it/s]\u001B[A\n",
      "Processing test pairs:  21%|██▏       | 660/3070 [00:42<01:52, 21.41it/s]\u001B[A\n",
      "Processing test pairs:  22%|██▏       | 664/3070 [00:42<01:59, 20.11it/s]\u001B[A\n",
      "Processing test pairs:  22%|██▏       | 668/3070 [00:43<02:00, 19.91it/s]\u001B[A\n",
      "Processing test pairs:  22%|██▏       | 672/3070 [00:43<01:55, 20.77it/s]\u001B[A\n",
      "Processing test pairs:  22%|██▏       | 676/3070 [00:43<01:51, 21.40it/s]\u001B[A\n",
      "Processing test pairs:  22%|██▏       | 680/3070 [00:43<02:03, 19.32it/s]\u001B[A\n",
      "Processing test pairs:  22%|██▏       | 684/3070 [00:43<02:11, 18.18it/s]\u001B[A\n",
      "Processing test pairs:  22%|██▏       | 688/3070 [00:44<02:10, 18.32it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 692/3070 [00:44<02:10, 18.17it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 696/3070 [00:44<02:14, 17.60it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 700/3070 [00:44<02:12, 17.89it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 704/3070 [00:45<02:10, 18.19it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 708/3070 [00:45<02:03, 19.11it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 712/3070 [00:45<02:08, 18.41it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 716/3070 [00:45<02:06, 18.65it/s]\u001B[A\n",
      "Processing test pairs:  23%|██▎       | 720/3070 [00:45<02:04, 18.83it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▎       | 724/3070 [00:46<02:12, 17.68it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▎       | 728/3070 [00:46<02:17, 17.00it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▍       | 732/3070 [00:46<02:21, 16.57it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▍       | 736/3070 [00:46<02:24, 16.15it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▍       | 740/3070 [00:47<02:15, 17.14it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▍       | 744/3070 [00:47<02:21, 16.39it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▍       | 748/3070 [00:47<02:21, 16.46it/s]\u001B[A\n",
      "Processing test pairs:  24%|██▍       | 752/3070 [00:47<01:57, 19.75it/s]\u001B[A\n",
      "Processing test pairs:  25%|██▍       | 756/3070 [00:47<01:57, 19.62it/s]\u001B[A\n",
      "Processing test pairs:  25%|██▍       | 760/3070 [00:48<02:03, 18.74it/s]\u001B[A\n",
      "Processing test pairs:  25%|██▍       | 764/3070 [00:48<01:58, 19.40it/s]\u001B[A\n",
      "Processing test pairs:  25%|██▌       | 768/3070 [00:48<02:15, 16.93it/s]\u001B[A\n",
      "Processing test pairs:  25%|██▌       | 772/3070 [00:48<02:13, 17.26it/s]\u001B[A\n",
      "Processing test pairs:  25%|██▌       | 776/3070 [00:49<02:33, 14.96it/s]\u001B[A\n",
      "Processing test pairs:  25%|██▌       | 780/3070 [00:49<03:10, 12.00it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▌       | 784/3070 [00:49<02:56, 12.93it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▌       | 788/3070 [00:50<02:47, 13.65it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▌       | 792/3070 [00:50<02:25, 15.70it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▌       | 796/3070 [00:50<02:18, 16.39it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▌       | 800/3070 [00:50<02:18, 16.43it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▌       | 804/3070 [00:51<02:11, 17.26it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▋       | 808/3070 [00:51<02:07, 17.70it/s]\u001B[A\n",
      "Processing test pairs:  26%|██▋       | 812/3070 [00:51<02:09, 17.39it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 816/3070 [00:51<02:23, 15.69it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 820/3070 [00:52<02:27, 15.29it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 824/3070 [00:52<02:20, 15.93it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 828/3070 [00:52<02:09, 17.31it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 832/3070 [00:52<02:11, 17.02it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 836/3070 [00:53<02:18, 16.14it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 840/3070 [00:53<02:13, 16.76it/s]\u001B[A\n",
      "Processing test pairs:  27%|██▋       | 844/3070 [00:53<02:07, 17.46it/s]\u001B[A\n",
      "Processing test pairs:  28%|██▊       | 848/3070 [00:53<02:03, 18.01it/s]\u001B[A\n",
      "Processing test pairs:  28%|██▊       | 852/3070 [00:53<02:03, 17.91it/s]\u001B[A\n",
      "Processing test pairs:  28%|██▊       | 856/3070 [00:54<02:11, 16.87it/s]\u001B[A\n",
      "Processing test pairs:  28%|██▊       | 860/3070 [00:54<02:03, 17.87it/s]\u001B[A\n",
      "Processing test pairs:  28%|██▊       | 864/3070 [00:54<01:46, 20.73it/s]\u001B[A\n",
      "Processing test pairs:  28%|██▊       | 868/3070 [00:54<01:53, 19.48it/s]\u001B[A\n",
      "Processing test pairs:  28%|██▊       | 872/3070 [00:55<02:09, 16.92it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▊       | 876/3070 [00:55<02:22, 15.42it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▊       | 880/3070 [00:55<02:28, 14.77it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▉       | 884/3070 [00:55<02:26, 14.92it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▉       | 888/3070 [00:56<02:16, 15.93it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▉       | 892/3070 [00:56<02:31, 14.37it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▉       | 896/3070 [00:56<02:52, 12.58it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▉       | 900/3070 [00:57<02:47, 12.97it/s]\u001B[A\n",
      "Processing test pairs:  29%|██▉       | 904/3070 [00:57<02:32, 14.21it/s]\u001B[A\n",
      "Processing test pairs:  30%|██▉       | 908/3070 [00:57<02:22, 15.14it/s]\u001B[A\n",
      "Processing test pairs:  30%|██▉       | 912/3070 [00:57<02:28, 14.53it/s]\u001B[A\n",
      "Processing test pairs:  30%|██▉       | 916/3070 [00:58<02:18, 15.50it/s]\u001B[A\n",
      "Processing test pairs:  30%|██▉       | 920/3070 [00:58<02:04, 17.33it/s]\u001B[A\n",
      "Processing test pairs:  30%|███       | 924/3070 [00:58<01:56, 18.45it/s]\u001B[A\n",
      "Processing test pairs:  30%|███       | 928/3070 [00:58<01:56, 18.32it/s]\u001B[A\n",
      "Processing test pairs:  30%|███       | 932/3070 [00:59<02:20, 15.26it/s]\u001B[A\n",
      "Processing test pairs:  30%|███       | 936/3070 [00:59<02:23, 14.89it/s]\u001B[A\n",
      "Processing test pairs:  31%|███       | 940/3070 [00:59<02:08, 16.57it/s]\u001B[A\n",
      "Processing test pairs:  31%|███       | 944/3070 [00:59<01:59, 17.75it/s]\u001B[A\n",
      "Processing test pairs:  31%|███       | 948/3070 [00:59<01:53, 18.67it/s]\u001B[A\n",
      "Processing test pairs:  31%|███       | 952/3070 [01:00<02:12, 15.99it/s]\u001B[A\n",
      "Processing test pairs:  31%|███       | 956/3070 [01:00<02:25, 14.55it/s]\u001B[A\n",
      "Processing test pairs:  31%|███▏      | 960/3070 [01:00<02:36, 13.51it/s]\u001B[A\n",
      "Processing test pairs:  31%|███▏      | 964/3070 [01:01<02:24, 14.53it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 968/3070 [01:01<02:33, 13.71it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 972/3070 [01:01<02:24, 14.51it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 976/3070 [01:01<02:18, 15.07it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 980/3070 [01:02<02:14, 15.59it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 984/3070 [01:02<02:21, 14.79it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 988/3070 [01:02<02:11, 15.85it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 992/3070 [01:02<02:13, 15.54it/s]\u001B[A\n",
      "Processing test pairs:  32%|███▏      | 996/3070 [01:03<02:05, 16.57it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1000/3070 [01:03<02:06, 16.39it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1004/3070 [01:03<02:00, 17.09it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1008/3070 [01:03<01:46, 19.45it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1012/3070 [01:03<01:45, 19.58it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1016/3070 [01:04<01:54, 17.89it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1020/3070 [01:04<02:06, 16.20it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1024/3070 [01:04<01:51, 18.34it/s]\u001B[A\n",
      "Processing test pairs:  33%|███▎      | 1028/3070 [01:05<02:16, 14.99it/s]\u001B[A\n",
      "Processing test pairs:  34%|███▎      | 1032/3070 [01:05<02:24, 14.14it/s]\u001B[A\n",
      "Processing test pairs:  34%|███▎      | 1036/3070 [01:05<02:18, 14.64it/s]\u001B[A\n",
      "Processing test pairs:  34%|███▍      | 1040/3070 [01:05<02:09, 15.65it/s]\u001B[A\n",
      "Processing test pairs:  34%|███▍      | 1044/3070 [01:06<02:12, 15.27it/s]\u001B[A\n",
      "Processing test pairs:  34%|███▍      | 1048/3070 [01:06<02:01, 16.58it/s]\u001B[A\n",
      "Processing test pairs:  34%|███▍      | 1052/3070 [01:06<01:51, 18.16it/s]\u001B[A\n",
      "Processing test pairs:  34%|███▍      | 1056/3070 [01:06<01:56, 17.24it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▍      | 1060/3070 [01:07<01:59, 16.80it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▍      | 1064/3070 [01:07<01:56, 17.24it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▍      | 1068/3070 [01:07<01:44, 19.09it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▍      | 1072/3070 [01:07<01:46, 18.81it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▌      | 1076/3070 [01:07<01:59, 16.62it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▌      | 1080/3070 [01:08<02:06, 15.74it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▌      | 1084/3070 [01:08<02:10, 15.20it/s]\u001B[A\n",
      "Processing test pairs:  35%|███▌      | 1088/3070 [01:08<02:10, 15.24it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▌      | 1092/3070 [01:08<01:54, 17.24it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▌      | 1096/3070 [01:09<02:01, 16.31it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▌      | 1100/3070 [01:09<02:04, 15.78it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▌      | 1104/3070 [01:09<02:00, 16.34it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▌      | 1108/3070 [01:09<02:09, 15.10it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▌      | 1112/3070 [01:10<02:12, 14.80it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▋      | 1116/3070 [01:10<02:10, 14.96it/s]\u001B[A\n",
      "Processing test pairs:  36%|███▋      | 1120/3070 [01:10<02:01, 16.07it/s]\u001B[A\n",
      "Processing test pairs:  37%|███▋      | 1124/3070 [01:11<02:06, 15.34it/s]\u001B[A\n",
      "Processing test pairs:  37%|███▋      | 1128/3070 [01:11<02:17, 14.08it/s]\u001B[A\n",
      "Processing test pairs:  37%|███▋      | 1132/3070 [01:11<02:16, 14.22it/s]\u001B[A\n",
      "Processing test pairs:  37%|███▋      | 1136/3070 [01:11<02:27, 13.12it/s]\u001B[A\n",
      "Processing test pairs:  37%|███▋      | 1140/3070 [01:12<02:34, 12.47it/s]\u001B[A\n",
      "Processing test pairs:  37%|███▋      | 1144/3070 [01:12<02:22, 13.53it/s]\u001B[A\n",
      "Processing test pairs:  37%|███▋      | 1148/3070 [01:12<02:10, 14.70it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1152/3070 [01:12<01:54, 16.75it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1156/3070 [01:13<01:45, 18.15it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1160/3070 [01:13<01:35, 20.05it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1164/3070 [01:13<01:33, 20.35it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1168/3070 [01:13<01:30, 21.04it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1172/3070 [01:13<01:34, 20.11it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1176/3070 [01:14<01:32, 20.57it/s]\u001B[A\n",
      "Processing test pairs:  38%|███▊      | 1180/3070 [01:14<01:32, 20.52it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▊      | 1184/3070 [01:14<01:34, 20.06it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▊      | 1188/3070 [01:14<01:36, 19.49it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▉      | 1192/3070 [01:14<01:34, 19.86it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▉      | 1196/3070 [01:15<01:32, 20.34it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▉      | 1200/3070 [01:15<01:31, 20.37it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▉      | 1204/3070 [01:15<01:39, 18.77it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▉      | 1208/3070 [01:15<01:38, 18.99it/s]\u001B[A\n",
      "Processing test pairs:  39%|███▉      | 1212/3070 [01:15<01:39, 18.74it/s]\u001B[A\n",
      "Processing test pairs:  40%|███▉      | 1216/3070 [01:16<01:47, 17.30it/s]\u001B[A\n",
      "Processing test pairs:  40%|███▉      | 1220/3070 [01:16<01:38, 18.77it/s]\u001B[A\n",
      "Processing test pairs:  40%|███▉      | 1224/3070 [01:16<01:37, 19.03it/s]\u001B[A\n",
      "Processing test pairs:  40%|████      | 1228/3070 [01:16<01:39, 18.43it/s]\u001B[A\n",
      "Processing test pairs:  40%|████      | 1232/3070 [01:17<02:21, 13.02it/s]\u001B[A\n",
      "Processing test pairs:  40%|████      | 1236/3070 [01:17<02:22, 12.91it/s]\u001B[A\n",
      "Processing test pairs:  40%|████      | 1240/3070 [01:17<02:14, 13.65it/s]\u001B[A\n",
      "Processing test pairs:  41%|████      | 1244/3070 [01:18<01:54, 15.93it/s]\u001B[A\n",
      "Processing test pairs:  41%|████      | 1248/3070 [01:18<01:58, 15.31it/s]\u001B[A\n",
      "Processing test pairs:  41%|████      | 1252/3070 [01:18<02:06, 14.34it/s]\u001B[A\n",
      "Processing test pairs:  41%|████      | 1256/3070 [01:19<02:19, 12.98it/s]\u001B[A\n",
      "Processing test pairs:  41%|████      | 1260/3070 [01:19<02:06, 14.33it/s]\u001B[A\n",
      "Processing test pairs:  41%|████      | 1264/3070 [01:19<01:53, 15.86it/s]\u001B[A\n",
      "Processing test pairs:  41%|████▏     | 1268/3070 [01:19<01:51, 16.22it/s]\u001B[A\n",
      "Processing test pairs:  41%|████▏     | 1272/3070 [01:19<01:49, 16.41it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1276/3070 [01:20<01:50, 16.17it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1280/3070 [01:20<01:48, 16.51it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1284/3070 [01:20<01:49, 16.35it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1288/3070 [01:20<01:51, 15.93it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1292/3070 [01:21<01:58, 14.96it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1296/3070 [01:21<01:52, 15.77it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1300/3070 [01:21<01:46, 16.58it/s]\u001B[A\n",
      "Processing test pairs:  42%|████▏     | 1304/3070 [01:21<01:45, 16.81it/s]\u001B[A\n",
      "Processing test pairs:  43%|████▎     | 1308/3070 [01:22<01:43, 17.04it/s]\u001B[A\n",
      "Processing test pairs:  43%|████▎     | 1312/3070 [01:22<01:40, 17.54it/s]\u001B[A\n",
      "Processing test pairs:  43%|████▎     | 1316/3070 [01:22<01:41, 17.22it/s]\u001B[A\n",
      "Processing test pairs:  43%|████▎     | 1320/3070 [01:22<01:30, 19.40it/s]\u001B[A\n",
      "Processing test pairs:  43%|████▎     | 1324/3070 [01:22<01:38, 17.65it/s]\u001B[A\n",
      "Processing test pairs:  43%|████▎     | 1328/3070 [01:23<01:43, 16.90it/s]\u001B[A\n",
      "Processing test pairs:  43%|████▎     | 1332/3070 [01:23<01:45, 16.52it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▎     | 1336/3070 [01:23<01:36, 17.90it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▎     | 1340/3070 [01:23<01:33, 18.52it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▍     | 1344/3070 [01:24<01:33, 18.41it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▍     | 1348/3070 [01:24<01:21, 21.03it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▍     | 1352/3070 [01:24<01:26, 19.82it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▍     | 1356/3070 [01:24<01:30, 19.01it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▍     | 1360/3070 [01:24<01:22, 20.71it/s]\u001B[A\n",
      "Processing test pairs:  44%|████▍     | 1364/3070 [01:25<01:27, 19.55it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▍     | 1368/3070 [01:25<01:35, 17.82it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▍     | 1372/3070 [01:25<01:29, 18.95it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▍     | 1376/3070 [01:25<01:18, 21.47it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▍     | 1380/3070 [01:25<01:21, 20.82it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▌     | 1384/3070 [01:26<01:16, 22.09it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▌     | 1388/3070 [01:26<01:16, 22.08it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▌     | 1392/3070 [01:26<01:30, 18.47it/s]\u001B[A\n",
      "Processing test pairs:  45%|████▌     | 1396/3070 [01:26<01:39, 16.88it/s]\u001B[A\n",
      "Processing test pairs:  46%|████▌     | 1400/3070 [01:26<01:36, 17.23it/s]\u001B[A\n",
      "Processing test pairs:  46%|████▌     | 1404/3070 [01:27<01:38, 16.98it/s]\u001B[A\n",
      "Processing test pairs:  46%|████▌     | 1408/3070 [01:27<01:38, 16.83it/s]\u001B[A\n",
      "Processing test pairs:  46%|████▌     | 1412/3070 [01:27<01:38, 16.89it/s]\u001B[A\n",
      "Processing test pairs:  46%|████▌     | 1416/3070 [01:27<01:34, 17.58it/s]\u001B[A\n",
      "Processing test pairs:  46%|████▋     | 1420/3070 [01:28<01:40, 16.49it/s]\u001B[A\n",
      "Processing test pairs:  46%|████▋     | 1424/3070 [01:28<01:36, 17.12it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1428/3070 [01:28<01:28, 18.59it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1432/3070 [01:28<01:28, 18.51it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1436/3070 [01:28<01:23, 19.65it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1440/3070 [01:29<01:22, 19.71it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1444/3070 [01:29<01:21, 19.89it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1448/3070 [01:29<01:22, 19.72it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1452/3070 [01:29<01:24, 19.22it/s]\u001B[A\n",
      "Processing test pairs:  47%|████▋     | 1456/3070 [01:30<01:24, 19.18it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1460/3070 [01:30<01:28, 18.19it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1464/3070 [01:30<01:20, 19.96it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1468/3070 [01:30<01:20, 19.79it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1472/3070 [01:30<01:23, 19.16it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1476/3070 [01:31<01:32, 17.28it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1480/3070 [01:31<01:43, 15.40it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1484/3070 [01:31<01:43, 15.31it/s]\u001B[A\n",
      "Processing test pairs:  48%|████▊     | 1488/3070 [01:32<01:50, 14.29it/s]\u001B[A\n",
      "Processing test pairs:  49%|████▊     | 1492/3070 [01:32<01:52, 14.04it/s]\u001B[A\n",
      "Processing test pairs:  49%|████▊     | 1496/3070 [01:32<01:35, 16.40it/s]\u001B[A\n",
      "Processing test pairs:  49%|████▉     | 1500/3070 [01:32<01:28, 17.66it/s]\u001B[A\n",
      "Processing test pairs:  49%|████▉     | 1504/3070 [01:32<01:27, 17.95it/s]\u001B[A\n",
      "Processing test pairs:  49%|████▉     | 1508/3070 [01:33<01:17, 20.10it/s]\u001B[A\n",
      "Processing test pairs:  49%|████▉     | 1512/3070 [01:33<01:28, 17.53it/s]\u001B[A\n",
      "Processing test pairs:  49%|████▉     | 1516/3070 [01:33<01:21, 19.10it/s]\u001B[A\n",
      "Processing test pairs:  50%|████▉     | 1520/3070 [01:33<01:14, 20.84it/s]\u001B[A\n",
      "Processing test pairs:  50%|████▉     | 1524/3070 [01:33<01:16, 20.31it/s]\u001B[A\n",
      "Processing test pairs:  50%|████▉     | 1528/3070 [01:34<01:11, 21.45it/s]\u001B[A\n",
      "Processing test pairs:  50%|████▉     | 1532/3070 [01:34<01:12, 21.09it/s]\u001B[A\n",
      "Processing test pairs:  50%|█████     | 1536/3070 [01:34<01:11, 21.36it/s]\u001B[A\n",
      "Processing test pairs:  50%|█████     | 1540/3070 [01:34<01:12, 21.23it/s]\u001B[A\n",
      "Processing test pairs:  50%|█████     | 1544/3070 [01:34<01:12, 21.06it/s]\u001B[A\n",
      "Processing test pairs:  50%|█████     | 1548/3070 [01:34<01:14, 20.30it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████     | 1552/3070 [01:35<01:17, 19.69it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████     | 1556/3070 [01:35<01:26, 17.60it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████     | 1560/3070 [01:35<01:26, 17.38it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████     | 1564/3070 [01:35<01:23, 17.99it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████     | 1568/3070 [01:36<01:21, 18.51it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████     | 1572/3070 [01:36<01:34, 15.80it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████▏    | 1576/3070 [01:36<01:28, 16.84it/s]\u001B[A\n",
      "Processing test pairs:  51%|█████▏    | 1580/3070 [01:36<01:22, 18.11it/s]\u001B[A\n",
      "Processing test pairs:  52%|█████▏    | 1584/3070 [01:37<01:20, 18.44it/s]\u001B[A\n",
      "Processing test pairs:  52%|█████▏    | 1588/3070 [01:37<01:17, 19.03it/s]\u001B[A\n",
      "Processing test pairs:  52%|█████▏    | 1592/3070 [01:37<01:17, 19.18it/s]\u001B[A\n",
      "Processing test pairs:  52%|█████▏    | 1596/3070 [01:37<01:09, 21.31it/s]\u001B[A\n",
      "Processing test pairs:  52%|█████▏    | 1600/3070 [01:37<01:23, 17.69it/s]\u001B[A\n",
      "Processing test pairs:  52%|█████▏    | 1604/3070 [01:38<01:32, 15.90it/s]\u001B[A\n",
      "Processing test pairs:  52%|█████▏    | 1608/3070 [01:38<01:35, 15.28it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1612/3070 [01:38<01:37, 14.90it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1616/3070 [01:39<01:30, 16.11it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1620/3070 [01:39<01:58, 12.26it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1624/3070 [01:39<01:48, 13.35it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1628/3070 [01:40<01:44, 13.86it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1632/3070 [01:40<01:58, 12.14it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1636/3070 [01:40<01:48, 13.21it/s]\u001B[A\n",
      "Processing test pairs:  53%|█████▎    | 1640/3070 [01:40<01:38, 14.50it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▎    | 1644/3070 [01:41<01:28, 16.06it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▎    | 1648/3070 [01:41<01:30, 15.72it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▍    | 1652/3070 [01:41<01:28, 16.11it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▍    | 1656/3070 [01:41<01:22, 17.08it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▍    | 1660/3070 [01:41<01:19, 17.80it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▍    | 1664/3070 [01:42<01:12, 19.33it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▍    | 1668/3070 [01:42<01:12, 19.34it/s]\u001B[A\n",
      "Processing test pairs:  54%|█████▍    | 1672/3070 [01:42<01:12, 19.18it/s]\u001B[A\n",
      "Processing test pairs:  55%|█████▍    | 1676/3070 [01:42<01:08, 20.39it/s]\u001B[A\n",
      "Processing test pairs:  55%|█████▍    | 1680/3070 [01:42<01:11, 19.51it/s]\u001B[A\n",
      "Processing test pairs:  55%|█████▍    | 1684/3070 [01:43<01:09, 19.82it/s]\u001B[A\n",
      "Processing test pairs:  55%|█████▍    | 1688/3070 [01:43<01:21, 17.06it/s]\u001B[A\n",
      "Processing test pairs:  55%|█████▌    | 1692/3070 [01:43<01:26, 15.91it/s]\u001B[A\n",
      "Processing test pairs:  55%|█████▌    | 1696/3070 [01:43<01:23, 16.51it/s]\u001B[A\n",
      "Processing test pairs:  55%|█████▌    | 1700/3070 [01:44<01:23, 16.48it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▌    | 1704/3070 [01:44<01:20, 16.91it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▌    | 1708/3070 [01:44<01:12, 18.66it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▌    | 1712/3070 [01:44<01:11, 19.06it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▌    | 1716/3070 [01:45<01:47, 12.64it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▌    | 1720/3070 [01:45<01:46, 12.69it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▌    | 1724/3070 [01:45<01:37, 13.84it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▋    | 1728/3070 [01:46<01:21, 16.52it/s]\u001B[A\n",
      "Processing test pairs:  56%|█████▋    | 1732/3070 [01:46<01:19, 16.85it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1736/3070 [01:46<01:21, 16.31it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1740/3070 [01:46<01:18, 17.00it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1744/3070 [01:46<01:15, 17.64it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1748/3070 [01:47<01:12, 18.22it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1752/3070 [01:47<01:15, 17.36it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1756/3070 [01:47<01:17, 16.98it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1760/3070 [01:47<01:13, 17.86it/s]\u001B[A\n",
      "Processing test pairs:  57%|█████▋    | 1764/3070 [01:48<01:12, 17.96it/s]\u001B[A\n",
      "Processing test pairs:  58%|█████▊    | 1768/3070 [01:48<01:12, 18.00it/s]\u001B[A\n",
      "Processing test pairs:  58%|█████▊    | 1772/3070 [01:48<01:18, 16.54it/s]\u001B[A\n",
      "Processing test pairs:  58%|█████▊    | 1776/3070 [01:48<01:22, 15.73it/s]\u001B[A\n",
      "Processing test pairs:  58%|█████▊    | 1780/3070 [01:49<01:12, 17.68it/s]\u001B[A\n",
      "Processing test pairs:  58%|█████▊    | 1784/3070 [01:49<01:24, 15.15it/s]\u001B[A\n",
      "Processing test pairs:  58%|█████▊    | 1788/3070 [01:49<01:21, 15.68it/s]\u001B[A\n",
      "Processing test pairs:  58%|█████▊    | 1792/3070 [01:49<01:22, 15.51it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▊    | 1796/3070 [01:50<01:24, 15.05it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▊    | 1800/3070 [01:50<01:24, 15.09it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▉    | 1804/3070 [01:50<01:20, 15.70it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▉    | 1808/3070 [01:50<01:25, 14.80it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▉    | 1812/3070 [01:51<01:21, 15.42it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▉    | 1816/3070 [01:51<01:17, 16.23it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▉    | 1820/3070 [01:51<01:14, 16.81it/s]\u001B[A\n",
      "Processing test pairs:  59%|█████▉    | 1824/3070 [01:51<01:07, 18.47it/s]\u001B[A\n",
      "Processing test pairs:  60%|█████▉    | 1828/3070 [01:52<01:10, 17.65it/s]\u001B[A\n",
      "Processing test pairs:  60%|█████▉    | 1832/3070 [01:52<01:05, 18.82it/s]\u001B[A\n",
      "Processing test pairs:  60%|█████▉    | 1836/3070 [01:52<01:09, 17.87it/s]\u001B[A\n",
      "Processing test pairs:  60%|█████▉    | 1840/3070 [01:52<01:16, 16.06it/s]\u001B[A\n",
      "Processing test pairs:  60%|██████    | 1844/3070 [01:53<01:17, 15.91it/s]\u001B[A\n",
      "Processing test pairs:  60%|██████    | 1848/3070 [01:53<01:17, 15.77it/s]\u001B[A\n",
      "Processing test pairs:  60%|██████    | 1852/3070 [01:53<01:18, 15.53it/s]\u001B[A\n",
      "Processing test pairs:  60%|██████    | 1856/3070 [01:53<01:12, 16.72it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████    | 1860/3070 [01:54<01:13, 16.41it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████    | 1864/3070 [01:54<01:11, 16.78it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████    | 1868/3070 [01:54<01:07, 17.69it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████    | 1872/3070 [01:54<01:07, 17.77it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████    | 1876/3070 [01:54<01:11, 16.69it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████    | 1880/3070 [01:55<01:10, 16.85it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████▏   | 1884/3070 [01:55<01:08, 17.33it/s]\u001B[A\n",
      "Processing test pairs:  61%|██████▏   | 1888/3070 [01:55<01:06, 17.83it/s]\u001B[A\n",
      "Processing test pairs:  62%|██████▏   | 1892/3070 [01:55<01:04, 18.19it/s]\u001B[A\n",
      "Processing test pairs:  62%|██████▏   | 1896/3070 [01:56<01:11, 16.46it/s]\u001B[A\n",
      "Processing test pairs:  62%|██████▏   | 1900/3070 [01:56<01:15, 15.40it/s]\u001B[A\n",
      "Processing test pairs:  62%|██████▏   | 1904/3070 [01:56<01:12, 16.05it/s]\u001B[A\n",
      "Processing test pairs:  62%|██████▏   | 1908/3070 [01:56<01:14, 15.70it/s]\u001B[A\n",
      "Processing test pairs:  62%|██████▏   | 1912/3070 [01:57<01:13, 15.71it/s]\u001B[A\n",
      "Processing test pairs:  62%|██████▏   | 1916/3070 [01:57<01:13, 15.65it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1920/3070 [01:57<01:10, 16.31it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1924/3070 [01:57<01:13, 15.53it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1928/3070 [01:58<01:04, 17.67it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1932/3070 [01:58<01:02, 18.19it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1936/3070 [01:58<01:00, 18.89it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1940/3070 [01:58<00:57, 19.54it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1944/3070 [01:58<00:57, 19.75it/s]\u001B[A\n",
      "Processing test pairs:  63%|██████▎   | 1948/3070 [01:59<01:03, 17.65it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▎   | 1952/3070 [01:59<01:01, 18.14it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▎   | 1956/3070 [01:59<00:59, 18.85it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▍   | 1960/3070 [01:59<01:03, 17.57it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▍   | 1964/3070 [02:00<01:40, 11.04it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▍   | 1968/3070 [02:00<01:37, 11.29it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▍   | 1972/3070 [02:01<01:29, 12.33it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▍   | 1976/3070 [02:01<01:15, 14.58it/s]\u001B[A\n",
      "Processing test pairs:  64%|██████▍   | 1980/3070 [02:01<01:11, 15.27it/s]\u001B[A\n",
      "Processing test pairs:  65%|██████▍   | 1984/3070 [02:01<01:08, 15.91it/s]\u001B[A\n",
      "Processing test pairs:  65%|██████▍   | 1988/3070 [02:01<00:59, 18.14it/s]\u001B[A\n",
      "Processing test pairs:  65%|██████▍   | 1992/3070 [02:02<01:04, 16.65it/s]\u001B[A\n",
      "Processing test pairs:  65%|██████▌   | 1996/3070 [02:02<00:59, 17.96it/s]\u001B[A\n",
      "Processing test pairs:  65%|██████▌   | 2000/3070 [02:02<00:58, 18.21it/s]\u001B[A\n",
      "Processing test pairs:  65%|██████▌   | 2004/3070 [02:02<00:56, 18.77it/s]\u001B[A\n",
      "Processing test pairs:  65%|██████▌   | 2008/3070 [02:02<00:52, 20.09it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▌   | 2012/3070 [02:03<00:57, 18.40it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▌   | 2016/3070 [02:03<01:04, 16.24it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▌   | 2020/3070 [02:03<01:03, 16.58it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▌   | 2024/3070 [02:03<00:59, 17.46it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▌   | 2028/3070 [02:04<01:12, 14.34it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▌   | 2032/3070 [02:04<01:10, 14.79it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▋   | 2036/3070 [02:04<01:09, 14.94it/s]\u001B[A\n",
      "Processing test pairs:  66%|██████▋   | 2040/3070 [02:05<01:05, 15.68it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2044/3070 [02:05<01:03, 16.11it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2048/3070 [02:05<00:54, 18.87it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2052/3070 [02:05<00:55, 18.20it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2056/3070 [02:05<00:56, 17.89it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2060/3070 [02:06<00:55, 18.11it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2064/3070 [02:06<00:55, 18.21it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2068/3070 [02:06<00:54, 18.23it/s]\u001B[A\n",
      "Processing test pairs:  67%|██████▋   | 2072/3070 [02:06<00:52, 18.98it/s]\u001B[A\n",
      "Processing test pairs:  68%|██████▊   | 2076/3070 [02:06<00:55, 17.93it/s]\u001B[A\n",
      "Processing test pairs:  68%|██████▊   | 2080/3070 [02:07<00:52, 18.93it/s]\u001B[A\n",
      "Processing test pairs:  68%|██████▊   | 2084/3070 [02:07<01:06, 14.78it/s]\u001B[A\n",
      "Processing test pairs:  68%|██████▊   | 2088/3070 [02:07<01:09, 14.23it/s]\u001B[A\n",
      "Processing test pairs:  68%|██████▊   | 2092/3070 [02:07<00:57, 16.93it/s]\u001B[A\n",
      "Processing test pairs:  68%|██████▊   | 2096/3070 [02:08<00:57, 16.89it/s]\u001B[A\n",
      "Processing test pairs:  68%|██████▊   | 2100/3070 [02:08<00:59, 16.25it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▊   | 2104/3070 [02:08<00:58, 16.56it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▊   | 2108/3070 [02:08<01:00, 15.83it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▉   | 2112/3070 [02:09<01:01, 15.56it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▉   | 2116/3070 [02:09<01:00, 15.73it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▉   | 2120/3070 [02:09<00:54, 17.35it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▉   | 2124/3070 [02:09<00:59, 15.81it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▉   | 2128/3070 [02:10<00:57, 16.28it/s]\u001B[A\n",
      "Processing test pairs:  69%|██████▉   | 2132/3070 [02:10<00:53, 17.44it/s]\u001B[A\n",
      "Processing test pairs:  70%|██████▉   | 2136/3070 [02:10<01:11, 13.02it/s]\u001B[A\n",
      "Processing test pairs:  70%|██████▉   | 2140/3070 [02:11<01:08, 13.63it/s]\u001B[A\n",
      "Processing test pairs:  70%|██████▉   | 2144/3070 [02:11<00:59, 15.47it/s]\u001B[A\n",
      "Processing test pairs:  70%|██████▉   | 2148/3070 [02:11<00:55, 16.57it/s]\u001B[A\n",
      "Processing test pairs:  70%|███████   | 2152/3070 [02:11<00:54, 16.78it/s]\u001B[A\n",
      "Processing test pairs:  70%|███████   | 2156/3070 [02:11<00:54, 16.65it/s]\u001B[A\n",
      "Processing test pairs:  70%|███████   | 2160/3070 [02:12<00:51, 17.59it/s]\u001B[A\n",
      "Processing test pairs:  70%|███████   | 2164/3070 [02:12<00:51, 17.67it/s]\u001B[A\n",
      "Processing test pairs:  71%|███████   | 2168/3070 [02:12<00:51, 17.60it/s]\u001B[A\n",
      "Processing test pairs:  71%|███████   | 2172/3070 [02:12<00:51, 17.48it/s]\u001B[A\n",
      "Processing test pairs:  71%|███████   | 2176/3070 [02:13<00:48, 18.41it/s]\u001B[A\n",
      "Processing test pairs:  71%|███████   | 2180/3070 [02:13<00:53, 16.75it/s]\u001B[A\n",
      "Processing test pairs:  71%|███████   | 2184/3070 [02:13<00:50, 17.70it/s]\u001B[A\n",
      "Processing test pairs:  71%|███████▏  | 2188/3070 [02:13<00:50, 17.32it/s]\u001B[A\n",
      "Processing test pairs:  71%|███████▏  | 2192/3070 [02:14<00:53, 16.31it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2196/3070 [02:14<00:50, 17.48it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2200/3070 [02:14<00:51, 16.89it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2204/3070 [02:14<00:53, 16.27it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2208/3070 [02:15<00:51, 16.71it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2212/3070 [02:15<01:17, 11.12it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2216/3070 [02:16<01:19, 10.77it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2220/3070 [02:16<01:12, 11.71it/s]\u001B[A\n",
      "Processing test pairs:  72%|███████▏  | 2224/3070 [02:16<01:01, 13.81it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2228/3070 [02:16<00:58, 14.40it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2232/3070 [02:16<00:52, 15.89it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2236/3070 [02:17<00:58, 14.24it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2240/3070 [02:17<00:59, 14.00it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2244/3070 [02:17<00:56, 14.71it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2248/3070 [02:18<00:54, 14.99it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2252/3070 [02:18<01:01, 13.41it/s]\u001B[A\n",
      "Processing test pairs:  73%|███████▎  | 2256/3070 [02:18<00:57, 14.05it/s]\u001B[A\n",
      "Processing test pairs:  74%|███████▎  | 2260/3070 [02:18<00:52, 15.49it/s]\u001B[A\n",
      "Processing test pairs:  74%|███████▎  | 2264/3070 [02:19<00:46, 17.41it/s]\u001B[A\n",
      "Processing test pairs:  74%|███████▍  | 2268/3070 [02:19<00:45, 17.74it/s]\u001B[A\n",
      "Processing test pairs:  74%|███████▍  | 2272/3070 [02:19<00:43, 18.27it/s]\u001B[A\n",
      "Processing test pairs:  74%|███████▍  | 2276/3070 [02:19<00:43, 18.14it/s]\u001B[A\n",
      "Processing test pairs:  74%|███████▍  | 2280/3070 [02:19<00:43, 18.04it/s]\u001B[A\n",
      "Processing test pairs:  74%|███████▍  | 2284/3070 [02:20<00:43, 17.91it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▍  | 2288/3070 [02:20<00:44, 17.50it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▍  | 2292/3070 [02:20<00:47, 16.27it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▍  | 2296/3070 [02:20<00:49, 15.79it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▍  | 2300/3070 [02:21<00:49, 15.67it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▌  | 2304/3070 [02:21<00:45, 16.81it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▌  | 2308/3070 [02:21<00:41, 18.29it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▌  | 2312/3070 [02:21<00:41, 18.28it/s]\u001B[A\n",
      "Processing test pairs:  75%|███████▌  | 2316/3070 [02:22<00:45, 16.68it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▌  | 2320/3070 [02:22<00:45, 16.41it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▌  | 2324/3070 [02:22<00:43, 17.18it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▌  | 2328/3070 [02:22<00:42, 17.33it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▌  | 2332/3070 [02:22<00:41, 17.60it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▌  | 2336/3070 [02:23<00:46, 15.81it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▌  | 2340/3070 [02:23<00:45, 16.18it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▋  | 2344/3070 [02:23<00:48, 14.89it/s]\u001B[A\n",
      "Processing test pairs:  76%|███████▋  | 2348/3070 [02:24<00:57, 12.62it/s]\u001B[A\n",
      "Processing test pairs:  77%|███████▋  | 2352/3070 [02:24<01:04, 11.14it/s]\u001B[A\n",
      "Processing test pairs:  77%|███████▋  | 2356/3070 [02:25<01:03, 11.22it/s]\u001B[A\n",
      "Processing test pairs:  77%|███████▋  | 2360/3070 [02:25<00:58, 12.09it/s]\u001B[A\n",
      "Processing test pairs:  77%|███████▋  | 2364/3070 [02:25<00:54, 13.07it/s]\u001B[A\n",
      "Processing test pairs:  77%|███████▋  | 2368/3070 [02:26<01:11,  9.80it/s]\u001B[A\n",
      "Processing test pairs:  77%|███████▋  | 2372/3070 [02:26<01:02, 11.17it/s]\u001B[A\n",
      "Processing test pairs:  77%|███████▋  | 2376/3070 [02:26<00:52, 13.19it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2380/3070 [02:27<00:54, 12.66it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2384/3070 [02:27<00:46, 14.73it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2388/3070 [02:27<00:43, 15.65it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2392/3070 [02:27<00:43, 15.52it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2396/3070 [02:27<00:43, 15.50it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2400/3070 [02:28<00:56, 11.92it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2404/3070 [02:28<00:54, 12.21it/s]\u001B[A\n",
      "Processing test pairs:  78%|███████▊  | 2408/3070 [02:28<00:49, 13.36it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▊  | 2412/3070 [02:29<00:48, 13.49it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▊  | 2416/3070 [02:29<00:48, 13.48it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▉  | 2420/3070 [02:29<00:46, 13.98it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▉  | 2424/3070 [02:30<00:46, 13.86it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▉  | 2428/3070 [02:30<00:45, 14.17it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▉  | 2432/3070 [02:30<00:41, 15.30it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▉  | 2436/3070 [02:30<00:40, 15.58it/s]\u001B[A\n",
      "Processing test pairs:  79%|███████▉  | 2440/3070 [02:31<00:39, 16.01it/s]\u001B[A\n",
      "Processing test pairs:  80%|███████▉  | 2444/3070 [02:31<00:36, 16.99it/s]\u001B[A\n",
      "Processing test pairs:  80%|███████▉  | 2448/3070 [02:31<00:36, 17.24it/s]\u001B[A\n",
      "Processing test pairs:  80%|███████▉  | 2452/3070 [02:31<00:36, 17.08it/s]\u001B[A\n",
      "Processing test pairs:  80%|████████  | 2456/3070 [02:31<00:35, 17.42it/s]\u001B[A\n",
      "Processing test pairs:  80%|████████  | 2460/3070 [02:32<00:35, 17.14it/s]\u001B[A\n",
      "Processing test pairs:  80%|████████  | 2464/3070 [02:32<00:35, 16.85it/s]\u001B[A\n",
      "Processing test pairs:  80%|████████  | 2468/3070 [02:32<00:34, 17.44it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████  | 2472/3070 [02:32<00:37, 16.02it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████  | 2476/3070 [02:33<00:35, 16.76it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████  | 2480/3070 [02:33<00:36, 16.11it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████  | 2484/3070 [02:33<00:36, 16.24it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████  | 2488/3070 [02:33<00:34, 16.87it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████  | 2492/3070 [02:34<00:33, 17.32it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████▏ | 2496/3070 [02:34<00:31, 18.18it/s]\u001B[A\n",
      "Processing test pairs:  81%|████████▏ | 2500/3070 [02:34<00:31, 18.28it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2504/3070 [02:34<00:33, 16.93it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2508/3070 [02:34<00:30, 18.52it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2512/3070 [02:35<00:31, 17.90it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2516/3070 [02:35<00:33, 16.48it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2520/3070 [02:35<00:34, 15.83it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2524/3070 [02:36<00:37, 14.73it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2528/3070 [02:36<00:34, 15.79it/s]\u001B[A\n",
      "Processing test pairs:  82%|████████▏ | 2532/3070 [02:36<00:30, 17.58it/s]\u001B[A\n",
      "Processing test pairs:  83%|████████▎ | 2536/3070 [02:36<00:33, 15.75it/s]\u001B[A\n",
      "Processing test pairs:  83%|████████▎ | 2540/3070 [02:37<00:32, 16.41it/s]\u001B[A\n",
      "Processing test pairs:  83%|████████▎ | 2544/3070 [02:37<00:29, 17.64it/s]\u001B[A\n",
      "Processing test pairs:  83%|████████▎ | 2548/3070 [02:37<00:29, 17.93it/s]\u001B[A\n",
      "Processing test pairs:  83%|████████▎ | 2552/3070 [02:37<00:28, 17.91it/s]\u001B[A\n",
      "Processing test pairs:  83%|████████▎ | 2556/3070 [02:37<00:28, 18.02it/s]\u001B[A\n",
      "Processing test pairs:  83%|████████▎ | 2560/3070 [02:38<00:29, 17.47it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▎ | 2564/3070 [02:38<00:28, 17.48it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▎ | 2568/3070 [02:38<00:28, 17.46it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▍ | 2572/3070 [02:38<00:29, 16.74it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▍ | 2576/3070 [02:39<00:27, 17.95it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▍ | 2580/3070 [02:39<00:34, 14.34it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▍ | 2584/3070 [02:39<00:32, 14.73it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▍ | 2588/3070 [02:40<00:51,  9.44it/s]\u001B[A\n",
      "Processing test pairs:  84%|████████▍ | 2592/3070 [02:40<00:50,  9.52it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▍ | 2596/3070 [02:41<00:43, 10.94it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▍ | 2600/3070 [02:41<00:38, 12.16it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▍ | 2604/3070 [02:41<00:32, 14.25it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▍ | 2608/3070 [02:41<00:30, 15.10it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▌ | 2612/3070 [02:41<00:28, 16.25it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▌ | 2616/3070 [02:42<00:34, 13.08it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▌ | 2620/3070 [02:42<00:31, 14.21it/s]\u001B[A\n",
      "Processing test pairs:  85%|████████▌ | 2624/3070 [02:42<00:32, 13.89it/s]\u001B[A\n",
      "Processing test pairs:  86%|████████▌ | 2628/3070 [02:43<00:29, 14.74it/s]\u001B[A\n",
      "Processing test pairs:  86%|████████▌ | 2632/3070 [02:43<00:27, 15.82it/s]\u001B[A\n",
      "Processing test pairs:  86%|████████▌ | 2636/3070 [02:43<00:26, 16.17it/s]\u001B[A\n",
      "Processing test pairs:  86%|████████▌ | 2640/3070 [02:43<00:25, 16.69it/s]\u001B[A\n",
      "Processing test pairs:  86%|████████▌ | 2644/3070 [02:43<00:23, 18.18it/s]\u001B[A\n",
      "Processing test pairs:  86%|████████▋ | 2648/3070 [02:44<00:21, 19.24it/s]\u001B[A\n",
      "Processing test pairs:  86%|████████▋ | 2652/3070 [02:44<00:22, 18.32it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2656/3070 [02:44<00:22, 18.26it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2660/3070 [02:44<00:23, 17.18it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2664/3070 [02:45<00:22, 17.79it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2668/3070 [02:45<00:24, 16.22it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2672/3070 [02:45<00:22, 17.31it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2676/3070 [02:45<00:21, 18.32it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2680/3070 [02:46<00:33, 11.50it/s]\u001B[A\n",
      "Processing test pairs:  87%|████████▋ | 2684/3070 [02:47<00:40,  9.48it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2688/3070 [02:47<00:38, 10.03it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2692/3070 [02:47<00:33, 11.23it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2696/3070 [02:47<00:32, 11.61it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2700/3070 [02:48<00:28, 12.78it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2704/3070 [02:48<00:26, 13.74it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2708/3070 [02:48<00:22, 16.06it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2712/3070 [02:48<00:23, 15.38it/s]\u001B[A\n",
      "Processing test pairs:  88%|████████▊ | 2716/3070 [02:49<00:22, 16.08it/s]\u001B[A\n",
      "Processing test pairs:  89%|████████▊ | 2720/3070 [02:49<00:22, 15.52it/s]\u001B[A\n",
      "Processing test pairs:  89%|████████▊ | 2724/3070 [02:49<00:20, 16.82it/s]\u001B[A\n",
      "Processing test pairs:  89%|████████▉ | 2728/3070 [02:49<00:20, 16.92it/s]\u001B[A\n",
      "Processing test pairs:  89%|████████▉ | 2732/3070 [02:50<00:19, 17.03it/s]\u001B[A\n",
      "Processing test pairs:  89%|████████▉ | 2736/3070 [02:50<00:18, 18.36it/s]\u001B[A\n",
      "Processing test pairs:  89%|████████▉ | 2740/3070 [02:50<00:18, 17.40it/s]\u001B[A\n",
      "Processing test pairs:  89%|████████▉ | 2744/3070 [02:50<00:19, 17.05it/s]\u001B[A\n",
      "Processing test pairs:  90%|████████▉ | 2748/3070 [02:50<00:19, 16.77it/s]\u001B[A\n",
      "Processing test pairs:  90%|████████▉ | 2752/3070 [02:51<00:18, 17.65it/s]\u001B[A\n",
      "Processing test pairs:  90%|████████▉ | 2756/3070 [02:51<00:16, 18.66it/s]\u001B[A\n",
      "Processing test pairs:  90%|████████▉ | 2760/3070 [02:51<00:17, 17.32it/s]\u001B[A\n",
      "Processing test pairs:  90%|█████████ | 2764/3070 [02:51<00:19, 15.96it/s]\u001B[A\n",
      "Processing test pairs:  90%|█████████ | 2768/3070 [02:52<00:20, 14.86it/s]\u001B[A\n",
      "Processing test pairs:  90%|█████████ | 2772/3070 [02:52<00:18, 16.01it/s]\u001B[A\n",
      "Processing test pairs:  90%|█████████ | 2776/3070 [02:52<00:19, 15.30it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████ | 2780/3070 [02:53<00:21, 13.41it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████ | 2784/3070 [02:53<00:20, 13.71it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████ | 2788/3070 [02:53<00:17, 15.78it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████ | 2792/3070 [02:53<00:18, 14.94it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████ | 2796/3070 [02:54<00:18, 14.75it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████ | 2800/3070 [02:54<00:17, 15.22it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████▏| 2804/3070 [02:54<00:16, 15.77it/s]\u001B[A\n",
      "Processing test pairs:  91%|█████████▏| 2808/3070 [02:54<00:16, 15.68it/s]\u001B[A\n",
      "Processing test pairs:  92%|█████████▏| 2812/3070 [02:55<00:19, 12.99it/s]\u001B[A\n",
      "Processing test pairs:  92%|█████████▏| 2816/3070 [02:55<00:22, 11.26it/s]\u001B[A\n",
      "Processing test pairs:  92%|█████████▏| 2820/3070 [02:56<00:20, 12.20it/s]\u001B[A\n",
      "Processing test pairs:  92%|█████████▏| 2824/3070 [02:56<00:21, 11.58it/s]\u001B[A\n",
      "Processing test pairs:  92%|█████████▏| 2828/3070 [02:56<00:19, 12.35it/s]\u001B[A\n",
      "Processing test pairs:  92%|█████████▏| 2832/3070 [02:56<00:17, 13.42it/s]\u001B[A\n",
      "Processing test pairs:  92%|█████████▏| 2836/3070 [02:57<00:16, 14.32it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2840/3070 [02:57<00:14, 15.83it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2844/3070 [02:57<00:14, 15.64it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2848/3070 [02:57<00:12, 17.55it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2852/3070 [02:57<00:12, 17.86it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2856/3070 [02:58<00:12, 17.00it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2860/3070 [02:58<00:12, 17.09it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2864/3070 [02:58<00:13, 14.85it/s]\u001B[A\n",
      "Processing test pairs:  93%|█████████▎| 2868/3070 [02:59<00:12, 16.45it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▎| 2872/3070 [02:59<00:13, 14.71it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▎| 2876/3070 [02:59<00:12, 15.22it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▍| 2880/3070 [02:59<00:12, 14.64it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▍| 2884/3070 [03:00<00:12, 14.76it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▍| 2888/3070 [03:00<00:16, 10.94it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▍| 2892/3070 [03:00<00:14, 12.18it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▍| 2896/3070 [03:01<00:14, 11.86it/s]\u001B[A\n",
      "Processing test pairs:  94%|█████████▍| 2900/3070 [03:01<00:14, 12.02it/s]\u001B[A\n",
      "Processing test pairs:  95%|█████████▍| 2904/3070 [03:01<00:13, 12.58it/s]\u001B[A\n",
      "Processing test pairs:  95%|█████████▍| 2908/3070 [03:02<00:12, 13.25it/s]\u001B[A\n",
      "Processing test pairs:  95%|█████████▍| 2912/3070 [03:02<00:11, 13.67it/s]\u001B[A\n",
      "Processing test pairs:  95%|█████████▍| 2916/3070 [03:02<00:11, 13.10it/s]\u001B[A\n",
      "Processing test pairs:  95%|█████████▌| 2920/3070 [03:03<00:13, 11.34it/s]\u001B[A\n",
      "Processing test pairs:  95%|█████████▌| 2924/3070 [03:03<00:12, 11.68it/s]\u001B[A\n",
      "Processing test pairs:  95%|█████████▌| 2928/3070 [03:03<00:11, 12.73it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▌| 2932/3070 [03:04<00:10, 13.29it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▌| 2936/3070 [03:04<00:10, 12.68it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▌| 2940/3070 [03:04<00:11, 11.01it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▌| 2944/3070 [03:05<00:11, 10.64it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▌| 2948/3070 [03:05<00:10, 11.36it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▌| 2952/3070 [03:05<00:09, 12.97it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▋| 2956/3070 [03:06<00:08, 13.49it/s]\u001B[A\n",
      "Processing test pairs:  96%|█████████▋| 2960/3070 [03:06<00:07, 15.24it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2964/3070 [03:06<00:06, 16.66it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2968/3070 [03:06<00:05, 17.27it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2972/3070 [03:06<00:05, 17.97it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2976/3070 [03:07<00:04, 18.93it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2980/3070 [03:07<00:05, 17.95it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2984/3070 [03:07<00:04, 18.97it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2988/3070 [03:07<00:04, 19.89it/s]\u001B[A\n",
      "Processing test pairs:  97%|█████████▋| 2992/3070 [03:07<00:04, 18.35it/s]\u001B[A\n",
      "Processing test pairs:  98%|█████████▊| 2996/3070 [03:08<00:04, 18.25it/s]\u001B[A\n",
      "Processing test pairs:  98%|█████████▊| 3000/3070 [03:08<00:03, 17.79it/s]\u001B[A\n",
      "Processing test pairs:  98%|█████████▊| 3004/3070 [03:08<00:03, 17.96it/s]\u001B[A\n",
      "Processing test pairs:  98%|█████████▊| 3008/3070 [03:08<00:03, 18.77it/s]\u001B[A\n",
      "Processing test pairs:  98%|█████████▊| 3012/3070 [03:09<00:03, 18.33it/s]\u001B[A\n",
      "Processing test pairs:  98%|█████████▊| 3016/3070 [03:09<00:03, 17.86it/s]\u001B[A\n",
      "Processing test pairs:  98%|█████████▊| 3020/3070 [03:09<00:02, 18.00it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▊| 3024/3070 [03:09<00:02, 18.62it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▊| 3028/3070 [03:10<00:02, 16.26it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▉| 3032/3070 [03:10<00:02, 17.27it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▉| 3036/3070 [03:10<00:02, 16.26it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▉| 3040/3070 [03:10<00:01, 16.86it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▉| 3044/3070 [03:10<00:01, 17.19it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▉| 3048/3070 [03:11<00:01, 16.01it/s]\u001B[A\n",
      "Processing test pairs:  99%|█████████▉| 3052/3070 [03:11<00:01, 17.82it/s]\u001B[A\n",
      "Processing test pairs: 100%|█████████▉| 3056/3070 [03:11<00:00, 18.48it/s]\u001B[A\n",
      "Processing test pairs: 100%|█████████▉| 3060/3070 [03:11<00:00, 19.27it/s]\u001B[A\n",
      "Processing test pairs: 100%|█████████▉| 3064/3070 [03:11<00:00, 20.23it/s]\u001B[A\n",
      "Processing test pairs: 100%|██████████| 3070/3070 [03:12<00:00, 15.97it/s]\u001B[A\n"
     ]
    }
   ],
   "source": [
    "# processed_test_data = process_test_data(dataroot2 + \"/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5be28f4f9bf193",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T21:52:16.109211Z",
     "start_time": "2025-05-13T21:52:14.936327Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # save processed data to pickle file\n",
    "# with open(\"task2_test_data.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(processed_test_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be5580d5470e9f2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:20:06.669847Z",
     "start_time": "2025-05-13T22:20:05.832347Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load processed data\n",
    "with open(\"task2_test_data.pkl\", \"rb\") as file:\n",
    "    processed_test_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e0479a1e4d64ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:20:09.897952Z",
     "start_time": "2025-05-13T22:20:09.874005Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch import optim\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "class model1():\n",
    "    def __init__(self):\n",
    "        self.model = MLPClassifier()\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        return\n",
    "    \n",
    "    def predict_instance(self, file1, file2):\n",
    "        model = self.model\n",
    "        device = torch.device(\"cpu\")\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        x = process_pair_predict((file1, file2))  # Already returns features\n",
    "        x = torch.tensor(x, dtype=torch.float).unsqueeze(0)  # Add batch dimension\n",
    "        output = model(x)\n",
    "        predicted = torch.argmax(output, dim=1).bool().tolist()\n",
    "        return predicted\n",
    "\n",
    "\n",
    "    def predict(self, path, outpath=None, n_jobs=4, processed_data=None):\n",
    "        import ast\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = self.model\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "        if not processed_data:\n",
    "            print(\"Test data not pre-processed\")\n",
    "    \n",
    "            print(\"Loading test data...\")\n",
    "            with open(path, 'r') as f:\n",
    "                data = ast.literal_eval(f.read())\n",
    "    \n",
    "            print(f\"Total test samples: {len(data)}\")\n",
    "    \n",
    "            # Parallelize feature processing\n",
    "            print(f\"Extracting features in parallel using {n_jobs} workers...\")\n",
    "            process_func = partial(process_pair_predict)\n",
    "            results = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(process_func)(d) for d in tqdm.tqdm(data, desc=\"Processing test pairs\")\n",
    "            )\n",
    "    \n",
    "            # Filter out failed results\n",
    "            results = [r for r in results if r is not None]\n",
    "            if len(results) < len(data):\n",
    "                print(f\"Warning: {len(data) - len(results)} samples failed during feature extraction.\")\n",
    "        else:\n",
    "            results = processed_data\n",
    "    \n",
    "            # You still need to load raw data to get the keys\n",
    "            with open(path, 'r') as f:\n",
    "                data = ast.literal_eval(f.read())\n",
    "    \n",
    "        # Convert to tensor\n",
    "        xTest = torch.tensor(results, dtype=torch.float32).to(device)\n",
    "    \n",
    "        # Make predictions\n",
    "        print(\"Running predictions...\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(xTest)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            predicted_bool = predicted.bool().tolist()\n",
    "    \n",
    "        # Map predictions to keys\n",
    "        predictions = dict(zip(data, predicted_bool))\n",
    "    \n",
    "        # Optionally write output\n",
    "        if outpath:\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "            print(f\"Predictions written to {outpath}\")\n",
    "    \n",
    "        print(\"Prediction process completed.\")\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Train your model. Note that this function will not be called from the autograder:\n",
    "    # instead you should upload your saved model using save()\n",
    "    import torch\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=100):\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "        model = self.model\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()  \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "        \n",
    "    \n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "    \n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        patience=10\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += batch_y.size(0)\n",
    "                correct_train += (predicted == batch_y).sum().item()\n",
    "    \n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_acc.append(train_accuracy)\n",
    "    \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    batch_y = batch_y.to(device)\n",
    "                    outputs = model(batch_x)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += batch_y.size(0)\n",
    "                    correct_val += (predicted == batch_y).sum().item()\n",
    "    \n",
    "            val_accuracy = correct_val / total_val\n",
    "            val_acc.append(val_accuracy)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "            # Step the LR scheduler\n",
    "            scheduler.step(val_accuracy)\n",
    "    \n",
    "            # Early Stopping\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict()  # Save best model\n",
    "                save_model(model, 'sol2_MLP_1.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "    \n",
    "        # Load best model state\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "        self.train_acc = train_acc\n",
    "        self.val_acc = val_acc\n",
    "        self.model = model\n",
    "                \n",
    "    def get_train_acc(self):\n",
    "        return self.train_acc, self.val_acc\n",
    "    \n",
    "    def _get_model_copy(self, model):\n",
    "        \"\"\"Create a deep copy of the model.\"\"\"\n",
    "        model_copy = type(model)(*model.__init_args__, **model.__init_kwargs__)\n",
    "        model_copy.load_state_dict(model.state_dict())\n",
    "        return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62032119941914e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:20:30.257070Z",
     "start_time": "2025-05-13T22:20:12.733106Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Accuracy: 0.6386, Validation Accuracy: 0.7008\n",
      "Model saved to sol2_MLP_1.pt\n",
      "Epoch [2/100], Train Accuracy: 0.6822, Validation Accuracy: 0.7186\n",
      "Model saved to sol2_MLP_1.pt\n",
      "Epoch [3/100], Train Accuracy: 0.6917, Validation Accuracy: 0.7296\n",
      "Model saved to sol2_MLP_1.pt\n",
      "Epoch [4/100], Train Accuracy: 0.7406, Validation Accuracy: 0.8499\n",
      "Model saved to sol2_MLP_1.pt\n",
      "Epoch [5/100], Train Accuracy: 0.8208, Validation Accuracy: 0.8985\n",
      "Model saved to sol2_MLP_1.pt\n",
      "Epoch [6/100], Train Accuracy: 0.8657, Validation Accuracy: 0.9100\n",
      "Model saved to sol2_MLP_1.pt\n",
      "Epoch [7/100], Train Accuracy: 0.8909, Validation Accuracy: 0.9257\n",
      "Model saved to sol2_MLP_1.pt\n",
      "Epoch [8/100], Train Accuracy: 0.9010, Validation Accuracy: 0.9236\n",
      "Epoch [9/100], Train Accuracy: 0.9122, Validation Accuracy: 0.9236\n",
      "Epoch [10/100], Train Accuracy: 0.9135, Validation Accuracy: 0.9247\n",
      "Epoch [11/100], Train Accuracy: 0.9139, Validation Accuracy: 0.9247\n",
      "Epoch [12/100], Train Accuracy: 0.9147, Validation Accuracy: 0.9236\n",
      "Epoch [13/100], Train Accuracy: 0.9215, Validation Accuracy: 0.9236\n",
      "Epoch [14/100], Train Accuracy: 0.9239, Validation Accuracy: 0.9236\n",
      "Epoch [15/100], Train Accuracy: 0.9223, Validation Accuracy: 0.9236\n",
      "Epoch [16/100], Train Accuracy: 0.9226, Validation Accuracy: 0.9236\n",
      "Epoch [17/100], Train Accuracy: 0.9230, Validation Accuracy: 0.9236\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model = model1()\n",
    "model.train(train_loader, val_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49b62fdffa592c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:20:30.565213Z",
     "start_time": "2025-05-13T22:20:30.259925Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc, val_acc = model.get_train_acc()\n",
    "\n",
    "\n",
    "\n",
    "x = range(1, len(train_acc) + 1)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(x, train_acc, label='Training Accuracy', color='blue', linestyle='-', marker='o')\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(x, val_acc, label='Validation Accuracy', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy per Epoch')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca063997f0a209",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2070bdd5dbc59425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:20:30.568039Z",
     "start_time": "2025-05-13T22:20:30.564695Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do predictions on the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5908c99c94f7458e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:21:57.983965Z",
     "start_time": "2025-05-13T22:21:57.575168Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_instance('midis/882.mid', 'midis/4220.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a383ff20d96fc49e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:20:30.862399Z",
     "start_time": "2025-05-13T22:20:30.802821Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions...\n",
      "Predictions written to predictions2.json\n",
      "Prediction process completed.\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(dataroot2 + \"/test.json\", \"predictions2.json\", processed_data=processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8488e81b0c94f85",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-13T03:35:45.469974Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b683782a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:49:53.467436Z",
     "start_time": "2025-05-20T19:49:53.338131Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac9eb7fcc41fe4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:35.957180Z",
     "start_time": "2025-05-20T19:48:35.955467Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance', 'blues', 'punk', 'chill', 'electronic', 'country']\n",
    "tag_to_index = {tag: i for i, tag in enumerate(TAGS)}\n",
    "\n",
    "\n",
    "# do multi-hot encoding\n",
    "\n",
    "def multi_hot_encode(tags):\n",
    "    \"\"\"\n",
    "    Given a list of tag strings, return a multi-hot encoded tensor.\n",
    "    Example input: ['jazz', 'pop']\n",
    "    Output: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
    "    \"\"\"\n",
    "    vec = torch.zeros(len(TAGS), dtype=torch.float32)\n",
    "    for tag in tags:\n",
    "        if tag in tag_to_index:\n",
    "            vec[tag_to_index[tag]] = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tag: {tag}\")\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b7e805028a7e525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:36.204961Z",
     "start_time": "2025-05-20T19:48:36.201754Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataroot3 = \"data/student_files/task3_audio_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785e4b6ec3ed4ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:36.612791Z",
     "start_time": "2025-05-20T19:48:36.610734Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model, filepath='sol_1.pt'):\n",
    "    \"\"\"Save a PyTorch model to a file\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model_class, filepath='sol_1.pt', *args, **kwargs):\n",
    "    \"\"\"Load a PyTorch model from a file\"\"\"\n",
    "    model = model_class(*args, **kwargs)  # instantiate the model\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # optional: sets dropout/batchnorm to eval mode\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f64eea8c4902a29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:00:52.185430Z",
     "start_time": "2025-05-20T20:00:52.178736Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mido import MidiFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import islice\n",
    "import librosa\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# create train loader \n",
    "\n",
    "\n",
    "def get_lowest_pitch(file_path):\n",
    "    # Initialize lowest_note to a high value (since MIDI notes are from 0 to 127)\n",
    "    lowest_note = 128  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note < lowest_note:\n",
    "                    lowest_note = msg.note\n",
    "    \n",
    "    # Return None if no note is found\n",
    "    return lowest_note if lowest_note != 128 else None\n",
    "\n",
    "def get_highest_pitch(file_path):\n",
    "    # Initialize highest_note to a low value (since MIDI notes are from 0 to 127)\n",
    "    highest_note = -1  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note > highest_note:\n",
    "                    highest_note = msg.note\n",
    "                    \n",
    "    # Return None if no note is found\n",
    "    return highest_note if highest_note != -1 else None\n",
    "\n",
    "def get_unique_pitch_num(file_path):\n",
    "    mid = MidiFile(file_path)\n",
    "    notes = set()\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.add(msg.note)\n",
    "    \n",
    "    return len(notes)\n",
    "\n",
    "def get_average_pitch_value(file_path):\n",
    "    #Q8: Your code goes here\n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    notes = []\n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.append(msg.note)\n",
    "    \n",
    "    if notes:\n",
    "        return sum(notes) / len(notes)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_waveform(path):\n",
    "    waveform, sr = librosa.load(path, sr=SAMPLE_RATE)  # waveform: 1D NumPy array\n",
    "    waveform = torch.FloatTensor(waveform)              # Convert to 1D torch tensor\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "        waveform = resample(waveform.unsqueeze(0)).squeeze(0)  # (1, N) → (N,)\n",
    "\n",
    "    # Pad or trim to target length (10 seconds)\n",
    "    target_len = SAMPLE_RATE * 10\n",
    "    if waveform.shape[0] < target_len:\n",
    "        pad_len = target_len - waveform.shape[0]\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    else:\n",
    "        waveform = waveform[:target_len]\n",
    "\n",
    "    return waveform.numpy()  # shape: (160000,)\n",
    "\n",
    "\n",
    "def extract_q(w):\n",
    "    # Your code here\n",
    "    result = librosa.cqt(y=w, sr=SAMPLE_RATE)\n",
    "    result = librosa.amplitude_to_db(np.abs(result))\n",
    "    q =torch.FloatTensor(result)\n",
    "    \n",
    "    mean = q.mean(dim=1)  # shape: (84,)\n",
    "    std = q.std(dim=1)    # shape: (84,)\n",
    "    return torch.cat([mean, std], dim=0)  # shape: (168,)\n",
    "\n",
    "def extract_mfcc(w):\n",
    "    # Your code here:\n",
    "    # load using librosa.feature.mfcc\n",
    "    mfcc = librosa.feature.mfcc(y=w, sr=SAMPLE_RATE, n_mfcc = 13)\n",
    "    # extract mean and \n",
    "    means = np.mean(mfcc, axis=1)\n",
    "    \n",
    "    stds = np.std(mfcc, axis=1)\n",
    "    # concatenate\n",
    "    features = np.concatenate([means, stds])\n",
    "    \n",
    "    return torch.FloatTensor(features)\n",
    "\n",
    "def extract_spec(w):\n",
    "    # Your code here\n",
    "    # load\n",
    "    stft = librosa.stft(y=w)\n",
    "    # take squared absolute values\n",
    "    spec = np.abs(stft) ** 2\n",
    "    \n",
    "    spec = torch.FloatTensor(spec)\n",
    "    \n",
    "    mean = spec.mean(dim=1)  # shape (128,)\n",
    "    std = spec.std(dim=1)    # shape (128,)\n",
    "    return torch.cat([mean, std], dim=0)  # shape (256,)\n",
    "\n",
    "\n",
    "import torch\n",
    "import pretty_midi\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def features(path):\n",
    "    full_path = dataroot3 + '/' + path\n",
    "    try:\n",
    "        w = extract_waveform(full_path)\n",
    "        # midi_obj = pretty_midi.PrettyMIDI(full_path)\n",
    "        # w = midi_obj.fluidsynth(fs=SAMPLE_RATE)\n",
    "\n",
    "        if w is None or len(w) < SAMPLE_RATE // 10:  # e.g. less than 0.1s\n",
    "            raise ValueError(\"Waveform too short or empty\")\n",
    "\n",
    "        mfcc = extract_mfcc(w)\n",
    "        spec = extract_spec(w)\n",
    "        q = extract_q(w)\n",
    "        \n",
    "        features = torch.cat(\n",
    "            [\n",
    "                mfcc, \n",
    "                spec, \n",
    "                q\n",
    "            ]) \n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {full_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def create_train_features(size=None, val_split=0.2, n_jobs=4):\n",
    "    # Load data\n",
    "    with open(dataroot3 + \"/train.json\", 'r') as f:\n",
    "        train_json = eval(f.read())\n",
    "    \n",
    "    # Limit size if specified\n",
    "    if size is not None:\n",
    "        train_json = dict(list(train_json.items())[:size])\n",
    "    \n",
    "    # Parallel feature extraction\n",
    "    keys = list(train_json.keys())\n",
    "    values = list(train_json.values())\n",
    "\n",
    "    X = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(features)(key) for key in keys\n",
    "    )\n",
    "    y = torch.stack([multi_hot_encode(tags) for tags in values])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
    "    Y = torch.tensor(y, dtype=torch.int64)\n",
    "    \n",
    "    # Return all data if no validation split needed\n",
    "    if val_split <= 0:\n",
    "        return X, Y\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    # X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    #     X, Y, test_size=val_split, random_state=42, shuffle=True\n",
    "    # )\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9393c96f1f25ecfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:00:53.245964Z",
     "start_time": "2025-05-20T20:00:53.239705Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00018311,  0.00024414,  0.00018311, ..., -0.06781006,\n",
       "       -0.01745605,  0.02740479], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(extract_waveform(\"data/student_files/task3_audio_classification//train/3590.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e90d615610af0da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:01:42.531346Z",
     "start_time": "2025-05-20T20:00:54.297201Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/937749269.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/937749269.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(y, dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4000, 2244]), torch.Size([4000, 10]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data, y_data  = create_train_features()\n",
    "X_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4093452b33f2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:02:18.973398Z",
     "start_time": "2025-05-20T20:02:18.960348Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "import pickle\n",
    "\n",
    "train_data_dict = {'x': X_data, 'y': y_data}\n",
    "\n",
    "with open(\"task3_train_data_3.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce4e52e6f2b58d4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"task3_train_data_3.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_data = data['x']\n",
    "y_data = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebccc7b058be32ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:50:33.244919Z",
     "start_time": "2025-05-20T20:50:33.184200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 2244) (3600, 10)\n",
      "(400, 2244) (400, 10)\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Convert tensors to numpy\n",
    "X_np = X_data.numpy()\n",
    "y_np = y_data.numpy()\n",
    "\n",
    "# Perform stratified split (e.g., 80% train, 20% val)\n",
    "X_train, y_train, X_val_global, y_val_global = iterative_train_test_split(X_np, y_np, test_size=0.1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val_global.shape, y_val_global.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11fe1e561fe8ad82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:02:24.416757Z",
     "start_time": "2025-05-20T20:02:24.414726Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244\n"
     ]
    }
   ],
   "source": [
    "print(len(X_data[0]))\n",
    "feature_size = (len(X_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a090bef86908022b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:59:26.632125Z",
     "start_time": "2025-05-20T20:59:26.623953Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def verify_data(y_train, mel=None, mfcc=None, num_classes=10):\n",
    "    \"\"\"Check label distribution and optional input stats. \n",
    "       Returns pos_weight tensor for BCEWithLogitsLoss to handle class imbalance.\n",
    "\n",
    "       Parameters:\n",
    "       - y_train (Tensor or ndarray): shape (N, num_classes), binary multi-label.\n",
    "       - mel (Tensor or ndarray): optional, for range checking.\n",
    "       - mfcc (Tensor or ndarray): optional, for range checking.\n",
    "    \"\"\"\n",
    "    if isinstance(y_train, np.ndarray):\n",
    "        y_train = torch.tensor(y_train)\n",
    "    \n",
    "    label_counter = Counter()\n",
    "    total_assignments = 0\n",
    "    sample_count = y_train.size(0)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        class_count = (y_train[:, i] == 1).sum().item()\n",
    "        label_counter[i] += class_count\n",
    "        total_assignments += class_count\n",
    "\n",
    "    print(f\"Total samples: {sample_count}\")\n",
    "    print(f\"Total class assignments (1s): {total_assignments}\\n\")\n",
    "\n",
    "    print(\"Class frequency distribution:\")\n",
    "    counts = []\n",
    "    for i in range(num_classes):\n",
    "        count = label_counter[i]\n",
    "        counts.append(count)\n",
    "        print(f\"  Class {i}: {count} assignments ({count / total_assignments:.2%})\")\n",
    "\n",
    "    # Compute pos_weight = (N - count) / count\n",
    "    label_counts_tensor = torch.tensor(counts, dtype=torch.float)\n",
    "    pos_weight = (sample_count - label_counts_tensor) / label_counts_tensor\n",
    "\n",
    "    print(\"\\nComputed pos_weight (for BCEWithLogitsLoss):\")\n",
    "    for i, w in enumerate(pos_weight):\n",
    "        print(f\"  Class {i}: {w.item():.4f}\")\n",
    "\n",
    "    # Optional: check mel / mfcc value ranges\n",
    "    if mel is not None and isinstance(mel, np.ndarray):\n",
    "        mel = torch.tensor(mel)\n",
    "    if mfcc is not None and isinstance(mfcc, np.ndarray):\n",
    "        mfcc = torch.tensor(mfcc)\n",
    "\n",
    "    if mel is not None:\n",
    "        if torch.isnan(mel).any() or torch.isinf(mel).any():\n",
    "            print(\"WARNING: NaN or Inf values found in mel data!\")\n",
    "        else:\n",
    "            print(f\"mel range: [{mel.min().item():.4f}, {mel.max().item():.4f}]\")\n",
    "\n",
    "    if mfcc is not None:\n",
    "        if torch.isnan(mfcc).any() or torch.isinf(mfcc).any():\n",
    "            print(\"WARNING: NaN or Inf values found in mfcc data!\")\n",
    "        else:\n",
    "            print(f\"mfcc range: [{mfcc.min().item():.4f}, {mfcc.max().item():.4f}]\")\n",
    "\n",
    "    return pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1b58a6c94720ea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:59:27.693773Z",
     "start_time": "2025-05-20T20:59:27.683848Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3600\n",
      "Total class assignments (1s): 4165\n",
      "\n",
      "Class frequency distribution:\n",
      "  Class 0: 1765 assignments (42.38%)\n",
      "  Class 1: 147 assignments (3.53%)\n",
      "  Class 2: 351 assignments (8.43%)\n",
      "  Class 3: 617 assignments (14.81%)\n",
      "  Class 4: 152 assignments (3.65%)\n",
      "  Class 5: 220 assignments (5.28%)\n",
      "  Class 6: 162 assignments (3.89%)\n",
      "  Class 7: 40 assignments (0.96%)\n",
      "  Class 8: 476 assignments (11.43%)\n",
      "  Class 9: 235 assignments (5.64%)\n",
      "\n",
      "Computed pos_weight (for BCEWithLogitsLoss):\n",
      "  Class 0: 1.0397\n",
      "  Class 1: 23.4898\n",
      "  Class 2: 9.2564\n",
      "  Class 3: 4.8347\n",
      "  Class 4: 22.6842\n",
      "  Class 5: 15.3636\n",
      "  Class 6: 21.2222\n",
      "  Class 7: 89.0000\n",
      "  Class 8: 6.5630\n",
      "  Class 9: 14.3191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7128, 3.1983, 2.3279, 1.7638, 3.1648, 2.7951, 3.1011, 4.4998, 2.0233,\n",
       "        2.7291])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = verify_data(y_train)\n",
    "pos_weight = torch.log1p(pos_weight) \n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "78d274fa9d5c7039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:26:25.614514Z",
     "start_time": "2025-05-20T21:26:25.565703Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=1):  # num_classes = 1 for binary output\n",
    "        super().__init__()\n",
    "        self.__init_args__ = (input_dim,)\n",
    "        self.__init_kwargs__ = {'num_classes': num_classes}\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # More expressive feature extractor\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        # Attention mechanism to emphasize important features\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),  # output matches feature dim\n",
    "            nn.Sigmoid()  # attention weights between 0 and 1\n",
    "        )\n",
    "\n",
    "        # Final classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)  # Output logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_net(x)\n",
    "        attention_weights = torch.sigmoid(self.attention(features))\n",
    "        weighted_features = features * attention_weights\n",
    "        logits = self.classifier(weighted_features)\n",
    "        return logits  # raw logit\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Intermediate representation before attention and classification\"\"\"\n",
    "        return self.feature_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c68e2dbc7c467c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:32:23.656401Z",
     "start_time": "2025-05-20T21:32:23.651503Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class SklearnMLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=100, lr=1e-3, batch_size=32, device='cpu', n_classes=1, verbose=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.n_classes = n_classes\n",
    "        self.verbose = verbose\n",
    "        self.history = []\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = MLPClassifier(input_dim=self.input_dim, num_classes=self.n_classes).to(self.device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(self.device))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import average_precision_score\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        device = torch.device(self.device)\n",
    "        if self.verbose:\n",
    "            print(f\"\\n🖥️  Using device: {device}\")\n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "        # Automatically create a validation set if not provided (e.g., when using MultiOutputClassifier)\n",
    "        if X_val is None or y_val is None:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, stratify=y_train if y_train.ndim == 1 else None\n",
    "            )\n",
    "    \n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "    \n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5)\n",
    "        best_val_map = float('-inf')\n",
    "        best_model_state = None\n",
    "        epochs_without_improvement = 0\n",
    "        patience = 20\n",
    "        start_time = time.time()\n",
    "    \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            all_preds_train, all_targets_train = [], []\n",
    "    \n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(xb)\n",
    "    \n",
    "                # Ensure yb has the same shape as logits\n",
    "                if yb.ndim == 1:\n",
    "                    yb = yb.unsqueeze(1)\n",
    "    \n",
    "                loss = self.criterion(logits, yb.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "    \n",
    "                preds = torch.sigmoid(logits).detach().cpu()\n",
    "                all_preds_train.append(preds)\n",
    "                all_targets_train.append(yb.detach().cpu())\n",
    "    \n",
    "            avg_train_loss = epoch_loss / num_batches\n",
    "            all_preds_train = torch.cat(all_preds_train).numpy()\n",
    "            all_targets_train = torch.cat(all_targets_train).numpy()\n",
    "    \n",
    "            try:\n",
    "                preds_binary_train = (all_preds_train > 0.5).astype(int)\n",
    "                train_map = average_precision_score(all_targets_train, preds_binary_train, average=\"macro\")\n",
    "\n",
    "                # train_map = average_precision_score(all_targets_train, all_preds_train, average=\"macro\")\n",
    "            except ValueError:\n",
    "                train_map = float(\"nan\")\n",
    "    \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            all_preds_val, all_targets_val = [], []\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    logits = self.model(xb)\n",
    "    \n",
    "                    if yb.ndim == 1:\n",
    "                        yb = yb.unsqueeze(1)\n",
    "    \n",
    "                    preds = torch.sigmoid(logits)\n",
    "                    loss = self.criterion(logits, yb.float())\n",
    "    \n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    all_preds_val.append(preds.cpu())\n",
    "                    all_targets_val.append(yb.cpu())\n",
    "    \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            all_preds_val = torch.cat(all_preds_val).numpy()\n",
    "            all_targets_val = torch.cat(all_targets_val).numpy()\n",
    "    \n",
    "            try:\n",
    "                preds_binary_val = (all_preds_val > 0.5).astype(int)\n",
    "                val_map = average_precision_score(all_targets_val, preds_binary_val, average=\"macro\")\n",
    "\n",
    "                # val_map = average_precision_score(all_targets_val, all_preds_val, average=\"macro\")\n",
    "            except ValueError:\n",
    "                val_map = float(\"nan\")\n",
    "    \n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            elapsed = time.time() - start_time\n",
    "    \n",
    "            if self.verbose:\n",
    "                print(f\"\\n✅ Epoch {epoch}/{self.epochs} — \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f} | Train mAP: {train_map:.4f} \"\n",
    "                      f\"| Val Loss: {avg_val_loss:.4f} | Val mAP: {val_map:.4f} \"\n",
    "                      f\"| LR: {current_lr:.6f} | Elapsed: {elapsed:.1f}s\")\n",
    "    \n",
    "            self.history.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_map\": train_map,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_map\": val_map,\n",
    "                \"lr\": current_lr\n",
    "            })\n",
    "    \n",
    "            scheduler.step(avg_val_loss)\n",
    "    \n",
    "            if val_map > best_val_map:\n",
    "                best_val_map = val_map\n",
    "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\n🛑 Early stopping at epoch {epoch}. No val mAP improvement for {patience} epochs.\")\n",
    "                    break\n",
    "    \n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "        if best_model_state:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "    \n",
    "        if self.verbose:\n",
    "            print(f\"\\n🏁 Training complete. Best val mAP: {best_val_map:.4f}. Total time: {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "        self.classes_ = [0, 1]\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
    "            print(\"logits.shape:\", logits.shape)\n",
    "        return (probs > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        return probs\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return average_precision_score(y, y_pred)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'input_dim': self.input_dim,\n",
    "            'epochs': self.epochs,\n",
    "            'lr': self.lr,\n",
    "            'batch_size': self.batch_size,\n",
    "            'device': self.device,\n",
    "            'n_classes': self.n_classes,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            setattr(self, k, v)\n",
    "        self._build_model()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2466cb4a137a317b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:15:10.327673Z",
     "start_time": "2025-05-20T21:12:38.436197Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.6037 | Train mAP: 0.6797 | Val Loss: 0.5306 | Val mAP: 0.7996 | LR: 0.001000 | Elapsed: 0.4s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.5405 | Train mAP: 0.7705 | Val Loss: 0.5040 | Val mAP: 0.8367 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.5265 | Train mAP: 0.7875 | Val Loss: 0.5096 | Val mAP: 0.8356 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.5126 | Train mAP: 0.8065 | Val Loss: 0.4856 | Val mAP: 0.8368 | LR: 0.001000 | Elapsed: 1.2s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.4902 | Train mAP: 0.8237 | Val Loss: 0.4681 | Val mAP: 0.8560 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.4936 | Train mAP: 0.8216 | Val Loss: 0.4862 | Val mAP: 0.8420 | LR: 0.001000 | Elapsed: 1.7s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.4815 | Train mAP: 0.8343 | Val Loss: 0.4682 | Val mAP: 0.8551 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.4746 | Train mAP: 0.8387 | Val Loss: 0.4692 | Val mAP: 0.8549 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.4682 | Train mAP: 0.8437 | Val Loss: 0.4611 | Val mAP: 0.8585 | LR: 0.001000 | Elapsed: 2.5s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.4547 | Train mAP: 0.8538 | Val Loss: 0.4579 | Val mAP: 0.8560 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.4573 | Train mAP: 0.8482 | Val Loss: 0.4595 | Val mAP: 0.8615 | LR: 0.001000 | Elapsed: 3.1s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.4573 | Train mAP: 0.8524 | Val Loss: 0.4694 | Val mAP: 0.8632 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.4498 | Train mAP: 0.8573 | Val Loss: 0.4758 | Val mAP: 0.8483 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.4534 | Train mAP: 0.8552 | Val Loss: 0.4614 | Val mAP: 0.8560 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.4389 | Train mAP: 0.8661 | Val Loss: 0.4737 | Val mAP: 0.8550 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.4436 | Train mAP: 0.8600 | Val Loss: 0.4633 | Val mAP: 0.8620 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.4261 | Train mAP: 0.8727 | Val Loss: 0.4628 | Val mAP: 0.8552 | LR: 0.000500 | Elapsed: 4.7s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.4251 | Train mAP: 0.8789 | Val Loss: 0.4507 | Val mAP: 0.8644 | LR: 0.000500 | Elapsed: 4.9s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.4277 | Train mAP: 0.8705 | Val Loss: 0.4587 | Val mAP: 0.8576 | LR: 0.000500 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.4152 | Train mAP: 0.8863 | Val Loss: 0.4489 | Val mAP: 0.8626 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.4068 | Train mAP: 0.8866 | Val Loss: 0.4517 | Val mAP: 0.8633 | LR: 0.000500 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.4193 | Train mAP: 0.8742 | Val Loss: 0.4557 | Val mAP: 0.8662 | LR: 0.000500 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.4136 | Train mAP: 0.8809 | Val Loss: 0.4510 | Val mAP: 0.8612 | LR: 0.000500 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.4045 | Train mAP: 0.8892 | Val Loss: 0.4524 | Val mAP: 0.8647 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.4061 | Train mAP: 0.8855 | Val Loss: 0.4521 | Val mAP: 0.8658 | LR: 0.000500 | Elapsed: 6.8s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.3953 | Train mAP: 0.8931 | Val Loss: 0.4543 | Val mAP: 0.8665 | LR: 0.000500 | Elapsed: 7.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.4054 | Train mAP: 0.8863 | Val Loss: 0.4529 | Val mAP: 0.8662 | LR: 0.000250 | Elapsed: 7.5s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.3958 | Train mAP: 0.8919 | Val Loss: 0.4473 | Val mAP: 0.8670 | LR: 0.000250 | Elapsed: 7.8s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.3936 | Train mAP: 0.8986 | Val Loss: 0.4501 | Val mAP: 0.8706 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.3888 | Train mAP: 0.8968 | Val Loss: 0.4586 | Val mAP: 0.8649 | LR: 0.000250 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.3984 | Train mAP: 0.8920 | Val Loss: 0.4476 | Val mAP: 0.8689 | LR: 0.000250 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.3809 | Train mAP: 0.9010 | Val Loss: 0.4453 | Val mAP: 0.8705 | LR: 0.000250 | Elapsed: 8.8s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.3849 | Train mAP: 0.9029 | Val Loss: 0.4502 | Val mAP: 0.8650 | LR: 0.000250 | Elapsed: 9.0s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.3847 | Train mAP: 0.9020 | Val Loss: 0.4559 | Val mAP: 0.8620 | LR: 0.000250 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.3901 | Train mAP: 0.8978 | Val Loss: 0.4422 | Val mAP: 0.8683 | LR: 0.000250 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.3769 | Train mAP: 0.9024 | Val Loss: 0.4419 | Val mAP: 0.8720 | LR: 0.000250 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.3744 | Train mAP: 0.9049 | Val Loss: 0.4433 | Val mAP: 0.8704 | LR: 0.000250 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.3785 | Train mAP: 0.9021 | Val Loss: 0.4467 | Val mAP: 0.8692 | LR: 0.000250 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.3719 | Train mAP: 0.9080 | Val Loss: 0.4392 | Val mAP: 0.8731 | LR: 0.000250 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.3870 | Train mAP: 0.8991 | Val Loss: 0.4543 | Val mAP: 0.8625 | LR: 0.000250 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.3698 | Train mAP: 0.9084 | Val Loss: 0.4463 | Val mAP: 0.8682 | LR: 0.000250 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.3697 | Train mAP: 0.9083 | Val Loss: 0.4435 | Val mAP: 0.8703 | LR: 0.000250 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.3782 | Train mAP: 0.9035 | Val Loss: 0.4420 | Val mAP: 0.8713 | LR: 0.000250 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.3699 | Train mAP: 0.9101 | Val Loss: 0.4474 | Val mAP: 0.8673 | LR: 0.000250 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.3725 | Train mAP: 0.9076 | Val Loss: 0.4405 | Val mAP: 0.8719 | LR: 0.000250 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.3632 | Train mAP: 0.9111 | Val Loss: 0.4452 | Val mAP: 0.8709 | LR: 0.000125 | Elapsed: 12.3s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.3830 | Train mAP: 0.9031 | Val Loss: 0.4422 | Val mAP: 0.8712 | LR: 0.000125 | Elapsed: 12.6s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.3543 | Train mAP: 0.9193 | Val Loss: 0.4409 | Val mAP: 0.8725 | LR: 0.000125 | Elapsed: 12.8s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.3677 | Train mAP: 0.9085 | Val Loss: 0.4447 | Val mAP: 0.8700 | LR: 0.000125 | Elapsed: 13.1s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.3600 | Train mAP: 0.9174 | Val Loss: 0.4427 | Val mAP: 0.8702 | LR: 0.000125 | Elapsed: 13.4s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.3529 | Train mAP: 0.9168 | Val Loss: 0.4438 | Val mAP: 0.8709 | LR: 0.000125 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.3563 | Train mAP: 0.9145 | Val Loss: 0.4560 | Val mAP: 0.8681 | LR: 0.000063 | Elapsed: 13.9s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.3446 | Train mAP: 0.9209 | Val Loss: 0.4483 | Val mAP: 0.8695 | LR: 0.000063 | Elapsed: 14.2s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.3600 | Train mAP: 0.9102 | Val Loss: 0.4426 | Val mAP: 0.8744 | LR: 0.000063 | Elapsed: 14.4s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.3501 | Train mAP: 0.9185 | Val Loss: 0.4484 | Val mAP: 0.8699 | LR: 0.000063 | Elapsed: 14.7s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.3460 | Train mAP: 0.9213 | Val Loss: 0.4416 | Val mAP: 0.8746 | LR: 0.000063 | Elapsed: 14.9s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.3532 | Train mAP: 0.9173 | Val Loss: 0.4420 | Val mAP: 0.8730 | LR: 0.000063 | Elapsed: 15.2s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.3576 | Train mAP: 0.9140 | Val Loss: 0.4434 | Val mAP: 0.8742 | LR: 0.000031 | Elapsed: 15.4s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.3521 | Train mAP: 0.9164 | Val Loss: 0.4433 | Val mAP: 0.8728 | LR: 0.000031 | Elapsed: 15.7s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.3422 | Train mAP: 0.9254 | Val Loss: 0.4449 | Val mAP: 0.8735 | LR: 0.000031 | Elapsed: 15.9s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.3559 | Train mAP: 0.9157 | Val Loss: 0.4465 | Val mAP: 0.8709 | LR: 0.000031 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.3475 | Train mAP: 0.9215 | Val Loss: 0.4432 | Val mAP: 0.8730 | LR: 0.000031 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.3534 | Train mAP: 0.9149 | Val Loss: 0.4405 | Val mAP: 0.8743 | LR: 0.000031 | Elapsed: 16.6s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.3480 | Train mAP: 0.9188 | Val Loss: 0.4455 | Val mAP: 0.8748 | LR: 0.000016 | Elapsed: 16.9s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.3506 | Train mAP: 0.9191 | Val Loss: 0.4437 | Val mAP: 0.8740 | LR: 0.000016 | Elapsed: 17.1s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.3539 | Train mAP: 0.9175 | Val Loss: 0.4444 | Val mAP: 0.8737 | LR: 0.000016 | Elapsed: 17.3s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.3425 | Train mAP: 0.9245 | Val Loss: 0.4429 | Val mAP: 0.8723 | LR: 0.000016 | Elapsed: 17.6s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.3445 | Train mAP: 0.9224 | Val Loss: 0.4483 | Val mAP: 0.8727 | LR: 0.000016 | Elapsed: 17.8s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.3391 | Train mAP: 0.9253 | Val Loss: 0.4406 | Val mAP: 0.8733 | LR: 0.000016 | Elapsed: 18.1s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.3427 | Train mAP: 0.9235 | Val Loss: 0.4425 | Val mAP: 0.8740 | LR: 0.000008 | Elapsed: 18.3s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.3427 | Train mAP: 0.9225 | Val Loss: 0.4456 | Val mAP: 0.8727 | LR: 0.000008 | Elapsed: 18.5s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.3582 | Train mAP: 0.9134 | Val Loss: 0.4452 | Val mAP: 0.8727 | LR: 0.000008 | Elapsed: 18.8s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.3596 | Train mAP: 0.9117 | Val Loss: 0.4436 | Val mAP: 0.8730 | LR: 0.000008 | Elapsed: 19.0s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.3407 | Train mAP: 0.9240 | Val Loss: 0.4439 | Val mAP: 0.8734 | LR: 0.000008 | Elapsed: 19.3s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.3497 | Train mAP: 0.9186 | Val Loss: 0.4459 | Val mAP: 0.8732 | LR: 0.000008 | Elapsed: 19.5s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.3508 | Train mAP: 0.9179 | Val Loss: 0.4452 | Val mAP: 0.8740 | LR: 0.000004 | Elapsed: 19.7s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.3542 | Train mAP: 0.9170 | Val Loss: 0.4490 | Val mAP: 0.8718 | LR: 0.000004 | Elapsed: 20.0s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.3505 | Train mAP: 0.9209 | Val Loss: 0.4412 | Val mAP: 0.8726 | LR: 0.000004 | Elapsed: 20.2s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.3395 | Train mAP: 0.9234 | Val Loss: 0.4443 | Val mAP: 0.8715 | LR: 0.000004 | Elapsed: 20.5s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.3512 | Train mAP: 0.9185 | Val Loss: 0.4448 | Val mAP: 0.8732 | LR: 0.000004 | Elapsed: 20.7s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.3414 | Train mAP: 0.9256 | Val Loss: 0.4428 | Val mAP: 0.8735 | LR: 0.000004 | Elapsed: 21.0s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.3445 | Train mAP: 0.9211 | Val Loss: 0.4391 | Val mAP: 0.8748 | LR: 0.000002 | Elapsed: 21.2s\n",
      "\n",
      "✅ Epoch 83/200 — Train Loss: 0.3459 | Train mAP: 0.9228 | Val Loss: 0.4415 | Val mAP: 0.8730 | LR: 0.000002 | Elapsed: 21.4s\n",
      "\n",
      "✅ Epoch 84/200 — Train Loss: 0.3444 | Train mAP: 0.9225 | Val Loss: 0.4424 | Val mAP: 0.8738 | LR: 0.000002 | Elapsed: 21.7s\n",
      "\n",
      "✅ Epoch 85/200 — Train Loss: 0.3588 | Train mAP: 0.9149 | Val Loss: 0.4447 | Val mAP: 0.8740 | LR: 0.000002 | Elapsed: 22.0s\n",
      "\n",
      "✅ Epoch 86/200 — Train Loss: 0.3488 | Train mAP: 0.9189 | Val Loss: 0.4475 | Val mAP: 0.8735 | LR: 0.000002 | Elapsed: 22.2s\n",
      "\n",
      "✅ Epoch 87/200 — Train Loss: 0.3451 | Train mAP: 0.9209 | Val Loss: 0.4394 | Val mAP: 0.8741 | LR: 0.000002 | Elapsed: 22.5s\n",
      "\n",
      "✅ Epoch 88/200 — Train Loss: 0.3391 | Train mAP: 0.9252 | Val Loss: 0.4491 | Val mAP: 0.8702 | LR: 0.000002 | Elapsed: 22.7s\n",
      "\n",
      "✅ Epoch 89/200 — Train Loss: 0.3574 | Train mAP: 0.9157 | Val Loss: 0.4445 | Val mAP: 0.8736 | LR: 0.000001 | Elapsed: 23.0s\n",
      "\n",
      "✅ Epoch 90/200 — Train Loss: 0.3432 | Train mAP: 0.9226 | Val Loss: 0.4482 | Val mAP: 0.8710 | LR: 0.000001 | Elapsed: 23.3s\n",
      "\n",
      "✅ Epoch 91/200 — Train Loss: 0.3396 | Train mAP: 0.9244 | Val Loss: 0.4433 | Val mAP: 0.8726 | LR: 0.000001 | Elapsed: 23.5s\n",
      "\n",
      "✅ Epoch 92/200 — Train Loss: 0.3441 | Train mAP: 0.9216 | Val Loss: 0.4437 | Val mAP: 0.8739 | LR: 0.000001 | Elapsed: 23.8s\n",
      "\n",
      "✅ Epoch 93/200 — Train Loss: 0.3427 | Train mAP: 0.9207 | Val Loss: 0.4483 | Val mAP: 0.8740 | LR: 0.000001 | Elapsed: 24.0s\n",
      "\n",
      "✅ Epoch 94/200 — Train Loss: 0.3424 | Train mAP: 0.9238 | Val Loss: 0.4453 | Val mAP: 0.8723 | LR: 0.000001 | Elapsed: 24.3s\n",
      "\n",
      "✅ Epoch 95/200 — Train Loss: 0.3517 | Train mAP: 0.9180 | Val Loss: 0.4444 | Val mAP: 0.8740 | LR: 0.000000 | Elapsed: 24.5s\n",
      "\n",
      "✅ Epoch 96/200 — Train Loss: 0.3456 | Train mAP: 0.9207 | Val Loss: 0.4436 | Val mAP: 0.8726 | LR: 0.000000 | Elapsed: 24.8s\n",
      "\n",
      "✅ Epoch 97/200 — Train Loss: 0.3451 | Train mAP: 0.9218 | Val Loss: 0.4439 | Val mAP: 0.8729 | LR: 0.000000 | Elapsed: 25.0s\n",
      "\n",
      "✅ Epoch 98/200 — Train Loss: 0.3458 | Train mAP: 0.9198 | Val Loss: 0.4452 | Val mAP: 0.8725 | LR: 0.000000 | Elapsed: 25.3s\n",
      "\n",
      "✅ Epoch 99/200 — Train Loss: 0.3569 | Train mAP: 0.9144 | Val Loss: 0.4475 | Val mAP: 0.8716 | LR: 0.000000 | Elapsed: 25.6s\n",
      "\n",
      "✅ Epoch 100/200 — Train Loss: 0.3439 | Train mAP: 0.9232 | Val Loss: 0.4459 | Val mAP: 0.8727 | LR: 0.000000 | Elapsed: 26.3s\n",
      "\n",
      "✅ Epoch 101/200 — Train Loss: 0.3451 | Train mAP: 0.9223 | Val Loss: 0.4435 | Val mAP: 0.8739 | LR: 0.000000 | Elapsed: 26.9s\n",
      "\n",
      "✅ Epoch 102/200 — Train Loss: 0.3427 | Train mAP: 0.9229 | Val Loss: 0.4440 | Val mAP: 0.8717 | LR: 0.000000 | Elapsed: 27.3s\n",
      "\n",
      "🛑 Early stopping at epoch 102. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.8748. Total time: 27.32s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3300 | Train mAP: 0.0376 | Val Loss: 0.2396 | Val mAP: 0.1044 | LR: 0.001000 | Elapsed: 0.4s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1815 | Train mAP: 0.0629 | Val Loss: 0.1916 | Val mAP: 0.1135 | LR: 0.001000 | Elapsed: 0.8s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1726 | Train mAP: 0.0731 | Val Loss: 0.1703 | Val mAP: 0.1317 | LR: 0.001000 | Elapsed: 1.0s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1698 | Train mAP: 0.0628 | Val Loss: 0.1564 | Val mAP: 0.1723 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1652 | Train mAP: 0.0706 | Val Loss: 0.1659 | Val mAP: 0.1901 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1668 | Train mAP: 0.0745 | Val Loss: 0.1498 | Val mAP: 0.2170 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1590 | Train mAP: 0.0979 | Val Loss: 0.1429 | Val mAP: 0.2707 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1606 | Train mAP: 0.1131 | Val Loss: 0.1492 | Val mAP: 0.2227 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1544 | Train mAP: 0.1262 | Val Loss: 0.1472 | Val mAP: 0.2324 | LR: 0.001000 | Elapsed: 2.7s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1499 | Train mAP: 0.1419 | Val Loss: 0.1452 | Val mAP: 0.2458 | LR: 0.001000 | Elapsed: 3.0s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1460 | Train mAP: 0.1572 | Val Loss: 0.1407 | Val mAP: 0.2696 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1467 | Train mAP: 0.1742 | Val Loss: 0.1397 | Val mAP: 0.2297 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1508 | Train mAP: 0.1614 | Val Loss: 0.1428 | Val mAP: 0.2419 | LR: 0.001000 | Elapsed: 4.0s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1469 | Train mAP: 0.1959 | Val Loss: 0.1557 | Val mAP: 0.2035 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1483 | Train mAP: 0.1678 | Val Loss: 0.1426 | Val mAP: 0.2655 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1465 | Train mAP: 0.1872 | Val Loss: 0.1531 | Val mAP: 0.2891 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1414 | Train mAP: 0.2220 | Val Loss: 0.1380 | Val mAP: 0.3084 | LR: 0.001000 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1468 | Train mAP: 0.1886 | Val Loss: 0.1610 | Val mAP: 0.2527 | LR: 0.001000 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1364 | Train mAP: 0.2440 | Val Loss: 0.1323 | Val mAP: 0.2993 | LR: 0.001000 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1463 | Train mAP: 0.2050 | Val Loss: 0.1510 | Val mAP: 0.2283 | LR: 0.001000 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1414 | Train mAP: 0.2176 | Val Loss: 0.1439 | Val mAP: 0.2787 | LR: 0.001000 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1378 | Train mAP: 0.2510 | Val Loss: 0.1435 | Val mAP: 0.2454 | LR: 0.001000 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1406 | Train mAP: 0.2318 | Val Loss: 0.1409 | Val mAP: 0.3290 | LR: 0.001000 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1358 | Train mAP: 0.2479 | Val Loss: 0.1393 | Val mAP: 0.2862 | LR: 0.001000 | Elapsed: 9.1s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1341 | Train mAP: 0.3133 | Val Loss: 0.1373 | Val mAP: 0.3154 | LR: 0.001000 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1310 | Train mAP: 0.2916 | Val Loss: 0.1333 | Val mAP: 0.3247 | LR: 0.000500 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1250 | Train mAP: 0.3451 | Val Loss: 0.1377 | Val mAP: 0.3106 | LR: 0.000500 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1290 | Train mAP: 0.3291 | Val Loss: 0.1413 | Val mAP: 0.3288 | LR: 0.000500 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1218 | Train mAP: 0.3834 | Val Loss: 0.1471 | Val mAP: 0.3483 | LR: 0.000500 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1284 | Train mAP: 0.3642 | Val Loss: 0.1401 | Val mAP: 0.3427 | LR: 0.000500 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1200 | Train mAP: 0.3800 | Val Loss: 0.1386 | Val mAP: 0.3594 | LR: 0.000500 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1213 | Train mAP: 0.4075 | Val Loss: 0.1411 | Val mAP: 0.3493 | LR: 0.000250 | Elapsed: 11.6s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1115 | Train mAP: 0.4503 | Val Loss: 0.1437 | Val mAP: 0.3477 | LR: 0.000250 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1186 | Train mAP: 0.4008 | Val Loss: 0.1427 | Val mAP: 0.3317 | LR: 0.000250 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1168 | Train mAP: 0.4136 | Val Loss: 0.1442 | Val mAP: 0.3311 | LR: 0.000250 | Elapsed: 12.3s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1167 | Train mAP: 0.4449 | Val Loss: 0.1442 | Val mAP: 0.3639 | LR: 0.000250 | Elapsed: 12.5s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1172 | Train mAP: 0.4157 | Val Loss: 0.1401 | Val mAP: 0.3496 | LR: 0.000250 | Elapsed: 12.8s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1140 | Train mAP: 0.4271 | Val Loss: 0.1425 | Val mAP: 0.3480 | LR: 0.000125 | Elapsed: 13.0s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1155 | Train mAP: 0.4200 | Val Loss: 0.1446 | Val mAP: 0.3250 | LR: 0.000125 | Elapsed: 13.3s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1142 | Train mAP: 0.4319 | Val Loss: 0.1439 | Val mAP: 0.3517 | LR: 0.000125 | Elapsed: 13.5s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1168 | Train mAP: 0.4099 | Val Loss: 0.1410 | Val mAP: 0.3434 | LR: 0.000125 | Elapsed: 13.7s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1102 | Train mAP: 0.4546 | Val Loss: 0.1402 | Val mAP: 0.3371 | LR: 0.000125 | Elapsed: 14.0s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1097 | Train mAP: 0.4900 | Val Loss: 0.1390 | Val mAP: 0.3494 | LR: 0.000125 | Elapsed: 14.2s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1088 | Train mAP: 0.4788 | Val Loss: 0.1405 | Val mAP: 0.3595 | LR: 0.000063 | Elapsed: 14.4s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1111 | Train mAP: 0.4660 | Val Loss: 0.1398 | Val mAP: 0.3501 | LR: 0.000063 | Elapsed: 14.7s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1115 | Train mAP: 0.4557 | Val Loss: 0.1416 | Val mAP: 0.3493 | LR: 0.000063 | Elapsed: 15.0s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1133 | Train mAP: 0.4315 | Val Loss: 0.1400 | Val mAP: 0.3768 | LR: 0.000063 | Elapsed: 15.2s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1152 | Train mAP: 0.4259 | Val Loss: 0.1423 | Val mAP: 0.3643 | LR: 0.000063 | Elapsed: 15.4s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1078 | Train mAP: 0.4587 | Val Loss: 0.1414 | Val mAP: 0.3695 | LR: 0.000063 | Elapsed: 15.6s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1077 | Train mAP: 0.4917 | Val Loss: 0.1390 | Val mAP: 0.3783 | LR: 0.000031 | Elapsed: 15.9s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1097 | Train mAP: 0.4768 | Val Loss: 0.1420 | Val mAP: 0.3886 | LR: 0.000031 | Elapsed: 16.1s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1063 | Train mAP: 0.5078 | Val Loss: 0.1397 | Val mAP: 0.3672 | LR: 0.000031 | Elapsed: 16.3s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1100 | Train mAP: 0.4651 | Val Loss: 0.1375 | Val mAP: 0.3750 | LR: 0.000031 | Elapsed: 16.6s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1097 | Train mAP: 0.4555 | Val Loss: 0.1402 | Val mAP: 0.3828 | LR: 0.000031 | Elapsed: 16.8s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1079 | Train mAP: 0.4655 | Val Loss: 0.1393 | Val mAP: 0.3598 | LR: 0.000031 | Elapsed: 17.0s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1133 | Train mAP: 0.4623 | Val Loss: 0.1377 | Val mAP: 0.3791 | LR: 0.000016 | Elapsed: 17.2s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.1113 | Train mAP: 0.4660 | Val Loss: 0.1390 | Val mAP: 0.3792 | LR: 0.000016 | Elapsed: 17.5s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.1072 | Train mAP: 0.4835 | Val Loss: 0.1411 | Val mAP: 0.3817 | LR: 0.000016 | Elapsed: 17.7s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.1103 | Train mAP: 0.4639 | Val Loss: 0.1403 | Val mAP: 0.3799 | LR: 0.000016 | Elapsed: 17.9s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.1167 | Train mAP: 0.4246 | Val Loss: 0.1396 | Val mAP: 0.3747 | LR: 0.000016 | Elapsed: 18.2s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.1070 | Train mAP: 0.4840 | Val Loss: 0.1384 | Val mAP: 0.3760 | LR: 0.000016 | Elapsed: 18.4s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.1072 | Train mAP: 0.4582 | Val Loss: 0.1395 | Val mAP: 0.3792 | LR: 0.000008 | Elapsed: 18.7s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.1063 | Train mAP: 0.4829 | Val Loss: 0.1407 | Val mAP: 0.3803 | LR: 0.000008 | Elapsed: 18.9s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.1113 | Train mAP: 0.4346 | Val Loss: 0.1407 | Val mAP: 0.3824 | LR: 0.000008 | Elapsed: 19.1s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.1098 | Train mAP: 0.4626 | Val Loss: 0.1399 | Val mAP: 0.3771 | LR: 0.000008 | Elapsed: 19.4s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.1054 | Train mAP: 0.4957 | Val Loss: 0.1403 | Val mAP: 0.3826 | LR: 0.000008 | Elapsed: 19.6s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.1056 | Train mAP: 0.4860 | Val Loss: 0.1393 | Val mAP: 0.3796 | LR: 0.000008 | Elapsed: 19.8s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.1066 | Train mAP: 0.4802 | Val Loss: 0.1405 | Val mAP: 0.3794 | LR: 0.000004 | Elapsed: 20.1s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.1082 | Train mAP: 0.4872 | Val Loss: 0.1395 | Val mAP: 0.3731 | LR: 0.000004 | Elapsed: 20.3s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.1120 | Train mAP: 0.4551 | Val Loss: 0.1391 | Val mAP: 0.3807 | LR: 0.000004 | Elapsed: 20.5s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.1078 | Train mAP: 0.5004 | Val Loss: 0.1401 | Val mAP: 0.3717 | LR: 0.000004 | Elapsed: 20.8s\n",
      "\n",
      "🛑 Early stopping at epoch 71. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.3886. Total time: 20.77s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3729 | Train mAP: 0.1528 | Val Loss: 0.3342 | Val mAP: 0.3528 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.2887 | Train mAP: 0.2851 | Val Loss: 0.2998 | Val mAP: 0.3130 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.2836 | Train mAP: 0.3015 | Val Loss: 0.2718 | Val mAP: 0.3900 | LR: 0.001000 | Elapsed: 0.8s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.2686 | Train mAP: 0.3596 | Val Loss: 0.2640 | Val mAP: 0.4229 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.2514 | Train mAP: 0.3944 | Val Loss: 0.2528 | Val mAP: 0.4820 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.2477 | Train mAP: 0.4354 | Val Loss: 0.2402 | Val mAP: 0.4817 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.2369 | Train mAP: 0.4652 | Val Loss: 0.2429 | Val mAP: 0.5026 | LR: 0.001000 | Elapsed: 1.8s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.2411 | Train mAP: 0.4657 | Val Loss: 0.2299 | Val mAP: 0.5356 | LR: 0.001000 | Elapsed: 2.1s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.2353 | Train mAP: 0.4838 | Val Loss: 0.2386 | Val mAP: 0.5132 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.2317 | Train mAP: 0.4825 | Val Loss: 0.2316 | Val mAP: 0.5287 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.2281 | Train mAP: 0.5265 | Val Loss: 0.2283 | Val mAP: 0.5457 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.2286 | Train mAP: 0.4956 | Val Loss: 0.2392 | Val mAP: 0.5149 | LR: 0.001000 | Elapsed: 3.1s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.2269 | Train mAP: 0.5172 | Val Loss: 0.2283 | Val mAP: 0.5483 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.2199 | Train mAP: 0.5497 | Val Loss: 0.2251 | Val mAP: 0.5530 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.2120 | Train mAP: 0.5623 | Val Loss: 0.2206 | Val mAP: 0.5738 | LR: 0.001000 | Elapsed: 3.8s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.2130 | Train mAP: 0.5705 | Val Loss: 0.2290 | Val mAP: 0.5153 | LR: 0.001000 | Elapsed: 4.0s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.2043 | Train mAP: 0.5873 | Val Loss: 0.2253 | Val mAP: 0.5240 | LR: 0.001000 | Elapsed: 4.2s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.2070 | Train mAP: 0.6079 | Val Loss: 0.2255 | Val mAP: 0.5735 | LR: 0.001000 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.2079 | Train mAP: 0.5968 | Val Loss: 0.2201 | Val mAP: 0.5618 | LR: 0.001000 | Elapsed: 4.7s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1975 | Train mAP: 0.6101 | Val Loss: 0.2297 | Val mAP: 0.5251 | LR: 0.001000 | Elapsed: 4.9s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.2013 | Train mAP: 0.6182 | Val Loss: 0.2243 | Val mAP: 0.5578 | LR: 0.001000 | Elapsed: 5.1s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.2042 | Train mAP: 0.6151 | Val Loss: 0.2250 | Val mAP: 0.5644 | LR: 0.001000 | Elapsed: 5.4s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1973 | Train mAP: 0.6258 | Val Loss: 0.2252 | Val mAP: 0.5330 | LR: 0.001000 | Elapsed: 5.6s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.2024 | Train mAP: 0.6097 | Val Loss: 0.2267 | Val mAP: 0.5383 | LR: 0.001000 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1888 | Train mAP: 0.6479 | Val Loss: 0.2229 | Val mAP: 0.5815 | LR: 0.001000 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1970 | Train mAP: 0.6369 | Val Loss: 0.2252 | Val mAP: 0.5620 | LR: 0.000500 | Elapsed: 6.2s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1886 | Train mAP: 0.6546 | Val Loss: 0.2206 | Val mAP: 0.5890 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1830 | Train mAP: 0.6794 | Val Loss: 0.2273 | Val mAP: 0.5871 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1812 | Train mAP: 0.6711 | Val Loss: 0.2277 | Val mAP: 0.6056 | LR: 0.000500 | Elapsed: 6.9s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1761 | Train mAP: 0.6885 | Val Loss: 0.2203 | Val mAP: 0.5765 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1748 | Train mAP: 0.6971 | Val Loss: 0.2260 | Val mAP: 0.6031 | LR: 0.000500 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1661 | Train mAP: 0.7220 | Val Loss: 0.2190 | Val mAP: 0.5706 | LR: 0.000250 | Elapsed: 7.6s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1727 | Train mAP: 0.7052 | Val Loss: 0.2191 | Val mAP: 0.5821 | LR: 0.000250 | Elapsed: 7.9s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1678 | Train mAP: 0.7249 | Val Loss: 0.2163 | Val mAP: 0.6039 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1663 | Train mAP: 0.7258 | Val Loss: 0.2182 | Val mAP: 0.5850 | LR: 0.000250 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1667 | Train mAP: 0.7242 | Val Loss: 0.2151 | Val mAP: 0.6074 | LR: 0.000250 | Elapsed: 8.5s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1647 | Train mAP: 0.7317 | Val Loss: 0.2195 | Val mAP: 0.5825 | LR: 0.000250 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1623 | Train mAP: 0.7308 | Val Loss: 0.2166 | Val mAP: 0.5958 | LR: 0.000250 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1604 | Train mAP: 0.7413 | Val Loss: 0.2126 | Val mAP: 0.5946 | LR: 0.000250 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1588 | Train mAP: 0.7460 | Val Loss: 0.2227 | Val mAP: 0.6053 | LR: 0.000250 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1709 | Train mAP: 0.7130 | Val Loss: 0.2241 | Val mAP: 0.6034 | LR: 0.000250 | Elapsed: 9.6s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1693 | Train mAP: 0.7202 | Val Loss: 0.2160 | Val mAP: 0.5974 | LR: 0.000250 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1593 | Train mAP: 0.7399 | Val Loss: 0.2136 | Val mAP: 0.6058 | LR: 0.000250 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1595 | Train mAP: 0.7404 | Val Loss: 0.2111 | Val mAP: 0.6073 | LR: 0.000250 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1647 | Train mAP: 0.7266 | Val Loss: 0.2226 | Val mAP: 0.5913 | LR: 0.000250 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1594 | Train mAP: 0.7417 | Val Loss: 0.2281 | Val mAP: 0.5884 | LR: 0.000250 | Elapsed: 10.7s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1552 | Train mAP: 0.7540 | Val Loss: 0.2209 | Val mAP: 0.5766 | LR: 0.000250 | Elapsed: 10.9s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1635 | Train mAP: 0.7374 | Val Loss: 0.2204 | Val mAP: 0.5705 | LR: 0.000250 | Elapsed: 11.1s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1547 | Train mAP: 0.7561 | Val Loss: 0.2197 | Val mAP: 0.5901 | LR: 0.000250 | Elapsed: 11.4s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1498 | Train mAP: 0.7746 | Val Loss: 0.2173 | Val mAP: 0.5760 | LR: 0.000250 | Elapsed: 11.6s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1507 | Train mAP: 0.7676 | Val Loss: 0.2178 | Val mAP: 0.5601 | LR: 0.000125 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1495 | Train mAP: 0.7800 | Val Loss: 0.2175 | Val mAP: 0.5803 | LR: 0.000125 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1492 | Train mAP: 0.7759 | Val Loss: 0.2136 | Val mAP: 0.5952 | LR: 0.000125 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1482 | Train mAP: 0.7745 | Val Loss: 0.2186 | Val mAP: 0.5840 | LR: 0.000125 | Elapsed: 12.5s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1490 | Train mAP: 0.7759 | Val Loss: 0.2115 | Val mAP: 0.6001 | LR: 0.000125 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1548 | Train mAP: 0.7599 | Val Loss: 0.2144 | Val mAP: 0.6003 | LR: 0.000125 | Elapsed: 12.9s\n",
      "\n",
      "🛑 Early stopping at epoch 56. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.6074. Total time: 12.91s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.5057 | Train mAP: 0.2166 | Val Loss: 0.4329 | Val mAP: 0.4335 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.4217 | Train mAP: 0.3307 | Val Loss: 0.3771 | Val mAP: 0.5157 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.3965 | Train mAP: 0.4011 | Val Loss: 0.3622 | Val mAP: 0.5121 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.3934 | Train mAP: 0.3905 | Val Loss: 0.3688 | Val mAP: 0.5112 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.3722 | Train mAP: 0.4481 | Val Loss: 0.3520 | Val mAP: 0.5409 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.3719 | Train mAP: 0.4518 | Val Loss: 0.3467 | Val mAP: 0.5443 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.3682 | Train mAP: 0.4546 | Val Loss: 0.3664 | Val mAP: 0.5358 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.3683 | Train mAP: 0.4605 | Val Loss: 0.3600 | Val mAP: 0.5246 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.3629 | Train mAP: 0.4648 | Val Loss: 0.3616 | Val mAP: 0.5530 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.3587 | Train mAP: 0.4734 | Val Loss: 0.3435 | Val mAP: 0.5618 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.3522 | Train mAP: 0.5052 | Val Loss: 0.3514 | Val mAP: 0.5699 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.3510 | Train mAP: 0.4860 | Val Loss: 0.3586 | Val mAP: 0.5268 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.3472 | Train mAP: 0.5126 | Val Loss: 0.3397 | Val mAP: 0.5637 | LR: 0.001000 | Elapsed: 3.0s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.3385 | Train mAP: 0.5227 | Val Loss: 0.3486 | Val mAP: 0.5616 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.3448 | Train mAP: 0.5149 | Val Loss: 0.3416 | Val mAP: 0.5792 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.3328 | Train mAP: 0.5519 | Val Loss: 0.3395 | Val mAP: 0.5737 | LR: 0.001000 | Elapsed: 3.7s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.3472 | Train mAP: 0.5196 | Val Loss: 0.3490 | Val mAP: 0.5702 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.3374 | Train mAP: 0.5317 | Val Loss: 0.3631 | Val mAP: 0.5570 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.3329 | Train mAP: 0.5364 | Val Loss: 0.3506 | Val mAP: 0.5495 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.3316 | Train mAP: 0.5528 | Val Loss: 0.3379 | Val mAP: 0.5803 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.3254 | Train mAP: 0.5590 | Val Loss: 0.3470 | Val mAP: 0.5544 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.3207 | Train mAP: 0.5835 | Val Loss: 0.3434 | Val mAP: 0.5697 | LR: 0.001000 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.3176 | Train mAP: 0.5965 | Val Loss: 0.3555 | Val mAP: 0.5740 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.3239 | Train mAP: 0.5936 | Val Loss: 0.3446 | Val mAP: 0.5773 | LR: 0.001000 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.3210 | Train mAP: 0.5879 | Val Loss: 0.3475 | Val mAP: 0.5580 | LR: 0.001000 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.3218 | Train mAP: 0.5757 | Val Loss: 0.3523 | Val mAP: 0.5415 | LR: 0.001000 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.3152 | Train mAP: 0.6106 | Val Loss: 0.3378 | Val mAP: 0.5720 | LR: 0.000500 | Elapsed: 6.4s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.3103 | Train mAP: 0.5997 | Val Loss: 0.3460 | Val mAP: 0.5671 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.3048 | Train mAP: 0.6307 | Val Loss: 0.3420 | Val mAP: 0.5702 | LR: 0.000500 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.3132 | Train mAP: 0.5866 | Val Loss: 0.3474 | Val mAP: 0.5852 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.3037 | Train mAP: 0.6111 | Val Loss: 0.3419 | Val mAP: 0.5670 | LR: 0.000500 | Elapsed: 7.5s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.3037 | Train mAP: 0.6239 | Val Loss: 0.3504 | Val mAP: 0.5648 | LR: 0.000500 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.3022 | Train mAP: 0.6183 | Val Loss: 0.3495 | Val mAP: 0.5616 | LR: 0.000500 | Elapsed: 8.0s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.3008 | Train mAP: 0.6206 | Val Loss: 0.3488 | Val mAP: 0.5727 | LR: 0.000250 | Elapsed: 8.2s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.2912 | Train mAP: 0.6518 | Val Loss: 0.3526 | Val mAP: 0.5651 | LR: 0.000250 | Elapsed: 8.4s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.2957 | Train mAP: 0.6454 | Val Loss: 0.3494 | Val mAP: 0.5765 | LR: 0.000250 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.2970 | Train mAP: 0.6302 | Val Loss: 0.3461 | Val mAP: 0.5835 | LR: 0.000250 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.2895 | Train mAP: 0.6501 | Val Loss: 0.3436 | Val mAP: 0.5788 | LR: 0.000250 | Elapsed: 9.1s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.2820 | Train mAP: 0.6703 | Val Loss: 0.3463 | Val mAP: 0.5676 | LR: 0.000250 | Elapsed: 9.3s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.2820 | Train mAP: 0.6625 | Val Loss: 0.3454 | Val mAP: 0.5753 | LR: 0.000125 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.2827 | Train mAP: 0.6664 | Val Loss: 0.3481 | Val mAP: 0.5803 | LR: 0.000125 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.2904 | Train mAP: 0.6461 | Val Loss: 0.3447 | Val mAP: 0.5845 | LR: 0.000125 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.2819 | Train mAP: 0.6612 | Val Loss: 0.3449 | Val mAP: 0.5745 | LR: 0.000125 | Elapsed: 10.2s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.2843 | Train mAP: 0.6532 | Val Loss: 0.3462 | Val mAP: 0.5718 | LR: 0.000125 | Elapsed: 10.4s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.2753 | Train mAP: 0.6703 | Val Loss: 0.3473 | Val mAP: 0.5711 | LR: 0.000125 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.2787 | Train mAP: 0.6804 | Val Loss: 0.3491 | Val mAP: 0.5820 | LR: 0.000063 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.2813 | Train mAP: 0.6758 | Val Loss: 0.3456 | Val mAP: 0.5855 | LR: 0.000063 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.2659 | Train mAP: 0.7021 | Val Loss: 0.3433 | Val mAP: 0.5810 | LR: 0.000063 | Elapsed: 11.2s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.2929 | Train mAP: 0.6430 | Val Loss: 0.3450 | Val mAP: 0.5831 | LR: 0.000063 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.2927 | Train mAP: 0.6428 | Val Loss: 0.3413 | Val mAP: 0.5795 | LR: 0.000063 | Elapsed: 11.7s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.2729 | Train mAP: 0.6884 | Val Loss: 0.3446 | Val mAP: 0.5807 | LR: 0.000063 | Elapsed: 11.9s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.2829 | Train mAP: 0.6642 | Val Loss: 0.3421 | Val mAP: 0.5851 | LR: 0.000031 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.2749 | Train mAP: 0.6846 | Val Loss: 0.3440 | Val mAP: 0.5792 | LR: 0.000031 | Elapsed: 12.4s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.2772 | Train mAP: 0.6777 | Val Loss: 0.3454 | Val mAP: 0.5836 | LR: 0.000031 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.2750 | Train mAP: 0.6796 | Val Loss: 0.3459 | Val mAP: 0.5880 | LR: 0.000031 | Elapsed: 12.9s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.2840 | Train mAP: 0.6589 | Val Loss: 0.3444 | Val mAP: 0.5869 | LR: 0.000031 | Elapsed: 13.1s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.2784 | Train mAP: 0.6677 | Val Loss: 0.3438 | Val mAP: 0.5925 | LR: 0.000031 | Elapsed: 13.3s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.2807 | Train mAP: 0.6780 | Val Loss: 0.3450 | Val mAP: 0.5865 | LR: 0.000016 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.2758 | Train mAP: 0.6786 | Val Loss: 0.3452 | Val mAP: 0.5825 | LR: 0.000016 | Elapsed: 13.8s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.2677 | Train mAP: 0.6855 | Val Loss: 0.3448 | Val mAP: 0.5841 | LR: 0.000016 | Elapsed: 14.0s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.2678 | Train mAP: 0.6932 | Val Loss: 0.3445 | Val mAP: 0.5903 | LR: 0.000016 | Elapsed: 14.4s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.2737 | Train mAP: 0.6783 | Val Loss: 0.3441 | Val mAP: 0.5910 | LR: 0.000016 | Elapsed: 14.8s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.2746 | Train mAP: 0.6764 | Val Loss: 0.3419 | Val mAP: 0.5895 | LR: 0.000016 | Elapsed: 15.3s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.2739 | Train mAP: 0.6911 | Val Loss: 0.3434 | Val mAP: 0.5901 | LR: 0.000008 | Elapsed: 15.7s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.2776 | Train mAP: 0.6847 | Val Loss: 0.3412 | Val mAP: 0.5930 | LR: 0.000008 | Elapsed: 16.0s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.2763 | Train mAP: 0.6825 | Val Loss: 0.3451 | Val mAP: 0.5838 | LR: 0.000008 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.2813 | Train mAP: 0.6704 | Val Loss: 0.3484 | Val mAP: 0.5816 | LR: 0.000008 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.2834 | Train mAP: 0.6738 | Val Loss: 0.3455 | Val mAP: 0.5948 | LR: 0.000008 | Elapsed: 16.6s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.2723 | Train mAP: 0.6914 | Val Loss: 0.3419 | Val mAP: 0.5867 | LR: 0.000008 | Elapsed: 16.8s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.2671 | Train mAP: 0.6936 | Val Loss: 0.3443 | Val mAP: 0.5880 | LR: 0.000004 | Elapsed: 17.1s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.2795 | Train mAP: 0.6636 | Val Loss: 0.3464 | Val mAP: 0.5825 | LR: 0.000004 | Elapsed: 17.3s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.2730 | Train mAP: 0.6774 | Val Loss: 0.3423 | Val mAP: 0.5833 | LR: 0.000004 | Elapsed: 17.5s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.2725 | Train mAP: 0.6995 | Val Loss: 0.3472 | Val mAP: 0.5788 | LR: 0.000004 | Elapsed: 17.7s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.2694 | Train mAP: 0.6901 | Val Loss: 0.3471 | Val mAP: 0.5811 | LR: 0.000004 | Elapsed: 18.0s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.2750 | Train mAP: 0.6870 | Val Loss: 0.3433 | Val mAP: 0.5861 | LR: 0.000004 | Elapsed: 18.2s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.2746 | Train mAP: 0.6842 | Val Loss: 0.3436 | Val mAP: 0.5951 | LR: 0.000002 | Elapsed: 18.4s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.2771 | Train mAP: 0.6721 | Val Loss: 0.3444 | Val mAP: 0.5919 | LR: 0.000002 | Elapsed: 18.6s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.2707 | Train mAP: 0.6925 | Val Loss: 0.3409 | Val mAP: 0.5906 | LR: 0.000002 | Elapsed: 18.8s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.2681 | Train mAP: 0.6992 | Val Loss: 0.3455 | Val mAP: 0.5872 | LR: 0.000002 | Elapsed: 19.1s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.2803 | Train mAP: 0.6825 | Val Loss: 0.3414 | Val mAP: 0.5939 | LR: 0.000002 | Elapsed: 19.3s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.2685 | Train mAP: 0.6931 | Val Loss: 0.3410 | Val mAP: 0.5898 | LR: 0.000002 | Elapsed: 19.5s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.2697 | Train mAP: 0.6871 | Val Loss: 0.3455 | Val mAP: 0.5808 | LR: 0.000001 | Elapsed: 19.7s\n",
      "\n",
      "✅ Epoch 83/200 — Train Loss: 0.2736 | Train mAP: 0.6725 | Val Loss: 0.3408 | Val mAP: 0.5936 | LR: 0.000001 | Elapsed: 19.9s\n",
      "\n",
      "✅ Epoch 84/200 — Train Loss: 0.2742 | Train mAP: 0.6743 | Val Loss: 0.3440 | Val mAP: 0.5862 | LR: 0.000001 | Elapsed: 20.1s\n",
      "\n",
      "✅ Epoch 85/200 — Train Loss: 0.2678 | Train mAP: 0.6924 | Val Loss: 0.3459 | Val mAP: 0.5903 | LR: 0.000001 | Elapsed: 20.4s\n",
      "\n",
      "✅ Epoch 86/200 — Train Loss: 0.2845 | Train mAP: 0.6638 | Val Loss: 0.3411 | Val mAP: 0.5853 | LR: 0.000001 | Elapsed: 20.6s\n",
      "\n",
      "✅ Epoch 87/200 — Train Loss: 0.2745 | Train mAP: 0.6826 | Val Loss: 0.3464 | Val mAP: 0.5831 | LR: 0.000001 | Elapsed: 20.8s\n",
      "\n",
      "✅ Epoch 88/200 — Train Loss: 0.2705 | Train mAP: 0.6997 | Val Loss: 0.3469 | Val mAP: 0.5861 | LR: 0.000000 | Elapsed: 21.0s\n",
      "\n",
      "✅ Epoch 89/200 — Train Loss: 0.2728 | Train mAP: 0.6803 | Val Loss: 0.3462 | Val mAP: 0.5806 | LR: 0.000000 | Elapsed: 21.2s\n",
      "\n",
      "✅ Epoch 90/200 — Train Loss: 0.2722 | Train mAP: 0.6802 | Val Loss: 0.3445 | Val mAP: 0.5882 | LR: 0.000000 | Elapsed: 21.4s\n",
      "\n",
      "✅ Epoch 91/200 — Train Loss: 0.2730 | Train mAP: 0.6862 | Val Loss: 0.3457 | Val mAP: 0.5874 | LR: 0.000000 | Elapsed: 21.7s\n",
      "\n",
      "✅ Epoch 92/200 — Train Loss: 0.2759 | Train mAP: 0.6833 | Val Loss: 0.3442 | Val mAP: 0.5900 | LR: 0.000000 | Elapsed: 21.9s\n",
      "\n",
      "✅ Epoch 93/200 — Train Loss: 0.2729 | Train mAP: 0.6849 | Val Loss: 0.3455 | Val mAP: 0.5952 | LR: 0.000000 | Elapsed: 22.1s\n",
      "\n",
      "✅ Epoch 94/200 — Train Loss: 0.2676 | Train mAP: 0.6839 | Val Loss: 0.3405 | Val mAP: 0.5896 | LR: 0.000000 | Elapsed: 22.3s\n",
      "\n",
      "✅ Epoch 95/200 — Train Loss: 0.2677 | Train mAP: 0.6923 | Val Loss: 0.3424 | Val mAP: 0.5846 | LR: 0.000000 | Elapsed: 22.5s\n",
      "\n",
      "✅ Epoch 96/200 — Train Loss: 0.2749 | Train mAP: 0.6718 | Val Loss: 0.3437 | Val mAP: 0.5895 | LR: 0.000000 | Elapsed: 22.7s\n",
      "\n",
      "✅ Epoch 97/200 — Train Loss: 0.2742 | Train mAP: 0.6871 | Val Loss: 0.3466 | Val mAP: 0.5828 | LR: 0.000000 | Elapsed: 23.0s\n",
      "\n",
      "✅ Epoch 98/200 — Train Loss: 0.2776 | Train mAP: 0.6738 | Val Loss: 0.3432 | Val mAP: 0.5841 | LR: 0.000000 | Elapsed: 23.2s\n",
      "\n",
      "✅ Epoch 99/200 — Train Loss: 0.2637 | Train mAP: 0.7032 | Val Loss: 0.3430 | Val mAP: 0.5820 | LR: 0.000000 | Elapsed: 23.4s\n",
      "\n",
      "✅ Epoch 100/200 — Train Loss: 0.2758 | Train mAP: 0.6817 | Val Loss: 0.3428 | Val mAP: 0.5920 | LR: 0.000000 | Elapsed: 23.6s\n",
      "\n",
      "✅ Epoch 101/200 — Train Loss: 0.2681 | Train mAP: 0.6921 | Val Loss: 0.3452 | Val mAP: 0.5889 | LR: 0.000000 | Elapsed: 23.8s\n",
      "\n",
      "✅ Epoch 102/200 — Train Loss: 0.2781 | Train mAP: 0.6797 | Val Loss: 0.3431 | Val mAP: 0.5880 | LR: 0.000000 | Elapsed: 24.0s\n",
      "\n",
      "✅ Epoch 103/200 — Train Loss: 0.2721 | Train mAP: 0.6791 | Val Loss: 0.3443 | Val mAP: 0.5862 | LR: 0.000000 | Elapsed: 24.2s\n",
      "\n",
      "✅ Epoch 104/200 — Train Loss: 0.2789 | Train mAP: 0.6732 | Val Loss: 0.3414 | Val mAP: 0.5899 | LR: 0.000000 | Elapsed: 24.5s\n",
      "\n",
      "✅ Epoch 105/200 — Train Loss: 0.2743 | Train mAP: 0.6800 | Val Loss: 0.3404 | Val mAP: 0.5944 | LR: 0.000000 | Elapsed: 24.7s\n",
      "\n",
      "✅ Epoch 106/200 — Train Loss: 0.2742 | Train mAP: 0.6731 | Val Loss: 0.3443 | Val mAP: 0.5857 | LR: 0.000000 | Elapsed: 24.9s\n",
      "\n",
      "✅ Epoch 107/200 — Train Loss: 0.2827 | Train mAP: 0.6759 | Val Loss: 0.3450 | Val mAP: 0.5880 | LR: 0.000000 | Elapsed: 25.1s\n",
      "\n",
      "✅ Epoch 108/200 — Train Loss: 0.2656 | Train mAP: 0.6862 | Val Loss: 0.3456 | Val mAP: 0.5824 | LR: 0.000000 | Elapsed: 25.3s\n",
      "\n",
      "✅ Epoch 109/200 — Train Loss: 0.2685 | Train mAP: 0.6988 | Val Loss: 0.3459 | Val mAP: 0.5887 | LR: 0.000000 | Elapsed: 25.6s\n",
      "\n",
      "✅ Epoch 110/200 — Train Loss: 0.2767 | Train mAP: 0.6705 | Val Loss: 0.3446 | Val mAP: 0.5829 | LR: 0.000000 | Elapsed: 25.8s\n",
      "\n",
      "✅ Epoch 111/200 — Train Loss: 0.2712 | Train mAP: 0.6870 | Val Loss: 0.3452 | Val mAP: 0.5850 | LR: 0.000000 | Elapsed: 26.0s\n",
      "\n",
      "✅ Epoch 112/200 — Train Loss: 0.2685 | Train mAP: 0.6877 | Val Loss: 0.3441 | Val mAP: 0.5917 | LR: 0.000000 | Elapsed: 26.2s\n",
      "\n",
      "✅ Epoch 113/200 — Train Loss: 0.2770 | Train mAP: 0.6850 | Val Loss: 0.3415 | Val mAP: 0.5893 | LR: 0.000000 | Elapsed: 26.4s\n",
      "\n",
      "🛑 Early stopping at epoch 113. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.5952. Total time: 26.42s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.2835 | Train mAP: 0.0432 | Val Loss: 0.2162 | Val mAP: 0.0997 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1856 | Train mAP: 0.0591 | Val Loss: 0.2025 | Val mAP: 0.0736 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1770 | Train mAP: 0.0747 | Val Loss: 0.1877 | Val mAP: 0.1029 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1705 | Train mAP: 0.0869 | Val Loss: 0.1702 | Val mAP: 0.1164 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1733 | Train mAP: 0.0888 | Val Loss: 0.1818 | Val mAP: 0.1054 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1609 | Train mAP: 0.1074 | Val Loss: 0.1718 | Val mAP: 0.1382 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1664 | Train mAP: 0.0949 | Val Loss: 0.1666 | Val mAP: 0.1237 | LR: 0.001000 | Elapsed: 1.7s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1605 | Train mAP: 0.1258 | Val Loss: 0.1671 | Val mAP: 0.1268 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1551 | Train mAP: 0.1478 | Val Loss: 0.1591 | Val mAP: 0.1491 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1570 | Train mAP: 0.1447 | Val Loss: 0.1653 | Val mAP: 0.1302 | LR: 0.001000 | Elapsed: 2.5s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1519 | Train mAP: 0.1723 | Val Loss: 0.1623 | Val mAP: 0.1286 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1592 | Train mAP: 0.1231 | Val Loss: 0.1660 | Val mAP: 0.1210 | LR: 0.001000 | Elapsed: 3.0s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1548 | Train mAP: 0.1481 | Val Loss: 0.1646 | Val mAP: 0.1313 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1475 | Train mAP: 0.2004 | Val Loss: 0.1627 | Val mAP: 0.1228 | LR: 0.001000 | Elapsed: 3.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1392 | Train mAP: 0.2134 | Val Loss: 0.1741 | Val mAP: 0.1288 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1400 | Train mAP: 0.2116 | Val Loss: 0.1615 | Val mAP: 0.1431 | LR: 0.000500 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1384 | Train mAP: 0.2475 | Val Loss: 0.1631 | Val mAP: 0.1551 | LR: 0.000500 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1430 | Train mAP: 0.2197 | Val Loss: 0.1685 | Val mAP: 0.1346 | LR: 0.000500 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1362 | Train mAP: 0.2399 | Val Loss: 0.1650 | Val mAP: 0.1623 | LR: 0.000500 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1319 | Train mAP: 0.2641 | Val Loss: 0.1652 | Val mAP: 0.1672 | LR: 0.000500 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1337 | Train mAP: 0.2641 | Val Loss: 0.1654 | Val mAP: 0.1816 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1287 | Train mAP: 0.2778 | Val Loss: 0.1658 | Val mAP: 0.1742 | LR: 0.000250 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1286 | Train mAP: 0.3215 | Val Loss: 0.1605 | Val mAP: 0.1790 | LR: 0.000250 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1244 | Train mAP: 0.3246 | Val Loss: 0.1671 | Val mAP: 0.1725 | LR: 0.000250 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1323 | Train mAP: 0.2782 | Val Loss: 0.1601 | Val mAP: 0.1837 | LR: 0.000250 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1270 | Train mAP: 0.2876 | Val Loss: 0.1609 | Val mAP: 0.1749 | LR: 0.000250 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1249 | Train mAP: 0.3633 | Val Loss: 0.1644 | Val mAP: 0.1801 | LR: 0.000250 | Elapsed: 6.4s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1231 | Train mAP: 0.3301 | Val Loss: 0.1608 | Val mAP: 0.1819 | LR: 0.000125 | Elapsed: 6.6s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1255 | Train mAP: 0.3452 | Val Loss: 0.1603 | Val mAP: 0.1866 | LR: 0.000125 | Elapsed: 6.8s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1231 | Train mAP: 0.3519 | Val Loss: 0.1627 | Val mAP: 0.2021 | LR: 0.000125 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1221 | Train mAP: 0.3470 | Val Loss: 0.1622 | Val mAP: 0.2010 | LR: 0.000125 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1176 | Train mAP: 0.3691 | Val Loss: 0.1601 | Val mAP: 0.2024 | LR: 0.000125 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1162 | Train mAP: 0.3960 | Val Loss: 0.1606 | Val mAP: 0.2017 | LR: 0.000125 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1208 | Train mAP: 0.3828 | Val Loss: 0.1590 | Val mAP: 0.2033 | LR: 0.000063 | Elapsed: 7.9s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1286 | Train mAP: 0.3193 | Val Loss: 0.1593 | Val mAP: 0.2039 | LR: 0.000063 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1177 | Train mAP: 0.3615 | Val Loss: 0.1601 | Val mAP: 0.1964 | LR: 0.000063 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1205 | Train mAP: 0.3734 | Val Loss: 0.1626 | Val mAP: 0.1937 | LR: 0.000063 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1201 | Train mAP: 0.3652 | Val Loss: 0.1613 | Val mAP: 0.1978 | LR: 0.000063 | Elapsed: 8.8s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1197 | Train mAP: 0.3759 | Val Loss: 0.1591 | Val mAP: 0.1988 | LR: 0.000063 | Elapsed: 9.0s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1184 | Train mAP: 0.3730 | Val Loss: 0.1598 | Val mAP: 0.2098 | LR: 0.000063 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1210 | Train mAP: 0.3934 | Val Loss: 0.1587 | Val mAP: 0.1904 | LR: 0.000031 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1220 | Train mAP: 0.3741 | Val Loss: 0.1590 | Val mAP: 0.2055 | LR: 0.000031 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1156 | Train mAP: 0.3878 | Val Loss: 0.1607 | Val mAP: 0.1948 | LR: 0.000031 | Elapsed: 9.9s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1080 | Train mAP: 0.4756 | Val Loss: 0.1594 | Val mAP: 0.2055 | LR: 0.000031 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1164 | Train mAP: 0.3781 | Val Loss: 0.1599 | Val mAP: 0.1922 | LR: 0.000031 | Elapsed: 10.4s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1121 | Train mAP: 0.4213 | Val Loss: 0.1600 | Val mAP: 0.1900 | LR: 0.000031 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1183 | Train mAP: 0.3594 | Val Loss: 0.1601 | Val mAP: 0.1897 | LR: 0.000031 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1171 | Train mAP: 0.3938 | Val Loss: 0.1607 | Val mAP: 0.2104 | LR: 0.000016 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1147 | Train mAP: 0.4018 | Val Loss: 0.1590 | Val mAP: 0.1926 | LR: 0.000016 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1125 | Train mAP: 0.4258 | Val Loss: 0.1607 | Val mAP: 0.1958 | LR: 0.000016 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1179 | Train mAP: 0.4083 | Val Loss: 0.1602 | Val mAP: 0.1942 | LR: 0.000016 | Elapsed: 11.7s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1114 | Train mAP: 0.4542 | Val Loss: 0.1602 | Val mAP: 0.1893 | LR: 0.000016 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1172 | Train mAP: 0.4070 | Val Loss: 0.1618 | Val mAP: 0.1897 | LR: 0.000016 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1091 | Train mAP: 0.4544 | Val Loss: 0.1603 | Val mAP: 0.1968 | LR: 0.000008 | Elapsed: 12.4s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1161 | Train mAP: 0.3916 | Val Loss: 0.1591 | Val mAP: 0.1924 | LR: 0.000008 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1174 | Train mAP: 0.3826 | Val Loss: 0.1605 | Val mAP: 0.1951 | LR: 0.000008 | Elapsed: 12.9s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.1201 | Train mAP: 0.3868 | Val Loss: 0.1606 | Val mAP: 0.1921 | LR: 0.000008 | Elapsed: 13.2s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.1144 | Train mAP: 0.4145 | Val Loss: 0.1612 | Val mAP: 0.1930 | LR: 0.000008 | Elapsed: 13.4s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.1168 | Train mAP: 0.4173 | Val Loss: 0.1617 | Val mAP: 0.1937 | LR: 0.000008 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.1187 | Train mAP: 0.3446 | Val Loss: 0.1604 | Val mAP: 0.1911 | LR: 0.000004 | Elapsed: 13.8s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.1123 | Train mAP: 0.4173 | Val Loss: 0.1606 | Val mAP: 0.2136 | LR: 0.000004 | Elapsed: 14.1s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.1128 | Train mAP: 0.4137 | Val Loss: 0.1605 | Val mAP: 0.2141 | LR: 0.000004 | Elapsed: 14.3s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.1117 | Train mAP: 0.4197 | Val Loss: 0.1619 | Val mAP: 0.1932 | LR: 0.000004 | Elapsed: 14.5s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.1126 | Train mAP: 0.4395 | Val Loss: 0.1606 | Val mAP: 0.1950 | LR: 0.000004 | Elapsed: 14.8s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.1144 | Train mAP: 0.4191 | Val Loss: 0.1595 | Val mAP: 0.1939 | LR: 0.000004 | Elapsed: 15.0s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.1100 | Train mAP: 0.4426 | Val Loss: 0.1595 | Val mAP: 0.1969 | LR: 0.000002 | Elapsed: 15.3s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.1134 | Train mAP: 0.4178 | Val Loss: 0.1608 | Val mAP: 0.1948 | LR: 0.000002 | Elapsed: 15.5s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.1190 | Train mAP: 0.3893 | Val Loss: 0.1607 | Val mAP: 0.1918 | LR: 0.000002 | Elapsed: 15.8s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.1135 | Train mAP: 0.4035 | Val Loss: 0.1595 | Val mAP: 0.1968 | LR: 0.000002 | Elapsed: 16.0s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.1160 | Train mAP: 0.4046 | Val Loss: 0.1601 | Val mAP: 0.1935 | LR: 0.000002 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.1174 | Train mAP: 0.4121 | Val Loss: 0.1612 | Val mAP: 0.1933 | LR: 0.000002 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.1185 | Train mAP: 0.4010 | Val Loss: 0.1600 | Val mAP: 0.1959 | LR: 0.000001 | Elapsed: 16.7s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.1152 | Train mAP: 0.3905 | Val Loss: 0.1593 | Val mAP: 0.2074 | LR: 0.000001 | Elapsed: 16.9s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.1156 | Train mAP: 0.4075 | Val Loss: 0.1601 | Val mAP: 0.1930 | LR: 0.000001 | Elapsed: 17.1s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.1157 | Train mAP: 0.4180 | Val Loss: 0.1614 | Val mAP: 0.1985 | LR: 0.000001 | Elapsed: 17.3s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.1186 | Train mAP: 0.3799 | Val Loss: 0.1611 | Val mAP: 0.1950 | LR: 0.000001 | Elapsed: 17.6s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.1155 | Train mAP: 0.4336 | Val Loss: 0.1611 | Val mAP: 0.1953 | LR: 0.000001 | Elapsed: 17.8s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.1124 | Train mAP: 0.4439 | Val Loss: 0.1602 | Val mAP: 0.1944 | LR: 0.000000 | Elapsed: 18.0s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.1158 | Train mAP: 0.3932 | Val Loss: 0.1600 | Val mAP: 0.1976 | LR: 0.000000 | Elapsed: 18.2s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.1178 | Train mAP: 0.3928 | Val Loss: 0.1615 | Val mAP: 0.1948 | LR: 0.000000 | Elapsed: 18.5s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.1192 | Train mAP: 0.3755 | Val Loss: 0.1599 | Val mAP: 0.2049 | LR: 0.000000 | Elapsed: 18.7s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.1155 | Train mAP: 0.4153 | Val Loss: 0.1608 | Val mAP: 0.1925 | LR: 0.000000 | Elapsed: 18.9s\n",
      "\n",
      "🛑 Early stopping at epoch 82. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.2141. Total time: 18.91s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3748 | Train mAP: 0.0663 | Val Loss: 0.2897 | Val mAP: 0.1561 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.2324 | Train mAP: 0.1092 | Val Loss: 0.2312 | Val mAP: 0.2239 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.2301 | Train mAP: 0.1261 | Val Loss: 0.2180 | Val mAP: 0.2004 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.2198 | Train mAP: 0.1356 | Val Loss: 0.2189 | Val mAP: 0.2293 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.2095 | Train mAP: 0.1991 | Val Loss: 0.2076 | Val mAP: 0.2776 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.2042 | Train mAP: 0.2143 | Val Loss: 0.1908 | Val mAP: 0.2598 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1942 | Train mAP: 0.3042 | Val Loss: 0.1867 | Val mAP: 0.3268 | LR: 0.001000 | Elapsed: 1.5s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1927 | Train mAP: 0.2928 | Val Loss: 0.1820 | Val mAP: 0.3227 | LR: 0.001000 | Elapsed: 1.8s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1925 | Train mAP: 0.3055 | Val Loss: 0.1860 | Val mAP: 0.3217 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1874 | Train mAP: 0.3351 | Val Loss: 0.1937 | Val mAP: 0.2886 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1873 | Train mAP: 0.3278 | Val Loss: 0.1859 | Val mAP: 0.3168 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1811 | Train mAP: 0.3673 | Val Loss: 0.1881 | Val mAP: 0.3230 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1800 | Train mAP: 0.3838 | Val Loss: 0.1805 | Val mAP: 0.4069 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1718 | Train mAP: 0.4163 | Val Loss: 0.1822 | Val mAP: 0.3507 | LR: 0.001000 | Elapsed: 3.1s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1773 | Train mAP: 0.3935 | Val Loss: 0.1830 | Val mAP: 0.3492 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1744 | Train mAP: 0.4006 | Val Loss: 0.1854 | Val mAP: 0.3732 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1680 | Train mAP: 0.4418 | Val Loss: 0.1744 | Val mAP: 0.3879 | LR: 0.001000 | Elapsed: 3.7s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1695 | Train mAP: 0.4132 | Val Loss: 0.1774 | Val mAP: 0.3709 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1630 | Train mAP: 0.4536 | Val Loss: 0.1743 | Val mAP: 0.3724 | LR: 0.001000 | Elapsed: 4.2s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1675 | Train mAP: 0.4594 | Val Loss: 0.1725 | Val mAP: 0.3923 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1649 | Train mAP: 0.4695 | Val Loss: 0.1761 | Val mAP: 0.3827 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1551 | Train mAP: 0.5258 | Val Loss: 0.1729 | Val mAP: 0.4216 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1547 | Train mAP: 0.5062 | Val Loss: 0.1743 | Val mAP: 0.3871 | LR: 0.001000 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1639 | Train mAP: 0.4697 | Val Loss: 0.1739 | Val mAP: 0.3991 | LR: 0.001000 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1615 | Train mAP: 0.4787 | Val Loss: 0.1847 | Val mAP: 0.3546 | LR: 0.001000 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1514 | Train mAP: 0.5210 | Val Loss: 0.1818 | Val mAP: 0.3612 | LR: 0.001000 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1538 | Train mAP: 0.5222 | Val Loss: 0.1739 | Val mAP: 0.4008 | LR: 0.000500 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1468 | Train mAP: 0.5466 | Val Loss: 0.1749 | Val mAP: 0.4000 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1437 | Train mAP: 0.5722 | Val Loss: 0.1720 | Val mAP: 0.4196 | LR: 0.000500 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1426 | Train mAP: 0.5640 | Val Loss: 0.1748 | Val mAP: 0.4060 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1362 | Train mAP: 0.5955 | Val Loss: 0.1743 | Val mAP: 0.4091 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1423 | Train mAP: 0.5730 | Val Loss: 0.1737 | Val mAP: 0.4155 | LR: 0.000500 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1382 | Train mAP: 0.5980 | Val Loss: 0.1794 | Val mAP: 0.3950 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1417 | Train mAP: 0.5778 | Val Loss: 0.1719 | Val mAP: 0.4238 | LR: 0.000500 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1426 | Train mAP: 0.5746 | Val Loss: 0.1780 | Val mAP: 0.3947 | LR: 0.000500 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1408 | Train mAP: 0.5792 | Val Loss: 0.1707 | Val mAP: 0.4262 | LR: 0.000500 | Elapsed: 7.9s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1408 | Train mAP: 0.5768 | Val Loss: 0.1767 | Val mAP: 0.4066 | LR: 0.000500 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1373 | Train mAP: 0.6015 | Val Loss: 0.1727 | Val mAP: 0.4187 | LR: 0.000500 | Elapsed: 8.4s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1364 | Train mAP: 0.6006 | Val Loss: 0.1818 | Val mAP: 0.3938 | LR: 0.000500 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1271 | Train mAP: 0.6433 | Val Loss: 0.1846 | Val mAP: 0.3776 | LR: 0.000500 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1341 | Train mAP: 0.6055 | Val Loss: 0.1811 | Val mAP: 0.4188 | LR: 0.000500 | Elapsed: 9.1s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1344 | Train mAP: 0.6029 | Val Loss: 0.1774 | Val mAP: 0.4205 | LR: 0.000500 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1268 | Train mAP: 0.6423 | Val Loss: 0.1768 | Val mAP: 0.4138 | LR: 0.000250 | Elapsed: 9.6s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1253 | Train mAP: 0.6500 | Val Loss: 0.1796 | Val mAP: 0.4020 | LR: 0.000250 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1285 | Train mAP: 0.6335 | Val Loss: 0.1817 | Val mAP: 0.3951 | LR: 0.000250 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1334 | Train mAP: 0.6200 | Val Loss: 0.1835 | Val mAP: 0.4059 | LR: 0.000250 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1289 | Train mAP: 0.6327 | Val Loss: 0.1768 | Val mAP: 0.4074 | LR: 0.000250 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1215 | Train mAP: 0.6645 | Val Loss: 0.1810 | Val mAP: 0.4059 | LR: 0.000250 | Elapsed: 10.7s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1216 | Train mAP: 0.6706 | Val Loss: 0.1812 | Val mAP: 0.4157 | LR: 0.000125 | Elapsed: 10.9s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1181 | Train mAP: 0.6816 | Val Loss: 0.1823 | Val mAP: 0.4148 | LR: 0.000125 | Elapsed: 11.1s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1247 | Train mAP: 0.6512 | Val Loss: 0.1759 | Val mAP: 0.4180 | LR: 0.000125 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1209 | Train mAP: 0.6737 | Val Loss: 0.1770 | Val mAP: 0.4222 | LR: 0.000125 | Elapsed: 11.6s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1247 | Train mAP: 0.6508 | Val Loss: 0.1808 | Val mAP: 0.4140 | LR: 0.000125 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1187 | Train mAP: 0.6740 | Val Loss: 0.1803 | Val mAP: 0.4194 | LR: 0.000125 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1171 | Train mAP: 0.6890 | Val Loss: 0.1797 | Val mAP: 0.4247 | LR: 0.000063 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1158 | Train mAP: 0.6944 | Val Loss: 0.1831 | Val mAP: 0.4117 | LR: 0.000063 | Elapsed: 12.4s\n",
      "\n",
      "🛑 Early stopping at epoch 56. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.4262. Total time: 12.41s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3046 | Train mAP: 0.0551 | Val Loss: 0.2539 | Val mAP: 0.1435 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1896 | Train mAP: 0.0883 | Val Loss: 0.1815 | Val mAP: 0.2246 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1850 | Train mAP: 0.0733 | Val Loss: 0.1883 | Val mAP: 0.2275 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1796 | Train mAP: 0.0939 | Val Loss: 0.1647 | Val mAP: 0.2445 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1707 | Train mAP: 0.1174 | Val Loss: 0.1572 | Val mAP: 0.2671 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1722 | Train mAP: 0.1203 | Val Loss: 0.1627 | Val mAP: 0.2945 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1617 | Train mAP: 0.1537 | Val Loss: 0.1551 | Val mAP: 0.2241 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1514 | Train mAP: 0.2118 | Val Loss: 0.1550 | Val mAP: 0.2105 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1555 | Train mAP: 0.1972 | Val Loss: 0.1583 | Val mAP: 0.2059 | LR: 0.001000 | Elapsed: 2.1s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1566 | Train mAP: 0.1947 | Val Loss: 0.1521 | Val mAP: 0.2393 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1491 | Train mAP: 0.2258 | Val Loss: 0.1550 | Val mAP: 0.2640 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1518 | Train mAP: 0.2316 | Val Loss: 0.1526 | Val mAP: 0.2675 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1503 | Train mAP: 0.2476 | Val Loss: 0.1496 | Val mAP: 0.2716 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1485 | Train mAP: 0.2359 | Val Loss: 0.1537 | Val mAP: 0.2190 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1428 | Train mAP: 0.2695 | Val Loss: 0.1561 | Val mAP: 0.1996 | LR: 0.001000 | Elapsed: 3.8s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1440 | Train mAP: 0.2620 | Val Loss: 0.1627 | Val mAP: 0.1766 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1457 | Train mAP: 0.2805 | Val Loss: 0.1502 | Val mAP: 0.2456 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1455 | Train mAP: 0.2899 | Val Loss: 0.1521 | Val mAP: 0.2366 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1410 | Train mAP: 0.3018 | Val Loss: 0.1584 | Val mAP: 0.2020 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1365 | Train mAP: 0.3443 | Val Loss: 0.1513 | Val mAP: 0.2447 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1384 | Train mAP: 0.3449 | Val Loss: 0.1461 | Val mAP: 0.2539 | LR: 0.000500 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1296 | Train mAP: 0.3667 | Val Loss: 0.1527 | Val mAP: 0.2240 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1306 | Train mAP: 0.3794 | Val Loss: 0.1544 | Val mAP: 0.2290 | LR: 0.000500 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1318 | Train mAP: 0.3926 | Val Loss: 0.1621 | Val mAP: 0.2029 | LR: 0.000500 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1242 | Train mAP: 0.4161 | Val Loss: 0.1600 | Val mAP: 0.2278 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1307 | Train mAP: 0.3876 | Val Loss: 0.1658 | Val mAP: 0.2131 | LR: 0.000500 | Elapsed: 6.4s\n",
      "\n",
      "🛑 Early stopping at epoch 26. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.2945. Total time: 6.38s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.2310 | Train mAP: 0.0133 | Val Loss: 0.1504 | Val mAP: 0.0307 | LR: 0.001000 | Elapsed: 0.3s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.0802 | Train mAP: 0.0119 | Val Loss: 0.0898 | Val mAP: 0.0210 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.0736 | Train mAP: 0.0125 | Val Loss: 0.0811 | Val mAP: 0.1292 | LR: 0.001000 | Elapsed: 0.8s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.0734 | Train mAP: 0.0108 | Val Loss: 0.0722 | Val mAP: 0.0329 | LR: 0.001000 | Elapsed: 1.0s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.0679 | Train mAP: 0.0186 | Val Loss: 0.0682 | Val mAP: 0.0204 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.0638 | Train mAP: 0.0255 | Val Loss: 0.0753 | Val mAP: 0.0185 | LR: 0.001000 | Elapsed: 1.5s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.0635 | Train mAP: 0.0214 | Val Loss: 0.0618 | Val mAP: 0.0244 | LR: 0.001000 | Elapsed: 1.8s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.0645 | Train mAP: 0.0293 | Val Loss: 0.0611 | Val mAP: 0.0236 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.0666 | Train mAP: 0.0236 | Val Loss: 0.0633 | Val mAP: 0.0208 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.0614 | Train mAP: 0.0232 | Val Loss: 0.0652 | Val mAP: 0.0183 | LR: 0.001000 | Elapsed: 2.5s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.0594 | Train mAP: 0.0412 | Val Loss: 0.0593 | Val mAP: 0.0276 | LR: 0.001000 | Elapsed: 2.7s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.0568 | Train mAP: 0.0597 | Val Loss: 0.0702 | Val mAP: 0.0208 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.0569 | Train mAP: 0.0559 | Val Loss: 0.0613 | Val mAP: 0.0323 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.0593 | Train mAP: 0.0570 | Val Loss: 0.0618 | Val mAP: 0.0279 | LR: 0.001000 | Elapsed: 3.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.0518 | Train mAP: 0.0990 | Val Loss: 0.0631 | Val mAP: 0.0316 | LR: 0.001000 | Elapsed: 3.7s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.0578 | Train mAP: 0.0568 | Val Loss: 0.0621 | Val mAP: 0.0382 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.0532 | Train mAP: 0.1247 | Val Loss: 0.0609 | Val mAP: 0.0399 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.0513 | Train mAP: 0.0818 | Val Loss: 0.0646 | Val mAP: 0.0304 | LR: 0.000500 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.0484 | Train mAP: 0.1948 | Val Loss: 0.0656 | Val mAP: 0.0374 | LR: 0.000500 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.0489 | Train mAP: 0.1699 | Val Loss: 0.0652 | Val mAP: 0.0308 | LR: 0.000500 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.0552 | Train mAP: 0.1408 | Val Loss: 0.0656 | Val mAP: 0.0392 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.0517 | Train mAP: 0.1828 | Val Loss: 0.0651 | Val mAP: 0.0366 | LR: 0.000500 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.0479 | Train mAP: 0.2226 | Val Loss: 0.0644 | Val mAP: 0.0402 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "🛑 Early stopping at epoch 23. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.1292. Total time: 5.46s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3146 | Train mAP: 0.7444 | Val Loss: 0.1815 | Val mAP: 0.8617 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1803 | Train mAP: 0.8205 | Val Loss: 0.1531 | Val mAP: 0.8513 | LR: 0.001000 | Elapsed: 0.4s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1590 | Train mAP: 0.8468 | Val Loss: 0.1641 | Val mAP: 0.8324 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1508 | Train mAP: 0.8592 | Val Loss: 0.1671 | Val mAP: 0.8182 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1482 | Train mAP: 0.8630 | Val Loss: 0.1661 | Val mAP: 0.7987 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1413 | Train mAP: 0.8752 | Val Loss: 0.1495 | Val mAP: 0.8452 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1310 | Train mAP: 0.8851 | Val Loss: 0.1422 | Val mAP: 0.8572 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1328 | Train mAP: 0.8846 | Val Loss: 0.1468 | Val mAP: 0.8422 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1362 | Train mAP: 0.8782 | Val Loss: 0.1432 | Val mAP: 0.8605 | LR: 0.001000 | Elapsed: 2.1s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1301 | Train mAP: 0.8895 | Val Loss: 0.1437 | Val mAP: 0.8685 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1212 | Train mAP: 0.8957 | Val Loss: 0.1392 | Val mAP: 0.8634 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1267 | Train mAP: 0.8925 | Val Loss: 0.1430 | Val mAP: 0.8506 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1216 | Train mAP: 0.8970 | Val Loss: 0.1418 | Val mAP: 0.8695 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1229 | Train mAP: 0.8963 | Val Loss: 0.1347 | Val mAP: 0.8756 | LR: 0.001000 | Elapsed: 3.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1181 | Train mAP: 0.9019 | Val Loss: 0.1563 | Val mAP: 0.8188 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1185 | Train mAP: 0.9006 | Val Loss: 0.1372 | Val mAP: 0.8678 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1166 | Train mAP: 0.9039 | Val Loss: 0.1511 | Val mAP: 0.8210 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1216 | Train mAP: 0.9002 | Val Loss: 0.1679 | Val mAP: 0.8355 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1207 | Train mAP: 0.8978 | Val Loss: 0.1636 | Val mAP: 0.8159 | LR: 0.001000 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1176 | Train mAP: 0.9012 | Val Loss: 0.1365 | Val mAP: 0.8745 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1127 | Train mAP: 0.9070 | Val Loss: 0.1293 | Val mAP: 0.8823 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1084 | Train mAP: 0.9095 | Val Loss: 0.1341 | Val mAP: 0.8769 | LR: 0.000500 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.0986 | Train mAP: 0.9226 | Val Loss: 0.1396 | Val mAP: 0.8672 | LR: 0.000500 | Elapsed: 5.4s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1005 | Train mAP: 0.9186 | Val Loss: 0.1440 | Val mAP: 0.8465 | LR: 0.000500 | Elapsed: 5.6s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1000 | Train mAP: 0.9224 | Val Loss: 0.1366 | Val mAP: 0.8731 | LR: 0.000500 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1047 | Train mAP: 0.9152 | Val Loss: 0.1367 | Val mAP: 0.8797 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1042 | Train mAP: 0.9138 | Val Loss: 0.1379 | Val mAP: 0.8780 | LR: 0.000500 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1005 | Train mAP: 0.9207 | Val Loss: 0.1370 | Val mAP: 0.8788 | LR: 0.000250 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.0891 | Train mAP: 0.9311 | Val Loss: 0.1360 | Val mAP: 0.8769 | LR: 0.000250 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1062 | Train mAP: 0.9130 | Val Loss: 0.1368 | Val mAP: 0.8589 | LR: 0.000250 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1031 | Train mAP: 0.9147 | Val Loss: 0.1340 | Val mAP: 0.8811 | LR: 0.000250 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.0876 | Train mAP: 0.9378 | Val Loss: 0.1355 | Val mAP: 0.8815 | LR: 0.000250 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.0997 | Train mAP: 0.9188 | Val Loss: 0.1445 | Val mAP: 0.8671 | LR: 0.000250 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.0991 | Train mAP: 0.9190 | Val Loss: 0.1368 | Val mAP: 0.8730 | LR: 0.000125 | Elapsed: 8.0s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.0949 | Train mAP: 0.9276 | Val Loss: 0.1385 | Val mAP: 0.8643 | LR: 0.000125 | Elapsed: 8.2s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.0908 | Train mAP: 0.9301 | Val Loss: 0.1404 | Val mAP: 0.8609 | LR: 0.000125 | Elapsed: 8.5s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.0965 | Train mAP: 0.9235 | Val Loss: 0.1455 | Val mAP: 0.8437 | LR: 0.000125 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.0894 | Train mAP: 0.9300 | Val Loss: 0.1376 | Val mAP: 0.8566 | LR: 0.000125 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.0912 | Train mAP: 0.9276 | Val Loss: 0.1433 | Val mAP: 0.8584 | LR: 0.000125 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.0938 | Train mAP: 0.9277 | Val Loss: 0.1379 | Val mAP: 0.8673 | LR: 0.000063 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.0917 | Train mAP: 0.9298 | Val Loss: 0.1431 | Val mAP: 0.8627 | LR: 0.000063 | Elapsed: 9.6s\n",
      "\n",
      "🛑 Early stopping at epoch 41. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.8823. Total time: 9.61s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3272 | Train mAP: 0.0874 | Val Loss: 0.2675 | Val mAP: 0.2120 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.2423 | Train mAP: 0.1124 | Val Loss: 0.2251 | Val mAP: 0.2686 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.2297 | Train mAP: 0.1520 | Val Loss: 0.2079 | Val mAP: 0.2547 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.2291 | Train mAP: 0.1674 | Val Loss: 0.2121 | Val mAP: 0.2492 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.2194 | Train mAP: 0.1932 | Val Loss: 0.2070 | Val mAP: 0.2698 | LR: 0.001000 | Elapsed: 1.2s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.2137 | Train mAP: 0.2115 | Val Loss: 0.2001 | Val mAP: 0.2514 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.2120 | Train mAP: 0.2313 | Val Loss: 0.2032 | Val mAP: 0.2995 | LR: 0.001000 | Elapsed: 1.7s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.2036 | Train mAP: 0.2536 | Val Loss: 0.1963 | Val mAP: 0.3114 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.2033 | Train mAP: 0.2749 | Val Loss: 0.1986 | Val mAP: 0.2805 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.2003 | Train mAP: 0.2939 | Val Loss: 0.1986 | Val mAP: 0.2807 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1947 | Train mAP: 0.3224 | Val Loss: 0.2016 | Val mAP: 0.2816 | LR: 0.001000 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1948 | Train mAP: 0.3169 | Val Loss: 0.2021 | Val mAP: 0.2908 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1942 | Train mAP: 0.3275 | Val Loss: 0.1986 | Val mAP: 0.3011 | LR: 0.001000 | Elapsed: 5.1s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1935 | Train mAP: 0.3383 | Val Loss: 0.2032 | Val mAP: 0.2969 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1900 | Train mAP: 0.3618 | Val Loss: 0.2012 | Val mAP: 0.2940 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1890 | Train mAP: 0.3504 | Val Loss: 0.2016 | Val mAP: 0.3280 | LR: 0.000500 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1794 | Train mAP: 0.3948 | Val Loss: 0.2010 | Val mAP: 0.3325 | LR: 0.000500 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1799 | Train mAP: 0.4204 | Val Loss: 0.1999 | Val mAP: 0.3305 | LR: 0.000500 | Elapsed: 6.2s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1765 | Train mAP: 0.4324 | Val Loss: 0.2019 | Val mAP: 0.3198 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1752 | Train mAP: 0.4532 | Val Loss: 0.1971 | Val mAP: 0.3296 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1687 | Train mAP: 0.4811 | Val Loss: 0.1974 | Val mAP: 0.3412 | LR: 0.000250 | Elapsed: 6.9s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1693 | Train mAP: 0.4648 | Val Loss: 0.1978 | Val mAP: 0.3523 | LR: 0.000250 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1678 | Train mAP: 0.4969 | Val Loss: 0.1997 | Val mAP: 0.3265 | LR: 0.000250 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1687 | Train mAP: 0.4893 | Val Loss: 0.1997 | Val mAP: 0.3372 | LR: 0.000250 | Elapsed: 7.6s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1688 | Train mAP: 0.4890 | Val Loss: 0.1982 | Val mAP: 0.3406 | LR: 0.000250 | Elapsed: 7.8s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1731 | Train mAP: 0.4567 | Val Loss: 0.2002 | Val mAP: 0.3277 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1590 | Train mAP: 0.5193 | Val Loss: 0.2013 | Val mAP: 0.3283 | LR: 0.000125 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1589 | Train mAP: 0.5169 | Val Loss: 0.2007 | Val mAP: 0.3369 | LR: 0.000125 | Elapsed: 8.5s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1560 | Train mAP: 0.5291 | Val Loss: 0.2010 | Val mAP: 0.3300 | LR: 0.000125 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1628 | Train mAP: 0.4923 | Val Loss: 0.2049 | Val mAP: 0.3253 | LR: 0.000125 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1593 | Train mAP: 0.5292 | Val Loss: 0.1997 | Val mAP: 0.3229 | LR: 0.000125 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1583 | Train mAP: 0.5295 | Val Loss: 0.2052 | Val mAP: 0.3150 | LR: 0.000125 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1553 | Train mAP: 0.5425 | Val Loss: 0.2078 | Val mAP: 0.3305 | LR: 0.000063 | Elapsed: 9.6s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1563 | Train mAP: 0.5382 | Val Loss: 0.2074 | Val mAP: 0.3159 | LR: 0.000063 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1542 | Train mAP: 0.5521 | Val Loss: 0.2032 | Val mAP: 0.3327 | LR: 0.000063 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1528 | Train mAP: 0.5624 | Val Loss: 0.2005 | Val mAP: 0.3457 | LR: 0.000063 | Elapsed: 10.2s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1533 | Train mAP: 0.5524 | Val Loss: 0.2050 | Val mAP: 0.3296 | LR: 0.000063 | Elapsed: 10.4s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1541 | Train mAP: 0.5605 | Val Loss: 0.2033 | Val mAP: 0.3195 | LR: 0.000063 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1536 | Train mAP: 0.5497 | Val Loss: 0.2053 | Val mAP: 0.3410 | LR: 0.000031 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1458 | Train mAP: 0.5747 | Val Loss: 0.2068 | Val mAP: 0.3235 | LR: 0.000031 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1521 | Train mAP: 0.5511 | Val Loss: 0.2046 | Val mAP: 0.3384 | LR: 0.000031 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1499 | Train mAP: 0.5724 | Val Loss: 0.2037 | Val mAP: 0.3287 | LR: 0.000031 | Elapsed: 11.5s\n",
      "\n",
      "🛑 Early stopping at epoch 42. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.3523. Total time: 11.52s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=SklearnMLPClassifier(epochs=200,\n",
       "                                                     input_dim=2244))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultiOutputClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=SklearnMLPClassifier(epochs=200,\n",
       "                                                     input_dim=2244))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: SklearnMLPClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SklearnMLPClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=SklearnMLPClassifier(epochs=200,\n",
       "                                                     input_dim=2244))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "base_clf = SklearnMLPClassifier(input_dim=feature_size, n_classes=1, epochs=200, verbose=True)\n",
    "multi_clf = MultiOutputClassifier(base_clf)\n",
    "\n",
    "multi_clf.fit(X_data, y_data)\n",
    "# y_pred = multi_clf.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2549d582405c8936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:20:53.348193Z",
     "start_time": "2025-05-20T21:20:53.326818Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def create_test_features(path, n_jobs=4):\n",
    "    # Load test data\n",
    "    with open(path, 'r') as f:\n",
    "        test_json = eval(f.read())  # or use json.load(f) if it's valid JSON\n",
    "\n",
    "    keys = list(test_json)\n",
    "\n",
    "    # Parallel feature extraction\n",
    "    X = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(features)(key) for key in keys\n",
    "    )\n",
    "    \n",
    "    # Convert to tensor format\n",
    "    X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
    "\n",
    "    return keys, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c5ddac9afa11896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:21:04.018375Z",
     "start_time": "2025-05-20T21:20:53.778689Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m keys, X \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_test_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataroot3\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/test.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m multi_clf\u001B[38;5;241m.\u001B[39mpredict(X\u001B[38;5;241m.\u001B[39mnumpy())  \u001B[38;5;66;03m# MultiOutputClassifier expects NumPy\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# add code to print to predictions3.json. format should be key:prediction converted back using an index to tag \u001B[39;00m\n",
      "Cell \u001B[0;32mIn[111], line 13\u001B[0m, in \u001B[0;36mcreate_test_features\u001B[0;34m(path, n_jobs)\u001B[0m\n\u001B[1;32m     10\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(test_json)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Parallel feature extraction\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Convert to tensor format\u001B[39;00m\n\u001B[1;32m     18\u001B[0m X \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack([torch\u001B[38;5;241m.\u001B[39mtensor(x, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m X])\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/joblib/parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/joblib/parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/joblib/parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "keys, X = create_test_features(dataroot3+\"/test.json\", n_jobs=4)\n",
    "y_pred = multi_clf.predict(X.numpy())  # MultiOutputClassifier expects NumPy\n",
    "\n",
    "# add code to print to predictions3.json. format should be key:prediction converted back using an index to tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed5d45b049ecedd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:17:21.894868Z",
     "start_time": "2025-05-20T21:17:21.888075Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6ad8599236a7fadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:17:21.906607Z",
     "start_time": "2025-05-20T21:17:21.891566Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions written to predictions3.json\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-hot predictions to tag lists\n",
    "predictions = {}\n",
    "for key, pred_vector in zip(keys, y_pred):\n",
    "    tag_list = [TAGS[i] for i, val in enumerate(pred_vector) if val == 1]\n",
    "    predictions[key] = tag_list\n",
    "\n",
    "# Save predictions to predictions3.json\n",
    "output_path = \"predictions3.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(f\"✅ Predictions written to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41c60c9cd955f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "LGBM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ce1513a0c788986f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:05.418049Z",
     "start_time": "2025-05-20T21:36:24.369594Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1765, number of negative: 1835\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.490278 -> initscore=-0.038894\n",
      "[LightGBM] [Info] Start training from score -0.038894\n",
      "[LightGBM] [Info] Number of positive: 147, number of negative: 3453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040833 -> initscore=-3.156566\n",
      "[LightGBM] [Info] Start training from score -3.156566\n",
      "[LightGBM] [Info] Number of positive: 351, number of negative: 3249\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097500 -> initscore=-2.225316\n",
      "[LightGBM] [Info] Start training from score -2.225316\n",
      "[LightGBM] [Info] Number of positive: 617, number of negative: 2983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171389 -> initscore=-1.575816\n",
      "[LightGBM] [Info] Start training from score -1.575816\n",
      "[LightGBM] [Info] Number of positive: 152, number of negative: 3448\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042222 -> initscore=-3.121669\n",
      "[LightGBM] [Info] Start training from score -3.121669\n",
      "[LightGBM] [Info] Number of positive: 220, number of negative: 3380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061111 -> initscore=-2.732003\n",
      "[LightGBM] [Info] Start training from score -2.732003\n",
      "[LightGBM] [Info] Number of positive: 162, number of negative: 3438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.045000 -> initscore=-3.055049\n",
      "[LightGBM] [Info] Start training from score -3.055049\n",
      "[LightGBM] [Info] Number of positive: 40, number of negative: 3560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011111 -> initscore=-4.488636\n",
      "[LightGBM] [Info] Start training from score -4.488636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 476, number of negative: 3124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.132222 -> initscore=-1.881452\n",
      "[LightGBM] [Info] Start training from score -1.881452\n",
      "[LightGBM] [Info] Number of positive: 235, number of negative: 3365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.065278 -> initscore=-2.661598\n",
      "[LightGBM] [Info] Start training from score -2.661598\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-11 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-11 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-11 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-11 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-11 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-11 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-11 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-11 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-11 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-11 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=LGBMClassifier(objective=&#x27;binary&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultiOutputClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=LGBMClassifier(objective=&#x27;binary&#x27;))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: LGBMClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=LGBMClassifier(objective='binary'))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Wrap LightGBM in MultiOutputClassifier\n",
    "\n",
    "base_model = LGBMClassifier(\n",
    "    objective='binary',  # each output is binary (0 or 1)\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "model = MultiOutputClassifier(base_model)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "11e71f67d315e29b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:46.567580Z",
     "start_time": "2025-05-20T21:37:30.247708Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/645101335.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "keys, X = create_test_features(dataroot3+\"/test.json\", n_jobs=4)\n",
    "y_pred = model.predict(X.numpy())  # MultiOutputClassifier expects NumPy\n",
    "# add code to print to predictions3.json. format should be key:prediction converted back using an index to tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe8de6ba403c7a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:58.170016Z",
     "start_time": "2025-05-20T21:37:58.158874Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fd7c694c39322bf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:59.903560Z",
     "start_time": "2025-05-20T21:37:59.887794Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions written to predictions3.json\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-hot predictions to tag lists\n",
    "predictions = {}\n",
    "for key, pred_vector in zip(keys, y_pred):\n",
    "    tag_list = [TAGS[i] for i, val in enumerate(pred_vector) if val == 1]\n",
    "    predictions[key] = tag_list\n",
    "\n",
    "# Save predictions to predictions3.json\n",
    "output_path = \"predictions3.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(f\"✅ Predictions written to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966f61b1cbaa4f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Custom MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "23109bf6b3229ce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:32:58.408671Z",
     "start_time": "2025-05-20T21:32:30.088059Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🖥️  Using device: cpu\n",
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.5250 | Train mAP: 0.1530 | Val Loss: 0.4342 | Val mAP: 0.2036 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.4214 | Train mAP: 0.1813 | Val Loss: 0.4036 | Val mAP: 0.2281 | LR: 0.001000 | Elapsed: 1.0s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.4012 | Train mAP: 0.1990 | Val Loss: 0.3959 | Val mAP: 0.2376 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.3936 | Train mAP: 0.2083 | Val Loss: 0.3894 | Val mAP: 0.2218 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.3784 | Train mAP: 0.2172 | Val Loss: 0.3935 | Val mAP: 0.2382 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.3792 | Train mAP: 0.2176 | Val Loss: 0.3840 | Val mAP: 0.2377 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.3736 | Train mAP: 0.2238 | Val Loss: 0.3800 | Val mAP: 0.2400 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.3654 | Train mAP: 0.2324 | Val Loss: 0.3781 | Val mAP: 0.2459 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.3635 | Train mAP: 0.2352 | Val Loss: 0.3798 | Val mAP: 0.2456 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.3645 | Train mAP: 0.2397 | Val Loss: 0.3835 | Val mAP: 0.2478 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.3565 | Train mAP: 0.2392 | Val Loss: 0.3806 | Val mAP: 0.2411 | LR: 0.001000 | Elapsed: 5.1s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.3570 | Train mAP: 0.2429 | Val Loss: 0.3833 | Val mAP: 0.2551 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.3534 | Train mAP: 0.2448 | Val Loss: 0.3847 | Val mAP: 0.2479 | LR: 0.001000 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.3501 | Train mAP: 0.2579 | Val Loss: 0.3820 | Val mAP: 0.2500 | LR: 0.001000 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.3493 | Train mAP: 0.2557 | Val Loss: 0.3822 | Val mAP: 0.2508 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.3426 | Train mAP: 0.2553 | Val Loss: 0.3804 | Val mAP: 0.2486 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.3406 | Train mAP: 0.2596 | Val Loss: 0.3842 | Val mAP: 0.2476 | LR: 0.000500 | Elapsed: 6.8s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.3392 | Train mAP: 0.2641 | Val Loss: 0.3787 | Val mAP: 0.2446 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.3293 | Train mAP: 0.2728 | Val Loss: 0.3798 | Val mAP: 0.2533 | LR: 0.000500 | Elapsed: 7.6s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.3317 | Train mAP: 0.2719 | Val Loss: 0.3806 | Val mAP: 0.2611 | LR: 0.000500 | Elapsed: 7.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.3302 | Train mAP: 0.2783 | Val Loss: 0.3843 | Val mAP: 0.2427 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.3246 | Train mAP: 0.2766 | Val Loss: 0.3779 | Val mAP: 0.2539 | LR: 0.000250 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.3263 | Train mAP: 0.2848 | Val Loss: 0.3774 | Val mAP: 0.2552 | LR: 0.000250 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.3263 | Train mAP: 0.2801 | Val Loss: 0.3819 | Val mAP: 0.2585 | LR: 0.000250 | Elapsed: 8.8s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.3226 | Train mAP: 0.2948 | Val Loss: 0.3787 | Val mAP: 0.2567 | LR: 0.000250 | Elapsed: 9.0s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.3177 | Train mAP: 0.2859 | Val Loss: 0.3812 | Val mAP: 0.2582 | LR: 0.000250 | Elapsed: 9.3s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.3253 | Train mAP: 0.2922 | Val Loss: 0.3812 | Val mAP: 0.2613 | LR: 0.000250 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.3221 | Train mAP: 0.2819 | Val Loss: 0.3802 | Val mAP: 0.2521 | LR: 0.000250 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.3173 | Train mAP: 0.2935 | Val Loss: 0.3789 | Val mAP: 0.2565 | LR: 0.000250 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.3149 | Train mAP: 0.2874 | Val Loss: 0.3789 | Val mAP: 0.2574 | LR: 0.000125 | Elapsed: 10.2s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.3128 | Train mAP: 0.3099 | Val Loss: 0.3800 | Val mAP: 0.2589 | LR: 0.000125 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.3130 | Train mAP: 0.2974 | Val Loss: 0.3813 | Val mAP: 0.2603 | LR: 0.000125 | Elapsed: 10.7s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.3088 | Train mAP: 0.2982 | Val Loss: 0.3780 | Val mAP: 0.2568 | LR: 0.000125 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.3109 | Train mAP: 0.3045 | Val Loss: 0.3798 | Val mAP: 0.2561 | LR: 0.000125 | Elapsed: 11.2s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.3142 | Train mAP: 0.2987 | Val Loss: 0.3841 | Val mAP: 0.2552 | LR: 0.000125 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.3055 | Train mAP: 0.3120 | Val Loss: 0.3809 | Val mAP: 0.2575 | LR: 0.000063 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.3062 | Train mAP: 0.3119 | Val Loss: 0.3812 | Val mAP: 0.2565 | LR: 0.000063 | Elapsed: 12.1s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.3101 | Train mAP: 0.3111 | Val Loss: 0.3827 | Val mAP: 0.2509 | LR: 0.000063 | Elapsed: 12.3s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.3059 | Train mAP: 0.3055 | Val Loss: 0.3792 | Val mAP: 0.2589 | LR: 0.000063 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.3052 | Train mAP: 0.3089 | Val Loss: 0.3811 | Val mAP: 0.2594 | LR: 0.000063 | Elapsed: 13.1s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.3024 | Train mAP: 0.3091 | Val Loss: 0.3802 | Val mAP: 0.2601 | LR: 0.000063 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.3070 | Train mAP: 0.3080 | Val Loss: 0.3797 | Val mAP: 0.2634 | LR: 0.000031 | Elapsed: 14.0s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.3061 | Train mAP: 0.3108 | Val Loss: 0.3812 | Val mAP: 0.2576 | LR: 0.000031 | Elapsed: 14.3s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.3040 | Train mAP: 0.3102 | Val Loss: 0.3790 | Val mAP: 0.2605 | LR: 0.000031 | Elapsed: 14.7s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.3030 | Train mAP: 0.3176 | Val Loss: 0.3813 | Val mAP: 0.2560 | LR: 0.000031 | Elapsed: 15.5s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.3008 | Train mAP: 0.3172 | Val Loss: 0.3818 | Val mAP: 0.2615 | LR: 0.000031 | Elapsed: 15.9s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.3003 | Train mAP: 0.3195 | Val Loss: 0.3822 | Val mAP: 0.2666 | LR: 0.000031 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.3016 | Train mAP: 0.3181 | Val Loss: 0.3822 | Val mAP: 0.2620 | LR: 0.000016 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.3042 | Train mAP: 0.3073 | Val Loss: 0.3805 | Val mAP: 0.2589 | LR: 0.000016 | Elapsed: 16.7s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.3039 | Train mAP: 0.3080 | Val Loss: 0.3822 | Val mAP: 0.2663 | LR: 0.000016 | Elapsed: 17.0s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.3030 | Train mAP: 0.3105 | Val Loss: 0.3826 | Val mAP: 0.2620 | LR: 0.000016 | Elapsed: 17.2s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.3015 | Train mAP: 0.3195 | Val Loss: 0.3786 | Val mAP: 0.2520 | LR: 0.000016 | Elapsed: 17.5s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.3002 | Train mAP: 0.3147 | Val Loss: 0.3831 | Val mAP: 0.2662 | LR: 0.000016 | Elapsed: 17.8s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.3041 | Train mAP: 0.3220 | Val Loss: 0.3804 | Val mAP: 0.2598 | LR: 0.000008 | Elapsed: 18.0s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.3052 | Train mAP: 0.3161 | Val Loss: 0.3797 | Val mAP: 0.2583 | LR: 0.000008 | Elapsed: 18.3s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.3018 | Train mAP: 0.3127 | Val Loss: 0.3821 | Val mAP: 0.2660 | LR: 0.000008 | Elapsed: 18.5s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.2997 | Train mAP: 0.3204 | Val Loss: 0.3803 | Val mAP: 0.2661 | LR: 0.000008 | Elapsed: 18.8s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.3016 | Train mAP: 0.3118 | Val Loss: 0.3836 | Val mAP: 0.2623 | LR: 0.000008 | Elapsed: 19.1s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.2958 | Train mAP: 0.3124 | Val Loss: 0.3807 | Val mAP: 0.2680 | LR: 0.000008 | Elapsed: 19.3s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.2977 | Train mAP: 0.3245 | Val Loss: 0.3794 | Val mAP: 0.2585 | LR: 0.000004 | Elapsed: 19.6s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.3016 | Train mAP: 0.3229 | Val Loss: 0.3844 | Val mAP: 0.2605 | LR: 0.000004 | Elapsed: 19.9s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.2998 | Train mAP: 0.3258 | Val Loss: 0.3818 | Val mAP: 0.2619 | LR: 0.000004 | Elapsed: 20.1s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.3046 | Train mAP: 0.3143 | Val Loss: 0.3836 | Val mAP: 0.2599 | LR: 0.000004 | Elapsed: 20.4s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.3015 | Train mAP: 0.3140 | Val Loss: 0.3825 | Val mAP: 0.2597 | LR: 0.000004 | Elapsed: 20.6s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.3009 | Train mAP: 0.3128 | Val Loss: 0.3791 | Val mAP: 0.2599 | LR: 0.000004 | Elapsed: 20.9s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.2979 | Train mAP: 0.3226 | Val Loss: 0.3803 | Val mAP: 0.2567 | LR: 0.000002 | Elapsed: 21.2s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.2985 | Train mAP: 0.3204 | Val Loss: 0.3802 | Val mAP: 0.2613 | LR: 0.000002 | Elapsed: 21.4s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.3013 | Train mAP: 0.3265 | Val Loss: 0.3809 | Val mAP: 0.2702 | LR: 0.000002 | Elapsed: 21.7s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.3003 | Train mAP: 0.3155 | Val Loss: 0.3813 | Val mAP: 0.2584 | LR: 0.000002 | Elapsed: 22.0s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.3023 | Train mAP: 0.3214 | Val Loss: 0.3821 | Val mAP: 0.2756 | LR: 0.000002 | Elapsed: 22.3s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.3029 | Train mAP: 0.3188 | Val Loss: 0.3822 | Val mAP: 0.2634 | LR: 0.000002 | Elapsed: 22.7s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.3044 | Train mAP: 0.3106 | Val Loss: 0.3852 | Val mAP: 0.2658 | LR: 0.000001 | Elapsed: 23.1s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.3023 | Train mAP: 0.3073 | Val Loss: 0.3782 | Val mAP: 0.2589 | LR: 0.000001 | Elapsed: 23.4s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.3020 | Train mAP: 0.3260 | Val Loss: 0.3829 | Val mAP: 0.2587 | LR: 0.000001 | Elapsed: 23.7s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.2993 | Train mAP: 0.3121 | Val Loss: 0.3807 | Val mAP: 0.2630 | LR: 0.000001 | Elapsed: 24.0s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.2991 | Train mAP: 0.3214 | Val Loss: 0.3829 | Val mAP: 0.2676 | LR: 0.000001 | Elapsed: 24.3s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.2986 | Train mAP: 0.3260 | Val Loss: 0.3825 | Val mAP: 0.2631 | LR: 0.000001 | Elapsed: 24.5s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.3035 | Train mAP: 0.3137 | Val Loss: 0.3792 | Val mAP: 0.2566 | LR: 0.000000 | Elapsed: 24.8s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.3012 | Train mAP: 0.3198 | Val Loss: 0.3812 | Val mAP: 0.2674 | LR: 0.000000 | Elapsed: 25.0s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.3006 | Train mAP: 0.3143 | Val Loss: 0.3804 | Val mAP: 0.2618 | LR: 0.000000 | Elapsed: 25.3s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.3000 | Train mAP: 0.3371 | Val Loss: 0.3806 | Val mAP: 0.2677 | LR: 0.000000 | Elapsed: 25.6s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.3047 | Train mAP: 0.3120 | Val Loss: 0.3794 | Val mAP: 0.2576 | LR: 0.000000 | Elapsed: 25.8s\n",
      "\n",
      "✅ Epoch 83/200 — Train Loss: 0.3033 | Train mAP: 0.3197 | Val Loss: 0.3817 | Val mAP: 0.2612 | LR: 0.000000 | Elapsed: 26.1s\n",
      "\n",
      "✅ Epoch 84/200 — Train Loss: 0.3028 | Train mAP: 0.3143 | Val Loss: 0.3815 | Val mAP: 0.2627 | LR: 0.000000 | Elapsed: 26.4s\n",
      "\n",
      "✅ Epoch 85/200 — Train Loss: 0.3001 | Train mAP: 0.3193 | Val Loss: 0.3820 | Val mAP: 0.2609 | LR: 0.000000 | Elapsed: 26.6s\n",
      "\n",
      "✅ Epoch 86/200 — Train Loss: 0.3013 | Train mAP: 0.3088 | Val Loss: 0.3807 | Val mAP: 0.2640 | LR: 0.000000 | Elapsed: 26.9s\n",
      "\n",
      "✅ Epoch 87/200 — Train Loss: 0.3038 | Train mAP: 0.3209 | Val Loss: 0.3773 | Val mAP: 0.2653 | LR: 0.000000 | Elapsed: 27.2s\n",
      "\n",
      "✅ Epoch 88/200 — Train Loss: 0.3032 | Train mAP: 0.3173 | Val Loss: 0.3812 | Val mAP: 0.2600 | LR: 0.000000 | Elapsed: 27.6s\n",
      "\n",
      "✅ Epoch 89/200 — Train Loss: 0.3010 | Train mAP: 0.3144 | Val Loss: 0.3801 | Val mAP: 0.2644 | LR: 0.000000 | Elapsed: 27.9s\n",
      "\n",
      "✅ Epoch 90/200 — Train Loss: 0.3004 | Train mAP: 0.3294 | Val Loss: 0.3824 | Val mAP: 0.2603 | LR: 0.000000 | Elapsed: 28.3s\n",
      "\n",
      "🛑 Early stopping at epoch 90. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.2756. Total time: 28.25s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244, n_classes=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SklearnMLPClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244, n_classes=10)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SklearnMLPClassifier(epochs=200, input_dim=2244, n_classes=10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SklearnMLPClassifier(input_dim=feature_size, n_classes=10, epochs=200, verbose=True)\n",
    "model.fit(X_train, y_train, X_val_global, y_val_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e5b6d7eef38a80c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:27:14.362957Z",
     "start_time": "2025-05-20T21:26:59.916371Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape: torch.Size([1000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/645101335.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n"
     ]
    }
   ],
   "source": [
    "keys, X = create_test_features(dataroot3+\"/test.json\", n_jobs=4)\n",
    "y_pred = model.predict(X.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "590367e8204d86c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:27:14.371153Z",
     "start_time": "2025-05-20T21:27:14.363255Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions written to predictions3.json\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-hot predictions to tag lists\n",
    "predictions = {}\n",
    "for key, pred_vector in zip(keys, y_pred):\n",
    "    tag_list = [TAGS[i] for i, val in enumerate(pred_vector) if val == 1]\n",
    "    predictions[key] = tag_list\n",
    "\n",
    "# Save predictions to predictions3.json\n",
    "output_path = \"predictions3.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(f\"✅ Predictions written to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c19317b1f67fca89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
