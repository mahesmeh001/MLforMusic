{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:17:51.722121Z",
     "start_time": "2025-05-20T18:17:51.717144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random\n",
    "import pretty_midi\n",
    "import librosa\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance', 'blues', 'punk', 'chill', 'electronic', 'country']\n",
    "tag_to_index = {tag: i for i, tag in enumerate(TAGS)}\n",
    "\n",
    "\n",
    "# do multi-hot encoding\n",
    "\n",
    "def multi_hot_encode(tags):\n",
    "    \"\"\"\n",
    "    Given a list of tag strings, return a multi-hot encoded tensor.\n",
    "    Example input: ['jazz', 'pop']\n",
    "    Output: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
    "    \"\"\"\n",
    "    vec = torch.zeros(len(TAGS), dtype=torch.float32)\n",
    "    for tag in tags:\n",
    "        if tag in tag_to_index:\n",
    "            vec[tag_to_index[tag]] = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tag: {tag}\")\n",
    "    return vec\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:17:52.445079Z",
     "start_time": "2025-05-20T18:17:52.442001Z"
    }
   },
   "id": "c1490336a864f57b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataroot3 = \"data/student_files/task3_audio_classification/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:17:53.550403Z",
     "start_time": "2025-05-20T18:17:53.541185Z"
    }
   },
   "id": "c5b2133e12eb2d52",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_model(model, filepath='sol_3.pt'):\n",
    "    \"\"\"Save a PyTorch model to a file\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model_class, filepath='sol_3.pt', *args, **kwargs):\n",
    "    \"\"\"Load a PyTorch model from a file\"\"\"\n",
    "    model = model_class(*args, **kwargs)  # instantiate the model\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # optional: sets dropout/batchnorm to eval mode\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:17:54.026564Z",
     "start_time": "2025-05-20T18:17:54.022086Z"
    }
   },
   "id": "44f78696ef1e9aa7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from mido import MidiFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import islice\n",
    "import fluidsynth\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# create train loader \n",
    "\n",
    "def extract_waveform(path):\n",
    "    # Your code here\n",
    "    wave, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    return wave \n",
    "\n",
    "def extract_q(w):\n",
    "    # Calculate frequency range that works with your sample rate\n",
    "    fmin = 32.70  # C1 note in Hz\n",
    "    fmax = 7000.0  # Just under Nyquist frequency\n",
    "    \n",
    "    # Use bins_per_octave to control the distribution\n",
    "    bins_per_octave = int(128 / np.log2(fmax/fmin))\n",
    "    \n",
    "    result = librosa.cqt(\n",
    "        y=w, \n",
    "        sr=SAMPLE_RATE, \n",
    "        n_bins=64,\n",
    "        bins_per_octave=bins_per_octave,\n",
    "        fmin=fmin,\n",
    "        hop_length=313\n",
    "    )\n",
    "    \n",
    "    result = librosa.amplitude_to_db(np.abs(result))\n",
    "    return torch.FloatTensor(result)\n",
    "\n",
    "\n",
    "def extract_mfcc(w):\n",
    "    # Your code here:\n",
    "    # load using librosa.feature.mfcc\n",
    "    mfcc = librosa.feature.mfcc(y=w, sr=SAMPLE_RATE, n_mfcc = 128, hop_length=313)\n",
    "    \n",
    "    return torch.FloatTensor(mfcc)\n",
    "\n",
    "def extract_spec(w, n_mels=128, max_time=512):\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=w, \n",
    "        sr=SAMPLE_RATE,\n",
    "        hop_length=313,\n",
    "        n_mels=n_mels,\n",
    "        fmin=0,\n",
    "        fmax=SAMPLE_RATE / 2\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Pad or crop time dimension\n",
    "    T = mel_spec_db.shape[1]\n",
    "    if T < max_time:\n",
    "        pad_width = max_time - T\n",
    "        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)))\n",
    "    else:\n",
    "        mel_spec_db = mel_spec_db[:, :max_time]\n",
    "    \n",
    "    return torch.FloatTensor(mel_spec_db)  # (n_mels, max_time)\n",
    "\n",
    "\n",
    "\n",
    "def pad_or_truncate(spec, max_time=500):\n",
    "\n",
    "    freq_bins, time_bins = spec.shape\n",
    "    if time_bins > max_time:\n",
    "        return spec[:, :max_time]\n",
    "    elif time_bins < max_time:\n",
    "        pad_width = max_time - time_bins\n",
    "        return F.pad(spec, (0, pad_width), mode='constant', value=0)\n",
    "    return spec\n",
    "\n",
    "import torch\n",
    "import pretty_midi\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def features(path):\n",
    "    # get mfcc, spec, q transform for a file\n",
    "    \n",
    "    full_path = dataroot3 + '/' + path\n",
    "    \n",
    "    w = extract_waveform(full_path)\n",
    "    \n",
    "    # pad or truncate here \n",
    "    mfcc = pad_or_truncate(extract_mfcc(w), max_time=512)\n",
    "    spec = pad_or_truncate(extract_spec(w), max_time=512)\n",
    "    q = pad_or_truncate(extract_q(w), max_time=512)\n",
    "    \n",
    "    # q is at 64x512; and i need it to be at 128x512\n",
    "    # if q.shape[0] < 128:\n",
    "    q = q.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 84, 512)\n",
    "    q = F.interpolate(q, size=(128, 512), mode='bilinear', align_corners=False)\n",
    "    q = q.squeeze(0).squeeze(0)  # Back to shape: (128, 512)\n",
    "    \n",
    "    \n",
    "    # mfcc = mfcc.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 84, 512)\n",
    "    # mfcc = F.interpolate(mfcc, size=(128, 512), mode='bilinear', align_corners=False)\n",
    "    # mfcc = mfcc.squeeze(0).squeeze(0)  # Back to shape: (128, 512)\n",
    "        \n",
    "    # print(f\"Q shape: {q.shape}\")  # <== DEBUG line\n",
    "    # print(f\"MFCC: {mfcc.shape}\")  # <== DEBUG line\n",
    "    # print(f\"Spec shape: {spec.shape}\")  # <== DEBUG line\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return mfcc, spec, q"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:17:55.117446Z",
     "start_time": "2025-05-20T18:17:55.115289Z"
    }
   },
   "id": "fa1eaee2942bb5bc",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(6860) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6861) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[-3.3780e+02, -3.1909e+02, -3.2192e+02,  ..., -4.4202e+01,\n          -4.9421e+01, -6.5830e+01],\n         [ 8.3267e+01,  8.6681e+01,  8.5426e+01,  ...,  1.7716e+01,\n           2.4289e+01,  3.4539e+01],\n         [-2.6709e+01, -3.3490e+01, -3.4710e+01,  ...,  3.0015e+01,\n           3.6966e+01,  4.1554e+01],\n         ...,\n         [ 3.1587e-02, -8.6079e-01, -1.4015e+00,  ..., -2.5887e-02,\n          -3.2444e-01, -7.9310e-01],\n         [-1.2375e+00, -2.1217e+00, -1.5122e+00,  ...,  3.5742e-01,\n           2.0656e-01,  6.0570e-02],\n         [-2.8523e-01, -1.0276e+00, -1.7786e+00,  ..., -2.9548e-02,\n           5.4415e-01,  1.1081e+00]]),\n tensor([[-64.4517, -65.3846, -69.1503,  ..., -25.6241, -25.5115, -26.3162],\n         [-55.3488, -54.6433, -53.8988,  ..., -13.5494, -15.2853, -18.6953],\n         [-46.8266, -45.1533, -46.9443,  ...,  -9.4312, -11.2589, -12.0087],\n         ...,\n         [-80.0000, -80.0000, -80.0000,  ..., -42.1651, -41.5706, -43.8325],\n         [-80.0000, -80.0000, -80.0000,  ..., -47.5185, -47.8790, -48.7839],\n         [-80.0000, -80.0000, -80.0000,  ..., -58.6585, -56.4115, -56.2556]]),\n tensor([[-52.3531, -52.2012, -52.1520,  ...,  -5.4342,  -6.4188,  -7.7390],\n         [-53.3368, -53.3687, -53.5235,  ...,  -5.5141,  -6.8567,  -8.5727],\n         [-55.3043, -55.7038, -56.2666,  ...,  -5.6740,  -7.7325, -10.2400],\n         ...,\n         [-40.2897, -39.0457, -41.7178,  ..., -33.8061, -30.9288, -21.0244],\n         [-38.6015, -40.4294, -42.4048,  ..., -35.6885, -32.7290, -20.1304],\n         [-37.7574, -41.1212, -42.7483,  ..., -36.6296, -33.6291, -19.6833]]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features('train/0.wav')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:17:58.305845Z",
     "start_time": "2025-05-20T18:17:56.741610Z"
    }
   },
   "id": "ab2103a5bf8aabb8",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def create_datasets(size=None, val_split=0.2, n_jobs=4):\n",
    "    # Load data\n",
    "    with open(dataroot3 + \"/train.json\", 'r') as f:\n",
    "        train_json = eval(f.read())\n",
    "    \n",
    "    # Limit size if specified\n",
    "    if size is not None:\n",
    "        train_json = dict(list(train_json.items())[:size])\n",
    "    \n",
    "    # Parallel feature extraction\n",
    "    keys = list(train_json.keys())\n",
    "    values = list(train_json.values())\n",
    "\n",
    "    # Parallel feature extraction (mfcc, spec, q → stacked into shape [3, 128, 512])\n",
    "    def process(key):\n",
    "        mfcc, spec, q = features(key)\n",
    "        x = torch.stack([mfcc, spec, q], dim=0)  # shape: (3, 128, 512)\n",
    "        return x\n",
    "\n",
    "    X = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process)(key) for key in keys\n",
    "    )\n",
    "    y = torch.stack([multi_hot_encode(tags) for tags in values])\n",
    "    \n",
    "    \n",
    "    # Convert to tensors\n",
    "    X = torch.stack(X)  # shape: (N, 3, 128, 512)\n",
    "    y = torch.tensor(y, dtype=torch.float32)  # shape: (N, num_classes) for multilabel\n",
    "\n",
    "    \n",
    " \n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:18:03.974685Z",
     "start_time": "2025-05-20T18:18:03.971240Z"
    }
   },
   "id": "26e0237cb2c4a18d",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(6864) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6865) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6866) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6867) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6868) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6869) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_6059/1743451323.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)  # shape: (N, num_classes) for multilabel\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = create_datasets()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:20:17.003124Z",
     "start_time": "2025-05-20T18:18:04.919352Z"
    }
   },
   "id": "d63a85e2ca8cca4f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save data\n",
    "import pickle\n",
    "\n",
    "train_data_dict = {'x': X_data, 'y': y_data}\n",
    "\n",
    "with open(\"task3_train_data.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_data_dict, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:20:37.354372Z",
     "start_time": "2025-05-20T18:20:16.875939Z"
    }
   },
   "id": "1ac5f83d74095a7e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"task3_train_data.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_data = data['x']\n",
    "y_data = data['y']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T18:50:05.261409Z",
     "start_time": "2025-05-19T18:50:02.423692Z"
    }
   },
   "id": "908e1baf2ab65dc3",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, use_modalities=(\"mfcc\", \"spec\")):\n",
    "        \"\"\"\n",
    "        use_modalities: tuple of 2 modalities to use out of (\"mfcc\", \"spec\", \"q\")\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.use_modalities = use_modalities\n",
    "        self.modality_to_idx = {\"mfcc\": 0, \"spec\": 1, \"q\": 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # shape: (3, 128, 512)\n",
    "        label = self.y[idx]\n",
    "\n",
    "        # Select only the two specified modalities\n",
    "        selected = [x[self.modality_to_idx[modality]] for modality in self.use_modalities]\n",
    "        \n",
    "        selected[0] = selected[0].unsqueeze(0)\n",
    "        selected[1] = selected[1].unsqueeze(0)\n",
    "\n",
    "\n",
    "        return selected[0], selected[1], label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T18:50:05.268863Z",
     "start_time": "2025-05-19T18:50:05.261994Z"
    }
   },
   "id": "a026ffb5b3e01c4c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "# X_data: shape (n_samples, channels, height, width)\n",
    "# Y_data: shape (n_samples, n_labels)\n",
    "\n",
    "# Flatten X for splitting\n",
    "X_flat = X_data.reshape((X_data.shape[0], -1))\n",
    "\n",
    "# Split preserving multilabel distribution\n",
    "X_train_flat, y_train, X_val_flat, y_val = iterative_train_test_split(\n",
    "    X_flat.numpy(), y_data.numpy(), test_size=0.1\n",
    ")\n",
    "\n",
    "# Reshape X back to original shape\n",
    "X_train = torch.tensor(X_train_flat).reshape(-1, *X_data.shape[1:])\n",
    "X_val = torch.tensor(X_val_flat).reshape(-1, *X_data.shape[1:])\n",
    "y_train = torch.tensor(y_train)\n",
    "y_val = torch.tensor(y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T18:50:13.208059Z",
     "start_time": "2025-05-19T18:50:05.264488Z"
    }
   },
   "id": "cd06222a5f2865ea",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_78449/1736831912.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_78449/1736831912.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_78449/1736831912.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_78449/1736831912.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)  \n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T18:50:20.302509Z",
     "start_time": "2025-05-19T18:50:13.224701Z"
    }
   },
   "id": "615bfa916f126eee",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomAudioDataset(X_train, y_train, use_modalities=(\"spec\", \"mfcc\"))\n",
    "val_dataset = CustomAudioDataset(X_val, y_val, use_modalities=(\"spec\", \"mfcc\"))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T18:50:20.330277Z",
     "start_time": "2025-05-19T18:50:20.315814Z"
    }
   },
   "id": "ece775f763d43677",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def verify_data(train_loader, num_classes=10):\n",
    "    \"\"\"Check data for potential issues and class frequency distribution for multi-label data.\n",
    "       Also returns pos_weight tensor for BCEWithLogitsLoss to handle class imbalance.\n",
    "    \"\"\"\n",
    "    batch_count = 0\n",
    "    label_counter = Counter()\n",
    "    total_assignments = 0  # Total number of 1s in all label vectors\n",
    "    sample_count = 0\n",
    "    \n",
    "    for spec, mfcc, labels in train_loader:\n",
    "        batch_count += 1\n",
    "        label_indices = labels.int()\n",
    "        \n",
    "        for label_vector in label_indices:\n",
    "            total_assignments += label_vector.sum().item()\n",
    "        \n",
    "        sample_count += labels.size(0)\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            label_counter[i] += (label_indices[:, i] == 1).sum().item()\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(spec).any() or torch.isinf(spec).any():\n",
    "            print(\"WARNING: NaN or Inf values found in spectrogram data!\")\n",
    "        \n",
    "        if torch.isnan(mfcc).any() or torch.isinf(mfcc).any():\n",
    "            print(\"WARNING: NaN or Inf values found in MFCC data!\")\n",
    "        \n",
    "        print(f\"Batch {batch_count}:\")\n",
    "        print(f\"  Spec range: [{spec.min().item():.4f}, {spec.max().item():.4f}]\")\n",
    "        print(f\"  MFCC range: [{mfcc.min().item():.4f}, {mfcc.max().item():.4f}]\")\n",
    "    \n",
    "    print(f\"\\nTotal samples processed: {sample_count}\")\n",
    "    print(f\"Total class assignments (1s): {total_assignments}\")\n",
    "    \n",
    "    print(\"\\nClass frequency distribution:\")\n",
    "    counts = []\n",
    "    for i in range(num_classes):\n",
    "        count = label_counter[i]\n",
    "        counts.append(count)\n",
    "        print(f\"  Class {i}: {count} assignments ({count / total_assignments:.2%})\")\n",
    "    \n",
    "    # Compute pos_weight = (N - count) / count for each class\n",
    "    label_counts_tensor = torch.tensor(counts, dtype=torch.float)\n",
    "    pos_weight = (sample_count - label_counts_tensor) / label_counts_tensor\n",
    "    \n",
    "    \n",
    "    print(\"\\nComputed pos_weight (for BCEWithLogitsLoss):\")\n",
    "    for i, w in enumerate(pos_weight):\n",
    "        print(f\"  Class {i}: {w.item():.4f}\")\n",
    "\n",
    "    return pos_weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T07:05:45.514194Z",
     "start_time": "2025-05-20T07:05:44.637022Z"
    }
   },
   "id": "9766809b793ad064",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "  Spec range: [-100.0000, 38.4801]\n",
      "  MFCC range: [-720.3176, 279.7045]\n",
      "Batch 2:\n",
      "  Spec range: [-100.0000, 41.0473]\n",
      "  MFCC range: [-761.3607, 259.0165]\n",
      "Batch 3:\n",
      "  Spec range: [-100.0000, 40.9389]\n",
      "  MFCC range: [-750.5443, 291.9468]\n",
      "Batch 4:\n",
      "  Spec range: [-100.0000, 40.8011]\n",
      "  MFCC range: [-761.3607, 275.6665]\n",
      "Batch 5:\n",
      "  Spec range: [-100.0000, 39.8696]\n",
      "  MFCC range: [-749.2927, 279.1788]\n",
      "Batch 6:\n",
      "  Spec range: [-100.0000, 40.2640]\n",
      "  MFCC range: [-813.8968, 279.7045]\n",
      "Batch 7:\n",
      "  Spec range: [-100.0000, 38.7742]\n",
      "  MFCC range: [-778.2305, 264.6042]\n",
      "Batch 8:\n",
      "  Spec range: [-100.0000, 39.3283]\n",
      "  MFCC range: [-751.4969, 251.0771]\n",
      "Batch 9:\n",
      "  Spec range: [-100.0000, 39.4752]\n",
      "  MFCC range: [-756.3655, 267.7703]\n",
      "Batch 10:\n",
      "  Spec range: [-100.0000, 38.9064]\n",
      "  MFCC range: [-753.0217, 260.6252]\n",
      "Batch 11:\n",
      "  Spec range: [-100.0000, 41.2199]\n",
      "  MFCC range: [-727.1412, 255.5780]\n",
      "Batch 12:\n",
      "  Spec range: [-100.0000, 39.5128]\n",
      "  MFCC range: [-752.8512, 279.5922]\n",
      "Batch 13:\n",
      "  Spec range: [-92.4183, 39.8696]\n",
      "  MFCC range: [-727.3490, 274.3304]\n",
      "Batch 14:\n",
      "  Spec range: [-99.1316, 38.9779]\n",
      "  MFCC range: [-738.6826, 255.5780]\n",
      "Batch 15:\n",
      "  Spec range: [-91.8865, 39.5558]\n",
      "  MFCC range: [-742.3831, 277.0133]\n",
      "Batch 16:\n",
      "  Spec range: [-79.4795, 40.7090]\n",
      "  MFCC range: [-737.7642, 291.9468]\n",
      "Batch 17:\n",
      "  Spec range: [-100.0000, 39.7985]\n",
      "  MFCC range: [-800.0450, 274.1124]\n",
      "Batch 18:\n",
      "  Spec range: [-84.8278, 38.4682]\n",
      "  MFCC range: [-718.0333, 254.7110]\n",
      "Batch 19:\n",
      "  Spec range: [-100.0000, 38.7203]\n",
      "  MFCC range: [-758.7413, 264.3524]\n",
      "Batch 20:\n",
      "  Spec range: [-100.0000, 41.2199]\n",
      "  MFCC range: [-778.4006, 257.2973]\n",
      "Batch 21:\n",
      "  Spec range: [-100.0000, 38.9434]\n",
      "  MFCC range: [-765.3419, 275.6665]\n",
      "Batch 22:\n",
      "  Spec range: [-100.0000, 39.7230]\n",
      "  MFCC range: [-797.6075, 258.2863]\n",
      "Batch 23:\n",
      "  Spec range: [-100.0000, 39.8188]\n",
      "  MFCC range: [-785.1575, 277.8046]\n",
      "Batch 24:\n",
      "  Spec range: [-85.9619, 38.9620]\n",
      "  MFCC range: [-746.0712, 279.1788]\n",
      "Batch 25:\n",
      "  Spec range: [-100.0000, 39.4267]\n",
      "  MFCC range: [-746.3941, 258.6417]\n",
      "Batch 26:\n",
      "  Spec range: [-100.0000, 39.0052]\n",
      "  MFCC range: [-807.7725, 272.5011]\n",
      "Batch 27:\n",
      "  Spec range: [-100.0000, 38.5923]\n",
      "  MFCC range: [-736.5687, 252.5385]\n",
      "Batch 28:\n",
      "  Spec range: [-100.0000, 38.9838]\n",
      "  MFCC range: [-741.7057, 260.4214]\n",
      "Batch 29:\n",
      "  Spec range: [-75.2205, 35.0313]\n",
      "  MFCC range: [-736.3790, 238.8964]\n",
      "\n",
      "Total samples processed: 3601\n",
      "Total class assignments (1s): 4166\n",
      "\n",
      "Class frequency distribution:\n",
      "  Class 0: 1765 assignments (42.37%)\n",
      "  Class 1: 147 assignments (3.53%)\n",
      "  Class 2: 351 assignments (8.43%)\n",
      "  Class 3: 617 assignments (14.81%)\n",
      "  Class 4: 152 assignments (3.65%)\n",
      "  Class 5: 220 assignments (5.28%)\n",
      "  Class 6: 162 assignments (3.89%)\n",
      "  Class 7: 41 assignments (0.98%)\n",
      "  Class 8: 476 assignments (11.43%)\n",
      "  Class 9: 235 assignments (5.64%)\n",
      "\n",
      "Computed pos_weight (for BCEWithLogitsLoss):\n",
      "  Class 0: 1.0402\n",
      "  Class 1: 23.4966\n",
      "  Class 2: 9.2593\n",
      "  Class 3: 4.8363\n",
      "  Class 4: 22.6908\n",
      "  Class 5: 15.3682\n",
      "  Class 6: 21.2284\n",
      "  Class 7: 86.8293\n",
      "  Class 8: 6.5651\n",
      "  Class 9: 14.3234\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.7131, 3.1985, 2.3282, 1.7641, 3.1651, 2.7953, 3.1014, 4.4754, 2.0235,\n        2.7294])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = verify_data(train_loader)\n",
    "pos_weight = torch.log1p(pos_weight) \n",
    "pos_weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T19:34:24.313614Z",
     "start_time": "2025-05-19T19:34:18.736790Z"
    }
   },
   "id": "3470acd1f5cce583",
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad816ecf21d72d5e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import ast\n",
    "\n",
    "class ThresholdLayer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ThresholdLayer, self).__init__()\n",
    "        self.thresholds = nn.Parameter(torch.zeros(num_classes))  # Trainable thresholds\n",
    "\n",
    "    def forward(self, logits):\n",
    "        # Apply the threshold per class\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return (probs > self.thresholds).float()  # Binarize based on thresholds\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=4, ff_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "    \n",
    "class DualResNetWithTransformer(nn.Module):\n",
    "    def __init__(self, num_labels, transformer_dim=512, seq_len=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone ResNet-18 - Use weights=None instead of pretrained=True\n",
    "        def resnet_branch():\n",
    "            model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            \n",
    "            # Replace the first layer to handle 1-channel input\n",
    "            model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            \n",
    "            # Reset the weights of the first conv layer since we modified it\n",
    "            nn.init.kaiming_normal_(model.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "            \n",
    "            return nn.Sequential(*list(model.children())[:-2])  # remove avgpool and fc\n",
    "\n",
    "        self.spec_branch = resnet_branch()\n",
    "        self.mfcc_branch = resnet_branch()\n",
    "        \n",
    "        # Disable gradients for pretrained layers to make training faster initially\n",
    "        for param in list(self.spec_branch.parameters())[:-9]:  # Keep last few layers trainable\n",
    "            param.requires_grad = False\n",
    "        for param in list(self.mfcc_branch.parameters())[:-9]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Instead of pooling completely:\n",
    "        self.partial_pool = nn.AdaptiveAvgPool2d((seq_len, 1))  # Output: (B, 512, seq_len, 1)\n",
    "        \n",
    "        # Multiple transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(dim=transformer_dim) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        self.attn_fusion = AttentionFusion(embed_dim=512)\n",
    "\n",
    "        # Modified classifier without sigmoid (will be handled by loss function)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Increased dropout for better regularization\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "        \n",
    "        # # Threshold Layer for per-class thresholding\n",
    "        # self.threshold_layer = ThresholdLayer(num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, spec, mfcc):\n",
    "        # ResNet feature maps (B, 512, H, W)\n",
    "        x_spec = self.spec_branch(spec)  # (B, 512, H, W)\n",
    "        x_mfcc = self.mfcc_branch(mfcc)  # (B, 512, H, W)\n",
    "        \n",
    "        # Partial pooling to create sequence\n",
    "        x_spec = self.partial_pool(x_spec).squeeze(-1).permute(0, 2, 1)  # (B, seq_len, 512)\n",
    "        x_mfcc = self.partial_pool(x_mfcc).squeeze(-1).permute(0, 2, 1)  # (B, seq_len, 512)\n",
    "        \n",
    "        # Fuse corresponding sequence elements\n",
    "        x_fused = torch.zeros_like(x_spec)\n",
    "        for i in range(x_spec.size(1)):\n",
    "            x_fused[:, i] = self.attn_fusion(x_spec[:, i], x_mfcc[:, i])\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        x = x_fused\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # Global pooling across sequence dimension\n",
    "        x_out = torch.mean(x, dim=1)  # (B, 512)\n",
    "        \n",
    "        # Return logits (without sigmoid) - BCEWithLogitsLoss will handle it\n",
    "        logits = self.classifier(x_out)\n",
    "        \n",
    "        # Apply the learned per-class thresholds to the logits\n",
    "        # thresholded_preds = self.threshold_layer(logits)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Enhanced AttentionFusion with layer normalization\n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, x2], dim=1)  # (B, 2*D)\n",
    "        fused = self.fusion(x)          # (B, D)\n",
    "        alpha = self.attn(fused)        # (B, 1)\n",
    "        return alpha * x1 + (1 - alpha) * x2  # Weighted fusion\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T18:50:23.280477Z",
     "start_time": "2025-05-19T18:50:21.612772Z"
    }
   },
   "id": "c7e33ea9a18fb9d2",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, num_labels):\n",
    "        self.model = DualResNetWithTransformer(num_labels=num_labels)\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_map=[]\n",
    "        self.train_losses=[]\n",
    "        self.val_losses=[]\n",
    "        self.val_map=[]\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=100, save_path=\"sol3_dualResNetTransformer.pt\"):\n",
    "        torch.mps.empty_cache()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = self.model.to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "        \n",
    "        # Create separate param groups\n",
    "        def get_trainable_params():\n",
    "            return [\n",
    "                {'params': [p for p in model.spec_branch.parameters() if p.requires_grad], 'lr': 0.0001},\n",
    "                {'params': [p for p in model.mfcc_branch.parameters() if p.requires_grad], 'lr': 0.0001},\n",
    "                {'params': model.attn_fusion.parameters(), 'lr': 0.001},\n",
    "                {'params': [p for block in model.transformer_blocks for p in block.parameters()], 'lr': 0.0005},\n",
    "                {'params': model.classifier.parameters(), 'lr': 0.001},\n",
    "            ]\n",
    "    \n",
    "        optimizer = optim.Adam(get_trainable_params())\n",
    "        \n",
    "        best_val_map_score = float('-inf')  # Start with the lowest possible value for mAP\n",
    "        patience_counter = 0\n",
    "        patience = 10\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            from tqdm import tqdm\n",
    "            loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    \n",
    "            all_train_probs = []\n",
    "            all_train_labels = []\n",
    "    \n",
    "            for spec, mfcc, labels in loader_iter:\n",
    "                spec, mfcc, labels = spec.to(device), mfcc.to(device), labels.to(device).float()\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass without autocast\n",
    "                logits = model(spec, mfcc)  # Now model returns both logits and thresholds\n",
    "                loss = criterion(logits, labels)\n",
    "    \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                probs = torch.sigmoid(logits).detach().cpu()\n",
    "                preds = (probs > 0.5).float()\n",
    "\n",
    "                labels_cpu = labels.detach().cpu()\n",
    "                \n",
    "                correct_train += (preds == labels_cpu).sum().item()\n",
    "                total_train += labels_cpu.numel()\n",
    "                \n",
    "                # Store for mAP calculation\n",
    "                all_train_probs.append(preds)\n",
    "                all_train_labels.append(labels_cpu)\n",
    "\n",
    "            train_probs = torch.cat(all_train_probs)\n",
    "            train_labels = torch.cat(all_train_labels)\n",
    "            \n",
    "            train_map_score = average_precision_score(train_labels.numpy(), train_probs.numpy(), average=\"macro\")\n",
    "            self.train_map.append(train_map_score)\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "            self.train_acc.append(train_accuracy)\n",
    "    \n",
    "            # === Validation ===\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "    \n",
    "            all_val_probs = []\n",
    "            all_val_labels = []\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for spec, mfcc, labels in val_loader:\n",
    "                    spec, mfcc, labels = spec.to(device), mfcc.to(device), labels.to(device).float()\n",
    "                    logits = model(spec, mfcc)\n",
    "                    \n",
    "                    \n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    probs = torch.sigmoid(logits).detach().cpu()\n",
    "                    preds = (probs > 0.5).float()\n",
    "                    labels_cpu = labels.detach().cpu()\n",
    "                    \n",
    "                    correct_val += (preds == labels_cpu).sum().item()\n",
    "                    total_val += labels_cpu.numel()\n",
    "                    \n",
    "                    # Store for mAP\n",
    "                    all_val_probs.append(preds)\n",
    "                    all_val_labels.append(labels_cpu)\n",
    "\n",
    "            val_probs = torch.cat(all_val_probs)\n",
    "            val_labels = torch.cat(all_val_labels)\n",
    "            \n",
    "            val_map_score = average_precision_score(val_labels.numpy(), val_probs.numpy(), average=\"macro\")\n",
    "            self.val_map.append(val_map_score)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = correct_val / total_val\n",
    "            self.val_losses.append(avg_val_loss)\n",
    "            self.val_acc.append(val_accuracy)\n",
    "    \n",
    "            print(f\"Epoch {epoch+1}/{epochs} — Train Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}, mAP: {train_map_score:.4f} | Val Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.4f}, mAP: {val_map_score:.4f}\")\n",
    "\n",
    "    \n",
    "            # Save best model based on val mAP\n",
    "            if val_map_score > best_val_map_score:\n",
    "                best_val_map_score = val_map_score\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Saving model (improved val mAP: {best_val_map_score:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "    \n",
    "            # === Unfreeze partial backbone after epoch 10 ===\n",
    "            if epoch == 10:\n",
    "                print(\"Unfreezing the last residual blocks of ResNet branches...\")\n",
    "                def unfreeze_last_block(branch):\n",
    "                    for name, module in branch.named_children():\n",
    "                        if 'layer4' in name:\n",
    "                            for param in module.parameters():\n",
    "                                param.requires_grad = True\n",
    "    \n",
    "                unfreeze_last_block(model.spec_branch)\n",
    "                unfreeze_last_block(model.mfcc_branch)\n",
    "    \n",
    "                optimizer = optim.Adam(get_trainable_params())  # Update optimizer with new params\n",
    "    \n",
    "        self.model.load_state_dict(torch.load(save_path))\n",
    "        self.plot_training_metrics()\n",
    "\n",
    "    def plot_training_metrics(self):\n",
    "        \"\"\"Plot training and validation metrics\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.title('Loss vs. Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot accuracies\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(self.train_acc, label='Training Accuracy')\n",
    "        plt.plot(self.val_acc, label='Validation Accuracy')\n",
    "        plt.title('Accuracy vs. Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        \n",
    "        # Plot mAP\n",
    "        plt.subplot(3,1,3)\n",
    "        plt.plot(self.train_map, label='Training mAP')\n",
    "        plt.plot(self.val_map, label='Validation mAP')\n",
    "        plt.title('mAP vs. Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Mean Average Precision')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        print(\"Training metrics plot saved as 'training_metrics.png'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T21:30:05.883146Z",
     "start_time": "2025-05-19T21:30:05.862823Z"
    }
   },
   "id": "c411b7821963b6b3",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 29/29 [01:53<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — Train Loss: 0.4577, Acc: 0.8784, mAP: 0.1535 | Val Loss: 0.4196, Acc: 0.9140, mAP: 0.2141\n",
      "Saving model (improved val mAP: 0.2141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 29/29 [01:31<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 — Train Loss: 0.3623, Acc: 0.9054, mAP: 0.2346 | Val Loss: 0.3885, Acc: 0.9233, mAP: 0.2720\n",
      "Saving model (improved val mAP: 0.2720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 29/29 [01:28<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 — Train Loss: 0.2921, Acc: 0.9209, mAP: 0.3416 | Val Loss: 0.3796, Acc: 0.8960, mAP: 0.3014\n",
      "Saving model (improved val mAP: 0.3014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 29/29 [01:27<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 — Train Loss: 0.2172, Acc: 0.9387, mAP: 0.4836 | Val Loss: 0.4386, Acc: 0.9115, mAP: 0.2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 29/29 [01:40<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 — Train Loss: 0.1505, Acc: 0.9588, mAP: 0.6176 | Val Loss: 0.4941, Acc: 0.9150, mAP: 0.3661\n",
      "Saving model (improved val mAP: 0.3661)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 29/29 [01:46<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 — Train Loss: 0.1047, Acc: 0.9718, mAP: 0.7472 | Val Loss: 0.5895, Acc: 0.9296, mAP: 0.3831\n",
      "Saving model (improved val mAP: 0.3831)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 29/29 [01:32<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 — Train Loss: 0.0655, Acc: 0.9828, mAP: 0.8621 | Val Loss: 0.6740, Acc: 0.9123, mAP: 0.3277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 29/29 [01:53<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 — Train Loss: 0.0568, Acc: 0.9851, mAP: 0.8838 | Val Loss: 0.6365, Acc: 0.9221, mAP: 0.3974\n",
      "Saving model (improved val mAP: 0.3974)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 29/29 [00:46<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 — Train Loss: 0.0466, Acc: 0.9871, mAP: 0.8784 | Val Loss: 0.7009, Acc: 0.9273, mAP: 0.3847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 29/29 [00:35<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 — Train Loss: 0.0488, Acc: 0.9873, mAP: 0.8796 | Val Loss: 0.8215, Acc: 0.9321, mAP: 0.3986\n",
      "Saving model (improved val mAP: 0.3986)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 29/29 [04:15<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 — Train Loss: 0.0412, Acc: 0.9893, mAP: 0.9176 | Val Loss: 0.7012, Acc: 0.9263, mAP: 0.3945\n",
      "Unfreezing the last residual blocks of ResNet branches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 29/29 [04:13<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 — Train Loss: 0.0415, Acc: 0.9890, mAP: 0.9067 | Val Loss: 0.8877, Acc: 0.9213, mAP: 0.3686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 29/29 [01:49<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 — Train Loss: 0.0421, Acc: 0.9888, mAP: 0.8929 | Val Loss: 0.8557, Acc: 0.9363, mAP: 0.4046\n",
      "Saving model (improved val mAP: 0.4046)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 29/29 [01:48<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 — Train Loss: 0.0338, Acc: 0.9911, mAP: 0.9108 | Val Loss: 0.9399, Acc: 0.9296, mAP: 0.3692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 29/29 [01:58<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 — Train Loss: 0.0308, Acc: 0.9920, mAP: 0.9155 | Val Loss: 0.9688, Acc: 0.9206, mAP: 0.3516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 29/29 [01:15<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 — Train Loss: 0.0311, Acc: 0.9920, mAP: 0.9231 | Val Loss: 0.8583, Acc: 0.9155, mAP: 0.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 29/29 [01:25<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 — Train Loss: 0.0361, Acc: 0.9918, mAP: 0.9226 | Val Loss: 0.9473, Acc: 0.9273, mAP: 0.3847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████| 29/29 [10:47<00:00, 22.32s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 — Train Loss: 0.0245, Acc: 0.9934, mAP: 0.9374 | Val Loss: 1.0164, Acc: 0.9273, mAP: 0.3482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "model = Model(10)\n",
    "model.train(train_loader, val_loader, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-19T21:30:06.279106Z"
    }
   },
   "id": "bd38d5b62e8e3200",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "def find_best_thresholds(logits, labels, metric='f1'):\n",
    "    \"\"\"\n",
    "    Finds the best threshold for each class to maximize the selected metric (F1 or AP).\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): shape (num_samples, num_classes)\n",
    "        labels (torch.Tensor): shape (num_samples, num_classes)\n",
    "        metric (str): either 'f1' or 'ap'\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: optimal threshold for each class, shape (num_classes,)\n",
    "    \"\"\"\n",
    "    num_classes = logits.shape[1]\n",
    "    thresholds = torch.zeros(num_classes)\n",
    "\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        best_score = -1\n",
    "        best_thresh = 0.5\n",
    "        for thresh in np.linspace(0.0, 1.0, 101):\n",
    "            preds = (probs[:, c] > thresh).astype(int)\n",
    "\n",
    "            if metric == 'f1':\n",
    "                score = f1_score(labels[:, c], preds, zero_division=0)\n",
    "            elif metric == 'ap':\n",
    "                from sklearn.metrics import average_precision_score\n",
    "                # Not binarized, so we must compute AP with probs\n",
    "                score = average_precision_score(labels[:, c], probs[:, c])\n",
    "                break  # mAP doesn't use thresholds, use probs directly\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported metric\")\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_thresh = thresh\n",
    "\n",
    "        thresholds[c] = best_thresh\n",
    "\n",
    "    return thresholds\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T21:12:15.580189Z",
     "start_time": "2025-05-19T21:12:15.500430Z"
    }
   },
   "id": "1f356c6266a2a175",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 128, 512])\n",
      "torch.Size([64, 1, 128, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mismatched Tensor types in NNPack convolutionOutput",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[76], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(spec\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(mfcc\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m----> 9\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmfcc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m all_logits\u001B[38;5;241m.\u001B[39mappend(logits)\n\u001B[1;32m     11\u001B[0m all_labels\u001B[38;5;241m.\u001B[39mappend(labels)\n",
      "Cell \u001B[0;32mIn[18], line 86\u001B[0m, in \u001B[0;36mDualResNetWithTransformer.forward\u001B[0;34m(self, spec, mfcc)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, spec, mfcc):\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;66;03m# ResNet feature maps (B, 512, H, W)\u001B[39;00m\n\u001B[0;32m---> 86\u001B[0m     x_spec \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspec_branch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (B, 512, H, W)\u001B[39;00m\n\u001B[1;32m     87\u001B[0m     x_mfcc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmfcc_branch(mfcc)  \u001B[38;5;66;03m# (B, 512, H, W)\u001B[39;00m\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;66;03m# Partial pooling to create sequence\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:240\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 240\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[1;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[1;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[1;32m    548\u001B[0m     )\n\u001B[0;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Mismatched Tensor types in NNPack convolutionOutput"
     ]
    }
   ],
   "source": [
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for spec,mfcc,labels in val_loader:\n",
    "        print(spec.shape)\n",
    "        print(mfcc.shape)\n",
    "        logits = model.model(spec,mfcc)\n",
    "        all_logits.append(logits)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_logits = torch.cat(all_logits)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "# Find best thresholds using F1\n",
    "best_thresholds = find_best_thresholds(all_logits, all_labels, metric='ap')\n",
    "\n",
    "# Set the thresholds in your ThresholdLayer\n",
    "model.model.threshold_layer.thresholds.data = best_thresholds\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T21:27:29.695126Z",
     "start_time": "2025-05-19T21:27:29.515648Z"
    }
   },
   "id": "87fae601ba809912",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def debug_single_batch(model, train_loader, device):\n",
    "    \"\"\"Test model on a single batch repeatedly to verify overfitting capability\"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Get a single batch\n",
    "    for spec, mfcc, labels in train_loader:\n",
    "        break\n",
    "    \n",
    "    spec = spec.to(device)\n",
    "    mfcc = mfcc.to(device)\n",
    "    labels = labels.to(device).float()\n",
    "    \n",
    "    print(\"Starting single batch overfitting test...\")\n",
    "    \n",
    "    # Train on this single batch for many iterations\n",
    "    for i in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(spec, mfcc)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        with torch.no_grad():\n",
    "            outputs = torch.sigmoid(logits)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            accuracy = (preds == labels).float().mean().item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Single batch test complete.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-19T19:10:30.451922Z"
    }
   },
   "id": "ee11500f22e92dee",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# debug_single_batch(model, train_loader, torch.device(\"mps\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T19:10:30.525522Z",
     "start_time": "2025-05-19T19:10:30.462444Z"
    }
   },
   "id": "654a1b3acc28b488",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "# \n",
    "# # After training:\n",
    "# all_probs = torch.cat(all_val_probs).numpy()\n",
    "# all_labels = torch.cat(all_val_labels).numpy()\n",
    "# \n",
    "# best_thresholds = []\n",
    "# for i in range(num_labels):\n",
    "#     precision, recall, thresholds = precision_recall_curve(all_labels[:, i], all_probs[:, i])\n",
    "#     f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "#     best_idx = f1.argmax()\n",
    "#     best_thresholds.append(thresholds[best_idx])\n",
    "# \n",
    "# # Save thresholds for inference\n",
    "# model.threshold_layer.thresholds.data = torch.tensor(best_thresholds).to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-19T19:10:30.462527Z"
    }
   },
   "id": "e51b86a314f6a721"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict_instance(model, file1):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    spec, mfcc, _ = features(file1)\n",
    "    spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    mfcc = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(spec, mfcc)  # logits: [1, num_labels], thresholds: [num_labels]\n",
    "        print(\"logits:\",logits)\n",
    "        probs = torch.sigmoid(logits).squeeze(0)  # [num_labels]\n",
    "        print(\"probs:\",probs)\n",
    "        preds = (probs > 0.5).int().cpu().tolist()\n",
    "\n",
    "    return preds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ee46233ab36a71",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predict_instance(model.model, \"test/197.wav\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d97ed37faa94f635",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]python(81040) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(81041) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(81042) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(81043) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 125.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "print(\"Loading and processing test data...\")\n",
    "with open(dataroot3+\"/test.json\"+'', 'r') as f:\n",
    "    data = ast.literal_eval(f.read())\n",
    "\n",
    "process_func = partial(features)\n",
    "processed_test_data = Parallel(n_jobs=4)(\n",
    "    delayed(process_func)(d) for d in tqdm.tqdm(data)\n",
    ")\n",
    "processed_test_data = [r for r in processed_test_data if r is not None]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T19:54:16.695330Z",
     "start_time": "2025-05-19T19:54:06.844462Z"
    }
   },
   "id": "e5b558883e6c8ff7",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "\n",
    "def predict(model, path, outpath=None, n_jobs=4, processed_data=None):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and process test data\n",
    "    if not processed_data:\n",
    "        print(\"Loading and processing test data...\")\n",
    "        with open(path, 'r') as f:\n",
    "            data = ast.literal_eval(f.read())\n",
    "\n",
    "        process_func = partial(features)\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_func)(d) for d in tqdm.tqdm(data)\n",
    "        )\n",
    "        results = [r for r in results if r is not None]\n",
    "    else:\n",
    "        results = processed_data\n",
    "        with open(path, 'r') as f:\n",
    "            data = ast.literal_eval(f.read())\n",
    "\n",
    "    specs = torch.stack([r[0] for r in results]).unsqueeze(1).to(device)\n",
    "    mfccs = torch.stack([r[1] for r in results]).unsqueeze(1).to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(specs, mfccs)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).int().cpu().tolist()\n",
    "\n",
    "    predicted = [[TAGS[i] for i, val in enumerate(pred_vec) if val == 1] for pred_vec in preds]\n",
    "\n",
    "    if len(predicted) == 1 and not isinstance(predicted[0], list):\n",
    "        predicted = [predicted]\n",
    "\n",
    "    predictions = dict(zip(data, predicted))\n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, \"w\") as z:\n",
    "            z.write(str(predictions) + '\\n')\n",
    "        print(f\"Predictions written to {outpath}\")\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef14f09363933b25",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predict(model.model, dataroot3+\"/test.json\",\"predictions3.json\", processed_data=processed_test_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70fafa46c3f6d634",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_cooccurrence_matrix(train_loader, num_classes=10):\n",
    "    co_matrix = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    for _, _, labels in train_loader:\n",
    "        labels = labels.int()\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            active = labels[i].nonzero().squeeze().tolist()\n",
    "            if isinstance(active, int):\n",
    "                active = [active]\n",
    "            for i in active:\n",
    "                for j in active:\n",
    "                    co_matrix[i][j] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(co_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Label Co-occurrence Matrix\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Class\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T19:29:40.811032Z",
     "start_time": "2025-05-19T19:29:34.444764Z"
    }
   },
   "id": "d1361036171553ab",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'd' for object of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mplot_cooccurrence_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[41], line 20\u001B[0m, in \u001B[0;36mplot_cooccurrence_matrix\u001B[0;34m(train_loader, num_classes)\u001B[0m\n\u001B[1;32m     17\u001B[0m                 co_matrix[i][j] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     19\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m8\u001B[39m))\n\u001B[0;32m---> 20\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheatmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mco_matrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mannot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfmt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43md\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcmap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mBlues\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLabel Co-occurrence Matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     22\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClass\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/seaborn/matrix.py:459\u001B[0m, in \u001B[0;36mheatmap\u001B[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m square:\n\u001B[1;32m    458\u001B[0m     ax\u001B[38;5;241m.\u001B[39mset_aspect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mequal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 459\u001B[0m \u001B[43mplotter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcbar_ax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ax\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/seaborn/matrix.py:352\u001B[0m, in \u001B[0;36m_HeatMapper.plot\u001B[0;34m(self, ax, cax, kws)\u001B[0m\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# Annotate the cells with the formatted values\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mannot:\n\u001B[0;32m--> 352\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_annotate_heatmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmesh\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/seaborn/matrix.py:260\u001B[0m, in \u001B[0;36m_HeatMapper._annotate_heatmap\u001B[0;34m(self, ax, mesh)\u001B[0m\n\u001B[1;32m    258\u001B[0m lum \u001B[38;5;241m=\u001B[39m relative_luminance(color)\n\u001B[1;32m    259\u001B[0m text_color \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.15\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m lum \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m.408\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 260\u001B[0m annotation \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m{\u001B[39;49m\u001B[38;5;124;43m:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfmt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m}\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    261\u001B[0m text_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(color\u001B[38;5;241m=\u001B[39mtext_color, ha\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcenter\u001B[39m\u001B[38;5;124m\"\u001B[39m, va\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcenter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    262\u001B[0m text_kwargs\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mannot_kws)\n",
      "\u001B[0;31mValueError\u001B[0m: Unknown format code 'd' for object of type 'float'"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x800 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cooccurrence_matrix(train_loader,10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T19:29:42.279200Z",
     "start_time": "2025-05-19T19:29:40.812247Z"
    }
   },
   "id": "de688ba02d55d76a",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "721baf81efe794c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
