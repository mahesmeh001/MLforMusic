{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:49:53.467436Z",
     "start_time": "2025-05-20T19:49:53.338131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance', 'blues', 'punk', 'chill', 'electronic', 'country']\n",
    "tag_to_index = {tag: i for i, tag in enumerate(TAGS)}\n",
    "\n",
    "\n",
    "# do multi-hot encoding\n",
    "\n",
    "def multi_hot_encode(tags):\n",
    "    \"\"\"\n",
    "    Given a list of tag strings, return a multi-hot encoded tensor.\n",
    "    Example input: ['jazz', 'pop']\n",
    "    Output: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
    "    \"\"\"\n",
    "    vec = torch.zeros(len(TAGS), dtype=torch.float32)\n",
    "    for tag in tags:\n",
    "        if tag in tag_to_index:\n",
    "            vec[tag_to_index[tag]] = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tag: {tag}\")\n",
    "    return vec\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:35.957180Z",
     "start_time": "2025-05-20T19:48:35.955467Z"
    }
   },
   "id": "fac9eb7fcc41fe4b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataroot3 = \"data/student_files/task3_audio_classification/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:36.204961Z",
     "start_time": "2025-05-20T19:48:36.201754Z"
    }
   },
   "id": "7b7e805028a7e525",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_model(model, filepath='sol_1.pt'):\n",
    "    \"\"\"Save a PyTorch model to a file\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model_class, filepath='sol_1.pt', *args, **kwargs):\n",
    "    \"\"\"Load a PyTorch model from a file\"\"\"\n",
    "    model = model_class(*args, **kwargs)  # instantiate the model\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # optional: sets dropout/batchnorm to eval mode\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:36.612791Z",
     "start_time": "2025-05-20T19:48:36.610734Z"
    }
   },
   "id": "785e4b6ec3ed4ae8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from mido import MidiFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import islice\n",
    "import librosa\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# create train loader \n",
    "\n",
    "\n",
    "def get_lowest_pitch(file_path):\n",
    "    # Initialize lowest_note to a high value (since MIDI notes are from 0 to 127)\n",
    "    lowest_note = 128  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note < lowest_note:\n",
    "                    lowest_note = msg.note\n",
    "    \n",
    "    # Return None if no note is found\n",
    "    return lowest_note if lowest_note != 128 else None\n",
    "\n",
    "def get_highest_pitch(file_path):\n",
    "    # Initialize highest_note to a low value (since MIDI notes are from 0 to 127)\n",
    "    highest_note = -1  \n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                if msg.note > highest_note:\n",
    "                    highest_note = msg.note\n",
    "                    \n",
    "    # Return None if no note is found\n",
    "    return highest_note if highest_note != -1 else None\n",
    "\n",
    "def get_unique_pitch_num(file_path):\n",
    "    mid = MidiFile(file_path)\n",
    "    notes = set()\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.add(msg.note)\n",
    "    \n",
    "    return len(notes)\n",
    "\n",
    "def get_average_pitch_value(file_path):\n",
    "    #Q8: Your code goes here\n",
    "    mid = MidiFile(file_path)\n",
    "    \n",
    "    notes = []\n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                notes.append(msg.note)\n",
    "    \n",
    "    if notes:\n",
    "        return sum(notes) / len(notes)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_waveform(path):\n",
    "    waveform, sr = librosa.load(path, sr=SAMPLE_RATE)  # waveform: 1D NumPy array\n",
    "    waveform = torch.FloatTensor(waveform)              # Convert to 1D torch tensor\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "        waveform = resample(waveform.unsqueeze(0)).squeeze(0)  # (1, N) → (N,)\n",
    "\n",
    "    # Pad or trim to target length (10 seconds)\n",
    "    target_len = SAMPLE_RATE * 10\n",
    "    if waveform.shape[0] < target_len:\n",
    "        pad_len = target_len - waveform.shape[0]\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    else:\n",
    "        waveform = waveform[:target_len]\n",
    "\n",
    "    return waveform.numpy()  # shape: (160000,)\n",
    "\n",
    "\n",
    "def extract_q(w):\n",
    "    # Your code here\n",
    "    result = librosa.cqt(y=w, sr=SAMPLE_RATE)\n",
    "    result = librosa.amplitude_to_db(np.abs(result))\n",
    "    q =torch.FloatTensor(result)\n",
    "    \n",
    "    mean = q.mean(dim=1)  # shape: (84,)\n",
    "    std = q.std(dim=1)    # shape: (84,)\n",
    "    return torch.cat([mean, std], dim=0)  # shape: (168,)\n",
    "\n",
    "def extract_mfcc(w):\n",
    "    # Your code here:\n",
    "    # load using librosa.feature.mfcc\n",
    "    mfcc = librosa.feature.mfcc(y=w, sr=SAMPLE_RATE, n_mfcc = 13)\n",
    "    # extract mean and \n",
    "    means = np.mean(mfcc, axis=1)\n",
    "    \n",
    "    stds = np.std(mfcc, axis=1)\n",
    "    # concatenate\n",
    "    features = np.concatenate([means, stds])\n",
    "    \n",
    "    return torch.FloatTensor(features)\n",
    "\n",
    "def extract_spec(w):\n",
    "    # Your code here\n",
    "    # load\n",
    "    stft = librosa.stft(y=w)\n",
    "    # take squared absolute values\n",
    "    spec = np.abs(stft) ** 2\n",
    "    \n",
    "    spec = torch.FloatTensor(spec)\n",
    "    \n",
    "    mean = spec.mean(dim=1)  # shape (128,)\n",
    "    std = spec.std(dim=1)    # shape (128,)\n",
    "    return torch.cat([mean, std], dim=0)  # shape (256,)\n",
    "\n",
    "\n",
    "import torch\n",
    "import pretty_midi\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def features(path):\n",
    "    full_path = dataroot3 + '/' + path\n",
    "    try:\n",
    "        w = extract_waveform(full_path)\n",
    "        # midi_obj = pretty_midi.PrettyMIDI(full_path)\n",
    "        # w = midi_obj.fluidsynth(fs=SAMPLE_RATE)\n",
    "\n",
    "        if w is None or len(w) < SAMPLE_RATE // 10:  # e.g. less than 0.1s\n",
    "            raise ValueError(\"Waveform too short or empty\")\n",
    "\n",
    "        mfcc = extract_mfcc(w)\n",
    "        spec = extract_spec(w)\n",
    "        q = extract_q(w)\n",
    "        \n",
    "        features = torch.cat(\n",
    "            [\n",
    "                mfcc, \n",
    "                spec, \n",
    "                q\n",
    "            ]) \n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {full_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def create_train_features(size=None, val_split=0.2, n_jobs=4):\n",
    "    # Load data\n",
    "    with open(dataroot3 + \"/train.json\", 'r') as f:\n",
    "        train_json = eval(f.read())\n",
    "    \n",
    "    # Limit size if specified\n",
    "    if size is not None:\n",
    "        train_json = dict(list(train_json.items())[:size])\n",
    "    \n",
    "    # Parallel feature extraction\n",
    "    keys = list(train_json.keys())\n",
    "    values = list(train_json.values())\n",
    "\n",
    "    X = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(features)(key) for key in keys\n",
    "    )\n",
    "    y = torch.stack([multi_hot_encode(tags) for tags in values])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
    "    Y = torch.tensor(y, dtype=torch.int64)\n",
    "    \n",
    "    # Return all data if no validation split needed\n",
    "    if val_split <= 0:\n",
    "        return X, Y\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    # X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    #     X, Y, test_size=val_split, random_state=42, shuffle=True\n",
    "    # )\n",
    "    \n",
    "    return X, Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:00:52.185430Z",
     "start_time": "2025-05-20T20:00:52.178736Z"
    }
   },
   "id": "1f64eea8c4902a29",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.00018311,  0.00024414,  0.00018311, ..., -0.06781006,\n       -0.01745605,  0.02740479], dtype=float32)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(extract_waveform(\"data/student_files/task3_audio_classification//train/3590.wav\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:00:53.245964Z",
     "start_time": "2025-05-20T20:00:53.239705Z"
    }
   },
   "id": "9393c96f1f25ecfc",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/937749269.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/937749269.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(y, dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([4000, 2244]), torch.Size([4000, 10]))"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data, y_data  = create_train_features()\n",
    "X_data.shape, y_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:01:42.531346Z",
     "start_time": "2025-05-20T20:00:54.297201Z"
    }
   },
   "id": "e90d615610af0da0",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save data\n",
    "import pickle\n",
    "\n",
    "train_data_dict = {'x': X_data, 'y': y_data}\n",
    "\n",
    "with open(\"task3_train_data_3.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_data_dict, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:02:18.973398Z",
     "start_time": "2025-05-20T20:02:18.960348Z"
    }
   },
   "id": "c4093452b33f2bc5",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"task3_train_data_3.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_data = data['x']\n",
    "y_data = data['y']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce4e52e6f2b58d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 2244) (3600, 10)\n",
      "(400, 2244) (400, 10)\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Convert tensors to numpy\n",
    "X_np = X_data.numpy()\n",
    "y_np = y_data.numpy()\n",
    "\n",
    "# Perform stratified split (e.g., 80% train, 20% val)\n",
    "X_train, y_train, X_val_global, y_val_global = iterative_train_test_split(X_np, y_np, test_size=0.1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val_global.shape, y_val_global.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:50:33.244919Z",
     "start_time": "2025-05-20T20:50:33.184200Z"
    }
   },
   "id": "ebccc7b058be32ba",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244\n"
     ]
    }
   ],
   "source": [
    "print(len(X_data[0]))\n",
    "feature_size = (len(X_data[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:02:24.416757Z",
     "start_time": "2025-05-20T20:02:24.414726Z"
    }
   },
   "id": "11fe1e561fe8ad82",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def verify_data(y_train, mel=None, mfcc=None, num_classes=10):\n",
    "    \"\"\"Check label distribution and optional input stats. \n",
    "       Returns pos_weight tensor for BCEWithLogitsLoss to handle class imbalance.\n",
    "\n",
    "       Parameters:\n",
    "       - y_train (Tensor or ndarray): shape (N, num_classes), binary multi-label.\n",
    "       - mel (Tensor or ndarray): optional, for range checking.\n",
    "       - mfcc (Tensor or ndarray): optional, for range checking.\n",
    "    \"\"\"\n",
    "    if isinstance(y_train, np.ndarray):\n",
    "        y_train = torch.tensor(y_train)\n",
    "    \n",
    "    label_counter = Counter()\n",
    "    total_assignments = 0\n",
    "    sample_count = y_train.size(0)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        class_count = (y_train[:, i] == 1).sum().item()\n",
    "        label_counter[i] += class_count\n",
    "        total_assignments += class_count\n",
    "\n",
    "    print(f\"Total samples: {sample_count}\")\n",
    "    print(f\"Total class assignments (1s): {total_assignments}\\n\")\n",
    "\n",
    "    print(\"Class frequency distribution:\")\n",
    "    counts = []\n",
    "    for i in range(num_classes):\n",
    "        count = label_counter[i]\n",
    "        counts.append(count)\n",
    "        print(f\"  Class {i}: {count} assignments ({count / total_assignments:.2%})\")\n",
    "\n",
    "    # Compute pos_weight = (N - count) / count\n",
    "    label_counts_tensor = torch.tensor(counts, dtype=torch.float)\n",
    "    pos_weight = (sample_count - label_counts_tensor) / label_counts_tensor\n",
    "\n",
    "    print(\"\\nComputed pos_weight (for BCEWithLogitsLoss):\")\n",
    "    for i, w in enumerate(pos_weight):\n",
    "        print(f\"  Class {i}: {w.item():.4f}\")\n",
    "\n",
    "    # Optional: check mel / mfcc value ranges\n",
    "    if mel is not None and isinstance(mel, np.ndarray):\n",
    "        mel = torch.tensor(mel)\n",
    "    if mfcc is not None and isinstance(mfcc, np.ndarray):\n",
    "        mfcc = torch.tensor(mfcc)\n",
    "\n",
    "    if mel is not None:\n",
    "        if torch.isnan(mel).any() or torch.isinf(mel).any():\n",
    "            print(\"WARNING: NaN or Inf values found in mel data!\")\n",
    "        else:\n",
    "            print(f\"mel range: [{mel.min().item():.4f}, {mel.max().item():.4f}]\")\n",
    "\n",
    "    if mfcc is not None:\n",
    "        if torch.isnan(mfcc).any() or torch.isinf(mfcc).any():\n",
    "            print(\"WARNING: NaN or Inf values found in mfcc data!\")\n",
    "        else:\n",
    "            print(f\"mfcc range: [{mfcc.min().item():.4f}, {mfcc.max().item():.4f}]\")\n",
    "\n",
    "    return pos_weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:59:26.632125Z",
     "start_time": "2025-05-20T20:59:26.623953Z"
    }
   },
   "id": "a090bef86908022b",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3600\n",
      "Total class assignments (1s): 4165\n",
      "\n",
      "Class frequency distribution:\n",
      "  Class 0: 1765 assignments (42.38%)\n",
      "  Class 1: 147 assignments (3.53%)\n",
      "  Class 2: 351 assignments (8.43%)\n",
      "  Class 3: 617 assignments (14.81%)\n",
      "  Class 4: 152 assignments (3.65%)\n",
      "  Class 5: 220 assignments (5.28%)\n",
      "  Class 6: 162 assignments (3.89%)\n",
      "  Class 7: 40 assignments (0.96%)\n",
      "  Class 8: 476 assignments (11.43%)\n",
      "  Class 9: 235 assignments (5.64%)\n",
      "\n",
      "Computed pos_weight (for BCEWithLogitsLoss):\n",
      "  Class 0: 1.0397\n",
      "  Class 1: 23.4898\n",
      "  Class 2: 9.2564\n",
      "  Class 3: 4.8347\n",
      "  Class 4: 22.6842\n",
      "  Class 5: 15.3636\n",
      "  Class 6: 21.2222\n",
      "  Class 7: 89.0000\n",
      "  Class 8: 6.5630\n",
      "  Class 9: 14.3191\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.7128, 3.1983, 2.3279, 1.7638, 3.1648, 2.7951, 3.1011, 4.4998, 2.0233,\n        2.7291])"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = verify_data(y_train)\n",
    "pos_weight = torch.log1p(pos_weight) \n",
    "pos_weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T20:59:27.693773Z",
     "start_time": "2025-05-20T20:59:27.683848Z"
    }
   },
   "id": "a1b58a6c94720ea1",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=1):  # num_classes = 1 for binary output\n",
    "        super().__init__()\n",
    "        self.__init_args__ = (input_dim,)\n",
    "        self.__init_kwargs__ = {'num_classes': num_classes}\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # More expressive feature extractor\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        # Attention mechanism to emphasize important features\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),  # output matches feature dim\n",
    "            nn.Sigmoid()  # attention weights between 0 and 1\n",
    "        )\n",
    "\n",
    "        # Final classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)  # Output logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_net(x)\n",
    "        attention_weights = torch.sigmoid(self.attention(features))\n",
    "        weighted_features = features * attention_weights\n",
    "        logits = self.classifier(weighted_features)\n",
    "        return logits  # raw logit\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Intermediate representation before attention and classification\"\"\"\n",
    "        return self.feature_net(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:26:25.614514Z",
     "start_time": "2025-05-20T21:26:25.565703Z"
    }
   },
   "id": "78d274fa9d5c7039",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class SklearnMLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=100, lr=1e-3, batch_size=32, device='cpu', n_classes=1, verbose=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.n_classes = n_classes\n",
    "        self.verbose = verbose\n",
    "        self.history = []\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = MLPClassifier(input_dim=self.input_dim, num_classes=self.n_classes).to(self.device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(self.device))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import average_precision_score\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        device = torch.device(self.device)\n",
    "        if self.verbose:\n",
    "            print(f\"\\n🖥️  Using device: {device}\")\n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "        # Automatically create a validation set if not provided (e.g., when using MultiOutputClassifier)\n",
    "        if X_val is None or y_val is None:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, stratify=y_train if y_train.ndim == 1 else None\n",
    "            )\n",
    "    \n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "    \n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5)\n",
    "        best_val_map = float('-inf')\n",
    "        best_model_state = None\n",
    "        epochs_without_improvement = 0\n",
    "        patience = 20\n",
    "        start_time = time.time()\n",
    "    \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            all_preds_train, all_targets_train = [], []\n",
    "    \n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(xb)\n",
    "    \n",
    "                # Ensure yb has the same shape as logits\n",
    "                if yb.ndim == 1:\n",
    "                    yb = yb.unsqueeze(1)\n",
    "    \n",
    "                loss = self.criterion(logits, yb.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "    \n",
    "                preds = torch.sigmoid(logits).detach().cpu()\n",
    "                all_preds_train.append(preds)\n",
    "                all_targets_train.append(yb.detach().cpu())\n",
    "    \n",
    "            avg_train_loss = epoch_loss / num_batches\n",
    "            all_preds_train = torch.cat(all_preds_train).numpy()\n",
    "            all_targets_train = torch.cat(all_targets_train).numpy()\n",
    "    \n",
    "            try:\n",
    "                preds_binary_train = (all_preds_train > 0.5).astype(int)\n",
    "                train_map = average_precision_score(all_targets_train, preds_binary_train, average=\"macro\")\n",
    "\n",
    "                # train_map = average_precision_score(all_targets_train, all_preds_train, average=\"macro\")\n",
    "            except ValueError:\n",
    "                train_map = float(\"nan\")\n",
    "    \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            all_preds_val, all_targets_val = [], []\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    logits = self.model(xb)\n",
    "    \n",
    "                    if yb.ndim == 1:\n",
    "                        yb = yb.unsqueeze(1)\n",
    "    \n",
    "                    preds = torch.sigmoid(logits)\n",
    "                    loss = self.criterion(logits, yb.float())\n",
    "    \n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    all_preds_val.append(preds.cpu())\n",
    "                    all_targets_val.append(yb.cpu())\n",
    "    \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            all_preds_val = torch.cat(all_preds_val).numpy()\n",
    "            all_targets_val = torch.cat(all_targets_val).numpy()\n",
    "    \n",
    "            try:\n",
    "                preds_binary_val = (all_preds_val > 0.5).astype(int)\n",
    "                val_map = average_precision_score(all_targets_val, preds_binary_val, average=\"macro\")\n",
    "\n",
    "                # val_map = average_precision_score(all_targets_val, all_preds_val, average=\"macro\")\n",
    "            except ValueError:\n",
    "                val_map = float(\"nan\")\n",
    "    \n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            elapsed = time.time() - start_time\n",
    "    \n",
    "            if self.verbose:\n",
    "                print(f\"\\n✅ Epoch {epoch}/{self.epochs} — \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f} | Train mAP: {train_map:.4f} \"\n",
    "                      f\"| Val Loss: {avg_val_loss:.4f} | Val mAP: {val_map:.4f} \"\n",
    "                      f\"| LR: {current_lr:.6f} | Elapsed: {elapsed:.1f}s\")\n",
    "    \n",
    "            self.history.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_map\": train_map,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_map\": val_map,\n",
    "                \"lr\": current_lr\n",
    "            })\n",
    "    \n",
    "            scheduler.step(avg_val_loss)\n",
    "    \n",
    "            if val_map > best_val_map:\n",
    "                best_val_map = val_map\n",
    "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\n🛑 Early stopping at epoch {epoch}. No val mAP improvement for {patience} epochs.\")\n",
    "                    break\n",
    "    \n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "        if best_model_state:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "    \n",
    "        if self.verbose:\n",
    "            print(f\"\\n🏁 Training complete. Best val mAP: {best_val_map:.4f}. Total time: {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "        self.classes_ = [0, 1]\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
    "            print(\"logits.shape:\", logits.shape)\n",
    "        return (probs > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        return probs\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return average_precision_score(y, y_pred)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'input_dim': self.input_dim,\n",
    "            'epochs': self.epochs,\n",
    "            'lr': self.lr,\n",
    "            'batch_size': self.batch_size,\n",
    "            'device': self.device,\n",
    "            'n_classes': self.n_classes,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            setattr(self, k, v)\n",
    "        self._build_model()\n",
    "        return self\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:32:23.656401Z",
     "start_time": "2025-05-20T21:32:23.651503Z"
    }
   },
   "id": "c68e2dbc7c467c01",
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.6037 | Train mAP: 0.6797 | Val Loss: 0.5306 | Val mAP: 0.7996 | LR: 0.001000 | Elapsed: 0.4s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.5405 | Train mAP: 0.7705 | Val Loss: 0.5040 | Val mAP: 0.8367 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.5265 | Train mAP: 0.7875 | Val Loss: 0.5096 | Val mAP: 0.8356 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.5126 | Train mAP: 0.8065 | Val Loss: 0.4856 | Val mAP: 0.8368 | LR: 0.001000 | Elapsed: 1.2s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.4902 | Train mAP: 0.8237 | Val Loss: 0.4681 | Val mAP: 0.8560 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.4936 | Train mAP: 0.8216 | Val Loss: 0.4862 | Val mAP: 0.8420 | LR: 0.001000 | Elapsed: 1.7s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.4815 | Train mAP: 0.8343 | Val Loss: 0.4682 | Val mAP: 0.8551 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.4746 | Train mAP: 0.8387 | Val Loss: 0.4692 | Val mAP: 0.8549 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.4682 | Train mAP: 0.8437 | Val Loss: 0.4611 | Val mAP: 0.8585 | LR: 0.001000 | Elapsed: 2.5s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.4547 | Train mAP: 0.8538 | Val Loss: 0.4579 | Val mAP: 0.8560 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.4573 | Train mAP: 0.8482 | Val Loss: 0.4595 | Val mAP: 0.8615 | LR: 0.001000 | Elapsed: 3.1s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.4573 | Train mAP: 0.8524 | Val Loss: 0.4694 | Val mAP: 0.8632 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.4498 | Train mAP: 0.8573 | Val Loss: 0.4758 | Val mAP: 0.8483 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.4534 | Train mAP: 0.8552 | Val Loss: 0.4614 | Val mAP: 0.8560 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.4389 | Train mAP: 0.8661 | Val Loss: 0.4737 | Val mAP: 0.8550 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.4436 | Train mAP: 0.8600 | Val Loss: 0.4633 | Val mAP: 0.8620 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.4261 | Train mAP: 0.8727 | Val Loss: 0.4628 | Val mAP: 0.8552 | LR: 0.000500 | Elapsed: 4.7s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.4251 | Train mAP: 0.8789 | Val Loss: 0.4507 | Val mAP: 0.8644 | LR: 0.000500 | Elapsed: 4.9s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.4277 | Train mAP: 0.8705 | Val Loss: 0.4587 | Val mAP: 0.8576 | LR: 0.000500 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.4152 | Train mAP: 0.8863 | Val Loss: 0.4489 | Val mAP: 0.8626 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.4068 | Train mAP: 0.8866 | Val Loss: 0.4517 | Val mAP: 0.8633 | LR: 0.000500 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.4193 | Train mAP: 0.8742 | Val Loss: 0.4557 | Val mAP: 0.8662 | LR: 0.000500 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.4136 | Train mAP: 0.8809 | Val Loss: 0.4510 | Val mAP: 0.8612 | LR: 0.000500 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.4045 | Train mAP: 0.8892 | Val Loss: 0.4524 | Val mAP: 0.8647 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.4061 | Train mAP: 0.8855 | Val Loss: 0.4521 | Val mAP: 0.8658 | LR: 0.000500 | Elapsed: 6.8s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.3953 | Train mAP: 0.8931 | Val Loss: 0.4543 | Val mAP: 0.8665 | LR: 0.000500 | Elapsed: 7.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.4054 | Train mAP: 0.8863 | Val Loss: 0.4529 | Val mAP: 0.8662 | LR: 0.000250 | Elapsed: 7.5s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.3958 | Train mAP: 0.8919 | Val Loss: 0.4473 | Val mAP: 0.8670 | LR: 0.000250 | Elapsed: 7.8s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.3936 | Train mAP: 0.8986 | Val Loss: 0.4501 | Val mAP: 0.8706 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.3888 | Train mAP: 0.8968 | Val Loss: 0.4586 | Val mAP: 0.8649 | LR: 0.000250 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.3984 | Train mAP: 0.8920 | Val Loss: 0.4476 | Val mAP: 0.8689 | LR: 0.000250 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.3809 | Train mAP: 0.9010 | Val Loss: 0.4453 | Val mAP: 0.8705 | LR: 0.000250 | Elapsed: 8.8s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.3849 | Train mAP: 0.9029 | Val Loss: 0.4502 | Val mAP: 0.8650 | LR: 0.000250 | Elapsed: 9.0s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.3847 | Train mAP: 0.9020 | Val Loss: 0.4559 | Val mAP: 0.8620 | LR: 0.000250 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.3901 | Train mAP: 0.8978 | Val Loss: 0.4422 | Val mAP: 0.8683 | LR: 0.000250 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.3769 | Train mAP: 0.9024 | Val Loss: 0.4419 | Val mAP: 0.8720 | LR: 0.000250 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.3744 | Train mAP: 0.9049 | Val Loss: 0.4433 | Val mAP: 0.8704 | LR: 0.000250 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.3785 | Train mAP: 0.9021 | Val Loss: 0.4467 | Val mAP: 0.8692 | LR: 0.000250 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.3719 | Train mAP: 0.9080 | Val Loss: 0.4392 | Val mAP: 0.8731 | LR: 0.000250 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.3870 | Train mAP: 0.8991 | Val Loss: 0.4543 | Val mAP: 0.8625 | LR: 0.000250 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.3698 | Train mAP: 0.9084 | Val Loss: 0.4463 | Val mAP: 0.8682 | LR: 0.000250 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.3697 | Train mAP: 0.9083 | Val Loss: 0.4435 | Val mAP: 0.8703 | LR: 0.000250 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.3782 | Train mAP: 0.9035 | Val Loss: 0.4420 | Val mAP: 0.8713 | LR: 0.000250 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.3699 | Train mAP: 0.9101 | Val Loss: 0.4474 | Val mAP: 0.8673 | LR: 0.000250 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.3725 | Train mAP: 0.9076 | Val Loss: 0.4405 | Val mAP: 0.8719 | LR: 0.000250 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.3632 | Train mAP: 0.9111 | Val Loss: 0.4452 | Val mAP: 0.8709 | LR: 0.000125 | Elapsed: 12.3s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.3830 | Train mAP: 0.9031 | Val Loss: 0.4422 | Val mAP: 0.8712 | LR: 0.000125 | Elapsed: 12.6s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.3543 | Train mAP: 0.9193 | Val Loss: 0.4409 | Val mAP: 0.8725 | LR: 0.000125 | Elapsed: 12.8s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.3677 | Train mAP: 0.9085 | Val Loss: 0.4447 | Val mAP: 0.8700 | LR: 0.000125 | Elapsed: 13.1s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.3600 | Train mAP: 0.9174 | Val Loss: 0.4427 | Val mAP: 0.8702 | LR: 0.000125 | Elapsed: 13.4s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.3529 | Train mAP: 0.9168 | Val Loss: 0.4438 | Val mAP: 0.8709 | LR: 0.000125 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.3563 | Train mAP: 0.9145 | Val Loss: 0.4560 | Val mAP: 0.8681 | LR: 0.000063 | Elapsed: 13.9s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.3446 | Train mAP: 0.9209 | Val Loss: 0.4483 | Val mAP: 0.8695 | LR: 0.000063 | Elapsed: 14.2s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.3600 | Train mAP: 0.9102 | Val Loss: 0.4426 | Val mAP: 0.8744 | LR: 0.000063 | Elapsed: 14.4s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.3501 | Train mAP: 0.9185 | Val Loss: 0.4484 | Val mAP: 0.8699 | LR: 0.000063 | Elapsed: 14.7s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.3460 | Train mAP: 0.9213 | Val Loss: 0.4416 | Val mAP: 0.8746 | LR: 0.000063 | Elapsed: 14.9s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.3532 | Train mAP: 0.9173 | Val Loss: 0.4420 | Val mAP: 0.8730 | LR: 0.000063 | Elapsed: 15.2s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.3576 | Train mAP: 0.9140 | Val Loss: 0.4434 | Val mAP: 0.8742 | LR: 0.000031 | Elapsed: 15.4s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.3521 | Train mAP: 0.9164 | Val Loss: 0.4433 | Val mAP: 0.8728 | LR: 0.000031 | Elapsed: 15.7s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.3422 | Train mAP: 0.9254 | Val Loss: 0.4449 | Val mAP: 0.8735 | LR: 0.000031 | Elapsed: 15.9s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.3559 | Train mAP: 0.9157 | Val Loss: 0.4465 | Val mAP: 0.8709 | LR: 0.000031 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.3475 | Train mAP: 0.9215 | Val Loss: 0.4432 | Val mAP: 0.8730 | LR: 0.000031 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.3534 | Train mAP: 0.9149 | Val Loss: 0.4405 | Val mAP: 0.8743 | LR: 0.000031 | Elapsed: 16.6s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.3480 | Train mAP: 0.9188 | Val Loss: 0.4455 | Val mAP: 0.8748 | LR: 0.000016 | Elapsed: 16.9s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.3506 | Train mAP: 0.9191 | Val Loss: 0.4437 | Val mAP: 0.8740 | LR: 0.000016 | Elapsed: 17.1s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.3539 | Train mAP: 0.9175 | Val Loss: 0.4444 | Val mAP: 0.8737 | LR: 0.000016 | Elapsed: 17.3s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.3425 | Train mAP: 0.9245 | Val Loss: 0.4429 | Val mAP: 0.8723 | LR: 0.000016 | Elapsed: 17.6s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.3445 | Train mAP: 0.9224 | Val Loss: 0.4483 | Val mAP: 0.8727 | LR: 0.000016 | Elapsed: 17.8s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.3391 | Train mAP: 0.9253 | Val Loss: 0.4406 | Val mAP: 0.8733 | LR: 0.000016 | Elapsed: 18.1s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.3427 | Train mAP: 0.9235 | Val Loss: 0.4425 | Val mAP: 0.8740 | LR: 0.000008 | Elapsed: 18.3s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.3427 | Train mAP: 0.9225 | Val Loss: 0.4456 | Val mAP: 0.8727 | LR: 0.000008 | Elapsed: 18.5s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.3582 | Train mAP: 0.9134 | Val Loss: 0.4452 | Val mAP: 0.8727 | LR: 0.000008 | Elapsed: 18.8s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.3596 | Train mAP: 0.9117 | Val Loss: 0.4436 | Val mAP: 0.8730 | LR: 0.000008 | Elapsed: 19.0s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.3407 | Train mAP: 0.9240 | Val Loss: 0.4439 | Val mAP: 0.8734 | LR: 0.000008 | Elapsed: 19.3s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.3497 | Train mAP: 0.9186 | Val Loss: 0.4459 | Val mAP: 0.8732 | LR: 0.000008 | Elapsed: 19.5s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.3508 | Train mAP: 0.9179 | Val Loss: 0.4452 | Val mAP: 0.8740 | LR: 0.000004 | Elapsed: 19.7s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.3542 | Train mAP: 0.9170 | Val Loss: 0.4490 | Val mAP: 0.8718 | LR: 0.000004 | Elapsed: 20.0s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.3505 | Train mAP: 0.9209 | Val Loss: 0.4412 | Val mAP: 0.8726 | LR: 0.000004 | Elapsed: 20.2s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.3395 | Train mAP: 0.9234 | Val Loss: 0.4443 | Val mAP: 0.8715 | LR: 0.000004 | Elapsed: 20.5s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.3512 | Train mAP: 0.9185 | Val Loss: 0.4448 | Val mAP: 0.8732 | LR: 0.000004 | Elapsed: 20.7s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.3414 | Train mAP: 0.9256 | Val Loss: 0.4428 | Val mAP: 0.8735 | LR: 0.000004 | Elapsed: 21.0s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.3445 | Train mAP: 0.9211 | Val Loss: 0.4391 | Val mAP: 0.8748 | LR: 0.000002 | Elapsed: 21.2s\n",
      "\n",
      "✅ Epoch 83/200 — Train Loss: 0.3459 | Train mAP: 0.9228 | Val Loss: 0.4415 | Val mAP: 0.8730 | LR: 0.000002 | Elapsed: 21.4s\n",
      "\n",
      "✅ Epoch 84/200 — Train Loss: 0.3444 | Train mAP: 0.9225 | Val Loss: 0.4424 | Val mAP: 0.8738 | LR: 0.000002 | Elapsed: 21.7s\n",
      "\n",
      "✅ Epoch 85/200 — Train Loss: 0.3588 | Train mAP: 0.9149 | Val Loss: 0.4447 | Val mAP: 0.8740 | LR: 0.000002 | Elapsed: 22.0s\n",
      "\n",
      "✅ Epoch 86/200 — Train Loss: 0.3488 | Train mAP: 0.9189 | Val Loss: 0.4475 | Val mAP: 0.8735 | LR: 0.000002 | Elapsed: 22.2s\n",
      "\n",
      "✅ Epoch 87/200 — Train Loss: 0.3451 | Train mAP: 0.9209 | Val Loss: 0.4394 | Val mAP: 0.8741 | LR: 0.000002 | Elapsed: 22.5s\n",
      "\n",
      "✅ Epoch 88/200 — Train Loss: 0.3391 | Train mAP: 0.9252 | Val Loss: 0.4491 | Val mAP: 0.8702 | LR: 0.000002 | Elapsed: 22.7s\n",
      "\n",
      "✅ Epoch 89/200 — Train Loss: 0.3574 | Train mAP: 0.9157 | Val Loss: 0.4445 | Val mAP: 0.8736 | LR: 0.000001 | Elapsed: 23.0s\n",
      "\n",
      "✅ Epoch 90/200 — Train Loss: 0.3432 | Train mAP: 0.9226 | Val Loss: 0.4482 | Val mAP: 0.8710 | LR: 0.000001 | Elapsed: 23.3s\n",
      "\n",
      "✅ Epoch 91/200 — Train Loss: 0.3396 | Train mAP: 0.9244 | Val Loss: 0.4433 | Val mAP: 0.8726 | LR: 0.000001 | Elapsed: 23.5s\n",
      "\n",
      "✅ Epoch 92/200 — Train Loss: 0.3441 | Train mAP: 0.9216 | Val Loss: 0.4437 | Val mAP: 0.8739 | LR: 0.000001 | Elapsed: 23.8s\n",
      "\n",
      "✅ Epoch 93/200 — Train Loss: 0.3427 | Train mAP: 0.9207 | Val Loss: 0.4483 | Val mAP: 0.8740 | LR: 0.000001 | Elapsed: 24.0s\n",
      "\n",
      "✅ Epoch 94/200 — Train Loss: 0.3424 | Train mAP: 0.9238 | Val Loss: 0.4453 | Val mAP: 0.8723 | LR: 0.000001 | Elapsed: 24.3s\n",
      "\n",
      "✅ Epoch 95/200 — Train Loss: 0.3517 | Train mAP: 0.9180 | Val Loss: 0.4444 | Val mAP: 0.8740 | LR: 0.000000 | Elapsed: 24.5s\n",
      "\n",
      "✅ Epoch 96/200 — Train Loss: 0.3456 | Train mAP: 0.9207 | Val Loss: 0.4436 | Val mAP: 0.8726 | LR: 0.000000 | Elapsed: 24.8s\n",
      "\n",
      "✅ Epoch 97/200 — Train Loss: 0.3451 | Train mAP: 0.9218 | Val Loss: 0.4439 | Val mAP: 0.8729 | LR: 0.000000 | Elapsed: 25.0s\n",
      "\n",
      "✅ Epoch 98/200 — Train Loss: 0.3458 | Train mAP: 0.9198 | Val Loss: 0.4452 | Val mAP: 0.8725 | LR: 0.000000 | Elapsed: 25.3s\n",
      "\n",
      "✅ Epoch 99/200 — Train Loss: 0.3569 | Train mAP: 0.9144 | Val Loss: 0.4475 | Val mAP: 0.8716 | LR: 0.000000 | Elapsed: 25.6s\n",
      "\n",
      "✅ Epoch 100/200 — Train Loss: 0.3439 | Train mAP: 0.9232 | Val Loss: 0.4459 | Val mAP: 0.8727 | LR: 0.000000 | Elapsed: 26.3s\n",
      "\n",
      "✅ Epoch 101/200 — Train Loss: 0.3451 | Train mAP: 0.9223 | Val Loss: 0.4435 | Val mAP: 0.8739 | LR: 0.000000 | Elapsed: 26.9s\n",
      "\n",
      "✅ Epoch 102/200 — Train Loss: 0.3427 | Train mAP: 0.9229 | Val Loss: 0.4440 | Val mAP: 0.8717 | LR: 0.000000 | Elapsed: 27.3s\n",
      "\n",
      "🛑 Early stopping at epoch 102. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.8748. Total time: 27.32s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3300 | Train mAP: 0.0376 | Val Loss: 0.2396 | Val mAP: 0.1044 | LR: 0.001000 | Elapsed: 0.4s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1815 | Train mAP: 0.0629 | Val Loss: 0.1916 | Val mAP: 0.1135 | LR: 0.001000 | Elapsed: 0.8s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1726 | Train mAP: 0.0731 | Val Loss: 0.1703 | Val mAP: 0.1317 | LR: 0.001000 | Elapsed: 1.0s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1698 | Train mAP: 0.0628 | Val Loss: 0.1564 | Val mAP: 0.1723 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1652 | Train mAP: 0.0706 | Val Loss: 0.1659 | Val mAP: 0.1901 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1668 | Train mAP: 0.0745 | Val Loss: 0.1498 | Val mAP: 0.2170 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1590 | Train mAP: 0.0979 | Val Loss: 0.1429 | Val mAP: 0.2707 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1606 | Train mAP: 0.1131 | Val Loss: 0.1492 | Val mAP: 0.2227 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1544 | Train mAP: 0.1262 | Val Loss: 0.1472 | Val mAP: 0.2324 | LR: 0.001000 | Elapsed: 2.7s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1499 | Train mAP: 0.1419 | Val Loss: 0.1452 | Val mAP: 0.2458 | LR: 0.001000 | Elapsed: 3.0s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1460 | Train mAP: 0.1572 | Val Loss: 0.1407 | Val mAP: 0.2696 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1467 | Train mAP: 0.1742 | Val Loss: 0.1397 | Val mAP: 0.2297 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1508 | Train mAP: 0.1614 | Val Loss: 0.1428 | Val mAP: 0.2419 | LR: 0.001000 | Elapsed: 4.0s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1469 | Train mAP: 0.1959 | Val Loss: 0.1557 | Val mAP: 0.2035 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1483 | Train mAP: 0.1678 | Val Loss: 0.1426 | Val mAP: 0.2655 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1465 | Train mAP: 0.1872 | Val Loss: 0.1531 | Val mAP: 0.2891 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1414 | Train mAP: 0.2220 | Val Loss: 0.1380 | Val mAP: 0.3084 | LR: 0.001000 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1468 | Train mAP: 0.1886 | Val Loss: 0.1610 | Val mAP: 0.2527 | LR: 0.001000 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1364 | Train mAP: 0.2440 | Val Loss: 0.1323 | Val mAP: 0.2993 | LR: 0.001000 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1463 | Train mAP: 0.2050 | Val Loss: 0.1510 | Val mAP: 0.2283 | LR: 0.001000 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1414 | Train mAP: 0.2176 | Val Loss: 0.1439 | Val mAP: 0.2787 | LR: 0.001000 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1378 | Train mAP: 0.2510 | Val Loss: 0.1435 | Val mAP: 0.2454 | LR: 0.001000 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1406 | Train mAP: 0.2318 | Val Loss: 0.1409 | Val mAP: 0.3290 | LR: 0.001000 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1358 | Train mAP: 0.2479 | Val Loss: 0.1393 | Val mAP: 0.2862 | LR: 0.001000 | Elapsed: 9.1s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1341 | Train mAP: 0.3133 | Val Loss: 0.1373 | Val mAP: 0.3154 | LR: 0.001000 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1310 | Train mAP: 0.2916 | Val Loss: 0.1333 | Val mAP: 0.3247 | LR: 0.000500 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1250 | Train mAP: 0.3451 | Val Loss: 0.1377 | Val mAP: 0.3106 | LR: 0.000500 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1290 | Train mAP: 0.3291 | Val Loss: 0.1413 | Val mAP: 0.3288 | LR: 0.000500 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1218 | Train mAP: 0.3834 | Val Loss: 0.1471 | Val mAP: 0.3483 | LR: 0.000500 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1284 | Train mAP: 0.3642 | Val Loss: 0.1401 | Val mAP: 0.3427 | LR: 0.000500 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1200 | Train mAP: 0.3800 | Val Loss: 0.1386 | Val mAP: 0.3594 | LR: 0.000500 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1213 | Train mAP: 0.4075 | Val Loss: 0.1411 | Val mAP: 0.3493 | LR: 0.000250 | Elapsed: 11.6s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1115 | Train mAP: 0.4503 | Val Loss: 0.1437 | Val mAP: 0.3477 | LR: 0.000250 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1186 | Train mAP: 0.4008 | Val Loss: 0.1427 | Val mAP: 0.3317 | LR: 0.000250 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1168 | Train mAP: 0.4136 | Val Loss: 0.1442 | Val mAP: 0.3311 | LR: 0.000250 | Elapsed: 12.3s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1167 | Train mAP: 0.4449 | Val Loss: 0.1442 | Val mAP: 0.3639 | LR: 0.000250 | Elapsed: 12.5s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1172 | Train mAP: 0.4157 | Val Loss: 0.1401 | Val mAP: 0.3496 | LR: 0.000250 | Elapsed: 12.8s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1140 | Train mAP: 0.4271 | Val Loss: 0.1425 | Val mAP: 0.3480 | LR: 0.000125 | Elapsed: 13.0s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1155 | Train mAP: 0.4200 | Val Loss: 0.1446 | Val mAP: 0.3250 | LR: 0.000125 | Elapsed: 13.3s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1142 | Train mAP: 0.4319 | Val Loss: 0.1439 | Val mAP: 0.3517 | LR: 0.000125 | Elapsed: 13.5s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1168 | Train mAP: 0.4099 | Val Loss: 0.1410 | Val mAP: 0.3434 | LR: 0.000125 | Elapsed: 13.7s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1102 | Train mAP: 0.4546 | Val Loss: 0.1402 | Val mAP: 0.3371 | LR: 0.000125 | Elapsed: 14.0s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1097 | Train mAP: 0.4900 | Val Loss: 0.1390 | Val mAP: 0.3494 | LR: 0.000125 | Elapsed: 14.2s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1088 | Train mAP: 0.4788 | Val Loss: 0.1405 | Val mAP: 0.3595 | LR: 0.000063 | Elapsed: 14.4s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1111 | Train mAP: 0.4660 | Val Loss: 0.1398 | Val mAP: 0.3501 | LR: 0.000063 | Elapsed: 14.7s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1115 | Train mAP: 0.4557 | Val Loss: 0.1416 | Val mAP: 0.3493 | LR: 0.000063 | Elapsed: 15.0s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1133 | Train mAP: 0.4315 | Val Loss: 0.1400 | Val mAP: 0.3768 | LR: 0.000063 | Elapsed: 15.2s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1152 | Train mAP: 0.4259 | Val Loss: 0.1423 | Val mAP: 0.3643 | LR: 0.000063 | Elapsed: 15.4s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1078 | Train mAP: 0.4587 | Val Loss: 0.1414 | Val mAP: 0.3695 | LR: 0.000063 | Elapsed: 15.6s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1077 | Train mAP: 0.4917 | Val Loss: 0.1390 | Val mAP: 0.3783 | LR: 0.000031 | Elapsed: 15.9s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1097 | Train mAP: 0.4768 | Val Loss: 0.1420 | Val mAP: 0.3886 | LR: 0.000031 | Elapsed: 16.1s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1063 | Train mAP: 0.5078 | Val Loss: 0.1397 | Val mAP: 0.3672 | LR: 0.000031 | Elapsed: 16.3s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1100 | Train mAP: 0.4651 | Val Loss: 0.1375 | Val mAP: 0.3750 | LR: 0.000031 | Elapsed: 16.6s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1097 | Train mAP: 0.4555 | Val Loss: 0.1402 | Val mAP: 0.3828 | LR: 0.000031 | Elapsed: 16.8s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1079 | Train mAP: 0.4655 | Val Loss: 0.1393 | Val mAP: 0.3598 | LR: 0.000031 | Elapsed: 17.0s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1133 | Train mAP: 0.4623 | Val Loss: 0.1377 | Val mAP: 0.3791 | LR: 0.000016 | Elapsed: 17.2s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.1113 | Train mAP: 0.4660 | Val Loss: 0.1390 | Val mAP: 0.3792 | LR: 0.000016 | Elapsed: 17.5s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.1072 | Train mAP: 0.4835 | Val Loss: 0.1411 | Val mAP: 0.3817 | LR: 0.000016 | Elapsed: 17.7s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.1103 | Train mAP: 0.4639 | Val Loss: 0.1403 | Val mAP: 0.3799 | LR: 0.000016 | Elapsed: 17.9s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.1167 | Train mAP: 0.4246 | Val Loss: 0.1396 | Val mAP: 0.3747 | LR: 0.000016 | Elapsed: 18.2s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.1070 | Train mAP: 0.4840 | Val Loss: 0.1384 | Val mAP: 0.3760 | LR: 0.000016 | Elapsed: 18.4s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.1072 | Train mAP: 0.4582 | Val Loss: 0.1395 | Val mAP: 0.3792 | LR: 0.000008 | Elapsed: 18.7s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.1063 | Train mAP: 0.4829 | Val Loss: 0.1407 | Val mAP: 0.3803 | LR: 0.000008 | Elapsed: 18.9s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.1113 | Train mAP: 0.4346 | Val Loss: 0.1407 | Val mAP: 0.3824 | LR: 0.000008 | Elapsed: 19.1s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.1098 | Train mAP: 0.4626 | Val Loss: 0.1399 | Val mAP: 0.3771 | LR: 0.000008 | Elapsed: 19.4s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.1054 | Train mAP: 0.4957 | Val Loss: 0.1403 | Val mAP: 0.3826 | LR: 0.000008 | Elapsed: 19.6s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.1056 | Train mAP: 0.4860 | Val Loss: 0.1393 | Val mAP: 0.3796 | LR: 0.000008 | Elapsed: 19.8s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.1066 | Train mAP: 0.4802 | Val Loss: 0.1405 | Val mAP: 0.3794 | LR: 0.000004 | Elapsed: 20.1s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.1082 | Train mAP: 0.4872 | Val Loss: 0.1395 | Val mAP: 0.3731 | LR: 0.000004 | Elapsed: 20.3s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.1120 | Train mAP: 0.4551 | Val Loss: 0.1391 | Val mAP: 0.3807 | LR: 0.000004 | Elapsed: 20.5s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.1078 | Train mAP: 0.5004 | Val Loss: 0.1401 | Val mAP: 0.3717 | LR: 0.000004 | Elapsed: 20.8s\n",
      "\n",
      "🛑 Early stopping at epoch 71. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.3886. Total time: 20.77s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3729 | Train mAP: 0.1528 | Val Loss: 0.3342 | Val mAP: 0.3528 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.2887 | Train mAP: 0.2851 | Val Loss: 0.2998 | Val mAP: 0.3130 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.2836 | Train mAP: 0.3015 | Val Loss: 0.2718 | Val mAP: 0.3900 | LR: 0.001000 | Elapsed: 0.8s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.2686 | Train mAP: 0.3596 | Val Loss: 0.2640 | Val mAP: 0.4229 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.2514 | Train mAP: 0.3944 | Val Loss: 0.2528 | Val mAP: 0.4820 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.2477 | Train mAP: 0.4354 | Val Loss: 0.2402 | Val mAP: 0.4817 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.2369 | Train mAP: 0.4652 | Val Loss: 0.2429 | Val mAP: 0.5026 | LR: 0.001000 | Elapsed: 1.8s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.2411 | Train mAP: 0.4657 | Val Loss: 0.2299 | Val mAP: 0.5356 | LR: 0.001000 | Elapsed: 2.1s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.2353 | Train mAP: 0.4838 | Val Loss: 0.2386 | Val mAP: 0.5132 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.2317 | Train mAP: 0.4825 | Val Loss: 0.2316 | Val mAP: 0.5287 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.2281 | Train mAP: 0.5265 | Val Loss: 0.2283 | Val mAP: 0.5457 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.2286 | Train mAP: 0.4956 | Val Loss: 0.2392 | Val mAP: 0.5149 | LR: 0.001000 | Elapsed: 3.1s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.2269 | Train mAP: 0.5172 | Val Loss: 0.2283 | Val mAP: 0.5483 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.2199 | Train mAP: 0.5497 | Val Loss: 0.2251 | Val mAP: 0.5530 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.2120 | Train mAP: 0.5623 | Val Loss: 0.2206 | Val mAP: 0.5738 | LR: 0.001000 | Elapsed: 3.8s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.2130 | Train mAP: 0.5705 | Val Loss: 0.2290 | Val mAP: 0.5153 | LR: 0.001000 | Elapsed: 4.0s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.2043 | Train mAP: 0.5873 | Val Loss: 0.2253 | Val mAP: 0.5240 | LR: 0.001000 | Elapsed: 4.2s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.2070 | Train mAP: 0.6079 | Val Loss: 0.2255 | Val mAP: 0.5735 | LR: 0.001000 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.2079 | Train mAP: 0.5968 | Val Loss: 0.2201 | Val mAP: 0.5618 | LR: 0.001000 | Elapsed: 4.7s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1975 | Train mAP: 0.6101 | Val Loss: 0.2297 | Val mAP: 0.5251 | LR: 0.001000 | Elapsed: 4.9s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.2013 | Train mAP: 0.6182 | Val Loss: 0.2243 | Val mAP: 0.5578 | LR: 0.001000 | Elapsed: 5.1s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.2042 | Train mAP: 0.6151 | Val Loss: 0.2250 | Val mAP: 0.5644 | LR: 0.001000 | Elapsed: 5.4s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1973 | Train mAP: 0.6258 | Val Loss: 0.2252 | Val mAP: 0.5330 | LR: 0.001000 | Elapsed: 5.6s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.2024 | Train mAP: 0.6097 | Val Loss: 0.2267 | Val mAP: 0.5383 | LR: 0.001000 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1888 | Train mAP: 0.6479 | Val Loss: 0.2229 | Val mAP: 0.5815 | LR: 0.001000 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1970 | Train mAP: 0.6369 | Val Loss: 0.2252 | Val mAP: 0.5620 | LR: 0.000500 | Elapsed: 6.2s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1886 | Train mAP: 0.6546 | Val Loss: 0.2206 | Val mAP: 0.5890 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1830 | Train mAP: 0.6794 | Val Loss: 0.2273 | Val mAP: 0.5871 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1812 | Train mAP: 0.6711 | Val Loss: 0.2277 | Val mAP: 0.6056 | LR: 0.000500 | Elapsed: 6.9s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1761 | Train mAP: 0.6885 | Val Loss: 0.2203 | Val mAP: 0.5765 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1748 | Train mAP: 0.6971 | Val Loss: 0.2260 | Val mAP: 0.6031 | LR: 0.000500 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1661 | Train mAP: 0.7220 | Val Loss: 0.2190 | Val mAP: 0.5706 | LR: 0.000250 | Elapsed: 7.6s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1727 | Train mAP: 0.7052 | Val Loss: 0.2191 | Val mAP: 0.5821 | LR: 0.000250 | Elapsed: 7.9s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1678 | Train mAP: 0.7249 | Val Loss: 0.2163 | Val mAP: 0.6039 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1663 | Train mAP: 0.7258 | Val Loss: 0.2182 | Val mAP: 0.5850 | LR: 0.000250 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1667 | Train mAP: 0.7242 | Val Loss: 0.2151 | Val mAP: 0.6074 | LR: 0.000250 | Elapsed: 8.5s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1647 | Train mAP: 0.7317 | Val Loss: 0.2195 | Val mAP: 0.5825 | LR: 0.000250 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1623 | Train mAP: 0.7308 | Val Loss: 0.2166 | Val mAP: 0.5958 | LR: 0.000250 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1604 | Train mAP: 0.7413 | Val Loss: 0.2126 | Val mAP: 0.5946 | LR: 0.000250 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1588 | Train mAP: 0.7460 | Val Loss: 0.2227 | Val mAP: 0.6053 | LR: 0.000250 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1709 | Train mAP: 0.7130 | Val Loss: 0.2241 | Val mAP: 0.6034 | LR: 0.000250 | Elapsed: 9.6s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1693 | Train mAP: 0.7202 | Val Loss: 0.2160 | Val mAP: 0.5974 | LR: 0.000250 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1593 | Train mAP: 0.7399 | Val Loss: 0.2136 | Val mAP: 0.6058 | LR: 0.000250 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1595 | Train mAP: 0.7404 | Val Loss: 0.2111 | Val mAP: 0.6073 | LR: 0.000250 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1647 | Train mAP: 0.7266 | Val Loss: 0.2226 | Val mAP: 0.5913 | LR: 0.000250 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1594 | Train mAP: 0.7417 | Val Loss: 0.2281 | Val mAP: 0.5884 | LR: 0.000250 | Elapsed: 10.7s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1552 | Train mAP: 0.7540 | Val Loss: 0.2209 | Val mAP: 0.5766 | LR: 0.000250 | Elapsed: 10.9s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1635 | Train mAP: 0.7374 | Val Loss: 0.2204 | Val mAP: 0.5705 | LR: 0.000250 | Elapsed: 11.1s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1547 | Train mAP: 0.7561 | Val Loss: 0.2197 | Val mAP: 0.5901 | LR: 0.000250 | Elapsed: 11.4s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1498 | Train mAP: 0.7746 | Val Loss: 0.2173 | Val mAP: 0.5760 | LR: 0.000250 | Elapsed: 11.6s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1507 | Train mAP: 0.7676 | Val Loss: 0.2178 | Val mAP: 0.5601 | LR: 0.000125 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1495 | Train mAP: 0.7800 | Val Loss: 0.2175 | Val mAP: 0.5803 | LR: 0.000125 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1492 | Train mAP: 0.7759 | Val Loss: 0.2136 | Val mAP: 0.5952 | LR: 0.000125 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1482 | Train mAP: 0.7745 | Val Loss: 0.2186 | Val mAP: 0.5840 | LR: 0.000125 | Elapsed: 12.5s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1490 | Train mAP: 0.7759 | Val Loss: 0.2115 | Val mAP: 0.6001 | LR: 0.000125 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1548 | Train mAP: 0.7599 | Val Loss: 0.2144 | Val mAP: 0.6003 | LR: 0.000125 | Elapsed: 12.9s\n",
      "\n",
      "🛑 Early stopping at epoch 56. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.6074. Total time: 12.91s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.5057 | Train mAP: 0.2166 | Val Loss: 0.4329 | Val mAP: 0.4335 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.4217 | Train mAP: 0.3307 | Val Loss: 0.3771 | Val mAP: 0.5157 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.3965 | Train mAP: 0.4011 | Val Loss: 0.3622 | Val mAP: 0.5121 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.3934 | Train mAP: 0.3905 | Val Loss: 0.3688 | Val mAP: 0.5112 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.3722 | Train mAP: 0.4481 | Val Loss: 0.3520 | Val mAP: 0.5409 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.3719 | Train mAP: 0.4518 | Val Loss: 0.3467 | Val mAP: 0.5443 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.3682 | Train mAP: 0.4546 | Val Loss: 0.3664 | Val mAP: 0.5358 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.3683 | Train mAP: 0.4605 | Val Loss: 0.3600 | Val mAP: 0.5246 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.3629 | Train mAP: 0.4648 | Val Loss: 0.3616 | Val mAP: 0.5530 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.3587 | Train mAP: 0.4734 | Val Loss: 0.3435 | Val mAP: 0.5618 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.3522 | Train mAP: 0.5052 | Val Loss: 0.3514 | Val mAP: 0.5699 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.3510 | Train mAP: 0.4860 | Val Loss: 0.3586 | Val mAP: 0.5268 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.3472 | Train mAP: 0.5126 | Val Loss: 0.3397 | Val mAP: 0.5637 | LR: 0.001000 | Elapsed: 3.0s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.3385 | Train mAP: 0.5227 | Val Loss: 0.3486 | Val mAP: 0.5616 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.3448 | Train mAP: 0.5149 | Val Loss: 0.3416 | Val mAP: 0.5792 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.3328 | Train mAP: 0.5519 | Val Loss: 0.3395 | Val mAP: 0.5737 | LR: 0.001000 | Elapsed: 3.7s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.3472 | Train mAP: 0.5196 | Val Loss: 0.3490 | Val mAP: 0.5702 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.3374 | Train mAP: 0.5317 | Val Loss: 0.3631 | Val mAP: 0.5570 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.3329 | Train mAP: 0.5364 | Val Loss: 0.3506 | Val mAP: 0.5495 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.3316 | Train mAP: 0.5528 | Val Loss: 0.3379 | Val mAP: 0.5803 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.3254 | Train mAP: 0.5590 | Val Loss: 0.3470 | Val mAP: 0.5544 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.3207 | Train mAP: 0.5835 | Val Loss: 0.3434 | Val mAP: 0.5697 | LR: 0.001000 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.3176 | Train mAP: 0.5965 | Val Loss: 0.3555 | Val mAP: 0.5740 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.3239 | Train mAP: 0.5936 | Val Loss: 0.3446 | Val mAP: 0.5773 | LR: 0.001000 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.3210 | Train mAP: 0.5879 | Val Loss: 0.3475 | Val mAP: 0.5580 | LR: 0.001000 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.3218 | Train mAP: 0.5757 | Val Loss: 0.3523 | Val mAP: 0.5415 | LR: 0.001000 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.3152 | Train mAP: 0.6106 | Val Loss: 0.3378 | Val mAP: 0.5720 | LR: 0.000500 | Elapsed: 6.4s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.3103 | Train mAP: 0.5997 | Val Loss: 0.3460 | Val mAP: 0.5671 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.3048 | Train mAP: 0.6307 | Val Loss: 0.3420 | Val mAP: 0.5702 | LR: 0.000500 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.3132 | Train mAP: 0.5866 | Val Loss: 0.3474 | Val mAP: 0.5852 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.3037 | Train mAP: 0.6111 | Val Loss: 0.3419 | Val mAP: 0.5670 | LR: 0.000500 | Elapsed: 7.5s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.3037 | Train mAP: 0.6239 | Val Loss: 0.3504 | Val mAP: 0.5648 | LR: 0.000500 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.3022 | Train mAP: 0.6183 | Val Loss: 0.3495 | Val mAP: 0.5616 | LR: 0.000500 | Elapsed: 8.0s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.3008 | Train mAP: 0.6206 | Val Loss: 0.3488 | Val mAP: 0.5727 | LR: 0.000250 | Elapsed: 8.2s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.2912 | Train mAP: 0.6518 | Val Loss: 0.3526 | Val mAP: 0.5651 | LR: 0.000250 | Elapsed: 8.4s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.2957 | Train mAP: 0.6454 | Val Loss: 0.3494 | Val mAP: 0.5765 | LR: 0.000250 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.2970 | Train mAP: 0.6302 | Val Loss: 0.3461 | Val mAP: 0.5835 | LR: 0.000250 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.2895 | Train mAP: 0.6501 | Val Loss: 0.3436 | Val mAP: 0.5788 | LR: 0.000250 | Elapsed: 9.1s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.2820 | Train mAP: 0.6703 | Val Loss: 0.3463 | Val mAP: 0.5676 | LR: 0.000250 | Elapsed: 9.3s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.2820 | Train mAP: 0.6625 | Val Loss: 0.3454 | Val mAP: 0.5753 | LR: 0.000125 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.2827 | Train mAP: 0.6664 | Val Loss: 0.3481 | Val mAP: 0.5803 | LR: 0.000125 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.2904 | Train mAP: 0.6461 | Val Loss: 0.3447 | Val mAP: 0.5845 | LR: 0.000125 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.2819 | Train mAP: 0.6612 | Val Loss: 0.3449 | Val mAP: 0.5745 | LR: 0.000125 | Elapsed: 10.2s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.2843 | Train mAP: 0.6532 | Val Loss: 0.3462 | Val mAP: 0.5718 | LR: 0.000125 | Elapsed: 10.4s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.2753 | Train mAP: 0.6703 | Val Loss: 0.3473 | Val mAP: 0.5711 | LR: 0.000125 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.2787 | Train mAP: 0.6804 | Val Loss: 0.3491 | Val mAP: 0.5820 | LR: 0.000063 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.2813 | Train mAP: 0.6758 | Val Loss: 0.3456 | Val mAP: 0.5855 | LR: 0.000063 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.2659 | Train mAP: 0.7021 | Val Loss: 0.3433 | Val mAP: 0.5810 | LR: 0.000063 | Elapsed: 11.2s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.2929 | Train mAP: 0.6430 | Val Loss: 0.3450 | Val mAP: 0.5831 | LR: 0.000063 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.2927 | Train mAP: 0.6428 | Val Loss: 0.3413 | Val mAP: 0.5795 | LR: 0.000063 | Elapsed: 11.7s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.2729 | Train mAP: 0.6884 | Val Loss: 0.3446 | Val mAP: 0.5807 | LR: 0.000063 | Elapsed: 11.9s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.2829 | Train mAP: 0.6642 | Val Loss: 0.3421 | Val mAP: 0.5851 | LR: 0.000031 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.2749 | Train mAP: 0.6846 | Val Loss: 0.3440 | Val mAP: 0.5792 | LR: 0.000031 | Elapsed: 12.4s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.2772 | Train mAP: 0.6777 | Val Loss: 0.3454 | Val mAP: 0.5836 | LR: 0.000031 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.2750 | Train mAP: 0.6796 | Val Loss: 0.3459 | Val mAP: 0.5880 | LR: 0.000031 | Elapsed: 12.9s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.2840 | Train mAP: 0.6589 | Val Loss: 0.3444 | Val mAP: 0.5869 | LR: 0.000031 | Elapsed: 13.1s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.2784 | Train mAP: 0.6677 | Val Loss: 0.3438 | Val mAP: 0.5925 | LR: 0.000031 | Elapsed: 13.3s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.2807 | Train mAP: 0.6780 | Val Loss: 0.3450 | Val mAP: 0.5865 | LR: 0.000016 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.2758 | Train mAP: 0.6786 | Val Loss: 0.3452 | Val mAP: 0.5825 | LR: 0.000016 | Elapsed: 13.8s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.2677 | Train mAP: 0.6855 | Val Loss: 0.3448 | Val mAP: 0.5841 | LR: 0.000016 | Elapsed: 14.0s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.2678 | Train mAP: 0.6932 | Val Loss: 0.3445 | Val mAP: 0.5903 | LR: 0.000016 | Elapsed: 14.4s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.2737 | Train mAP: 0.6783 | Val Loss: 0.3441 | Val mAP: 0.5910 | LR: 0.000016 | Elapsed: 14.8s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.2746 | Train mAP: 0.6764 | Val Loss: 0.3419 | Val mAP: 0.5895 | LR: 0.000016 | Elapsed: 15.3s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.2739 | Train mAP: 0.6911 | Val Loss: 0.3434 | Val mAP: 0.5901 | LR: 0.000008 | Elapsed: 15.7s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.2776 | Train mAP: 0.6847 | Val Loss: 0.3412 | Val mAP: 0.5930 | LR: 0.000008 | Elapsed: 16.0s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.2763 | Train mAP: 0.6825 | Val Loss: 0.3451 | Val mAP: 0.5838 | LR: 0.000008 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.2813 | Train mAP: 0.6704 | Val Loss: 0.3484 | Val mAP: 0.5816 | LR: 0.000008 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.2834 | Train mAP: 0.6738 | Val Loss: 0.3455 | Val mAP: 0.5948 | LR: 0.000008 | Elapsed: 16.6s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.2723 | Train mAP: 0.6914 | Val Loss: 0.3419 | Val mAP: 0.5867 | LR: 0.000008 | Elapsed: 16.8s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.2671 | Train mAP: 0.6936 | Val Loss: 0.3443 | Val mAP: 0.5880 | LR: 0.000004 | Elapsed: 17.1s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.2795 | Train mAP: 0.6636 | Val Loss: 0.3464 | Val mAP: 0.5825 | LR: 0.000004 | Elapsed: 17.3s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.2730 | Train mAP: 0.6774 | Val Loss: 0.3423 | Val mAP: 0.5833 | LR: 0.000004 | Elapsed: 17.5s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.2725 | Train mAP: 0.6995 | Val Loss: 0.3472 | Val mAP: 0.5788 | LR: 0.000004 | Elapsed: 17.7s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.2694 | Train mAP: 0.6901 | Val Loss: 0.3471 | Val mAP: 0.5811 | LR: 0.000004 | Elapsed: 18.0s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.2750 | Train mAP: 0.6870 | Val Loss: 0.3433 | Val mAP: 0.5861 | LR: 0.000004 | Elapsed: 18.2s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.2746 | Train mAP: 0.6842 | Val Loss: 0.3436 | Val mAP: 0.5951 | LR: 0.000002 | Elapsed: 18.4s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.2771 | Train mAP: 0.6721 | Val Loss: 0.3444 | Val mAP: 0.5919 | LR: 0.000002 | Elapsed: 18.6s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.2707 | Train mAP: 0.6925 | Val Loss: 0.3409 | Val mAP: 0.5906 | LR: 0.000002 | Elapsed: 18.8s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.2681 | Train mAP: 0.6992 | Val Loss: 0.3455 | Val mAP: 0.5872 | LR: 0.000002 | Elapsed: 19.1s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.2803 | Train mAP: 0.6825 | Val Loss: 0.3414 | Val mAP: 0.5939 | LR: 0.000002 | Elapsed: 19.3s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.2685 | Train mAP: 0.6931 | Val Loss: 0.3410 | Val mAP: 0.5898 | LR: 0.000002 | Elapsed: 19.5s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.2697 | Train mAP: 0.6871 | Val Loss: 0.3455 | Val mAP: 0.5808 | LR: 0.000001 | Elapsed: 19.7s\n",
      "\n",
      "✅ Epoch 83/200 — Train Loss: 0.2736 | Train mAP: 0.6725 | Val Loss: 0.3408 | Val mAP: 0.5936 | LR: 0.000001 | Elapsed: 19.9s\n",
      "\n",
      "✅ Epoch 84/200 — Train Loss: 0.2742 | Train mAP: 0.6743 | Val Loss: 0.3440 | Val mAP: 0.5862 | LR: 0.000001 | Elapsed: 20.1s\n",
      "\n",
      "✅ Epoch 85/200 — Train Loss: 0.2678 | Train mAP: 0.6924 | Val Loss: 0.3459 | Val mAP: 0.5903 | LR: 0.000001 | Elapsed: 20.4s\n",
      "\n",
      "✅ Epoch 86/200 — Train Loss: 0.2845 | Train mAP: 0.6638 | Val Loss: 0.3411 | Val mAP: 0.5853 | LR: 0.000001 | Elapsed: 20.6s\n",
      "\n",
      "✅ Epoch 87/200 — Train Loss: 0.2745 | Train mAP: 0.6826 | Val Loss: 0.3464 | Val mAP: 0.5831 | LR: 0.000001 | Elapsed: 20.8s\n",
      "\n",
      "✅ Epoch 88/200 — Train Loss: 0.2705 | Train mAP: 0.6997 | Val Loss: 0.3469 | Val mAP: 0.5861 | LR: 0.000000 | Elapsed: 21.0s\n",
      "\n",
      "✅ Epoch 89/200 — Train Loss: 0.2728 | Train mAP: 0.6803 | Val Loss: 0.3462 | Val mAP: 0.5806 | LR: 0.000000 | Elapsed: 21.2s\n",
      "\n",
      "✅ Epoch 90/200 — Train Loss: 0.2722 | Train mAP: 0.6802 | Val Loss: 0.3445 | Val mAP: 0.5882 | LR: 0.000000 | Elapsed: 21.4s\n",
      "\n",
      "✅ Epoch 91/200 — Train Loss: 0.2730 | Train mAP: 0.6862 | Val Loss: 0.3457 | Val mAP: 0.5874 | LR: 0.000000 | Elapsed: 21.7s\n",
      "\n",
      "✅ Epoch 92/200 — Train Loss: 0.2759 | Train mAP: 0.6833 | Val Loss: 0.3442 | Val mAP: 0.5900 | LR: 0.000000 | Elapsed: 21.9s\n",
      "\n",
      "✅ Epoch 93/200 — Train Loss: 0.2729 | Train mAP: 0.6849 | Val Loss: 0.3455 | Val mAP: 0.5952 | LR: 0.000000 | Elapsed: 22.1s\n",
      "\n",
      "✅ Epoch 94/200 — Train Loss: 0.2676 | Train mAP: 0.6839 | Val Loss: 0.3405 | Val mAP: 0.5896 | LR: 0.000000 | Elapsed: 22.3s\n",
      "\n",
      "✅ Epoch 95/200 — Train Loss: 0.2677 | Train mAP: 0.6923 | Val Loss: 0.3424 | Val mAP: 0.5846 | LR: 0.000000 | Elapsed: 22.5s\n",
      "\n",
      "✅ Epoch 96/200 — Train Loss: 0.2749 | Train mAP: 0.6718 | Val Loss: 0.3437 | Val mAP: 0.5895 | LR: 0.000000 | Elapsed: 22.7s\n",
      "\n",
      "✅ Epoch 97/200 — Train Loss: 0.2742 | Train mAP: 0.6871 | Val Loss: 0.3466 | Val mAP: 0.5828 | LR: 0.000000 | Elapsed: 23.0s\n",
      "\n",
      "✅ Epoch 98/200 — Train Loss: 0.2776 | Train mAP: 0.6738 | Val Loss: 0.3432 | Val mAP: 0.5841 | LR: 0.000000 | Elapsed: 23.2s\n",
      "\n",
      "✅ Epoch 99/200 — Train Loss: 0.2637 | Train mAP: 0.7032 | Val Loss: 0.3430 | Val mAP: 0.5820 | LR: 0.000000 | Elapsed: 23.4s\n",
      "\n",
      "✅ Epoch 100/200 — Train Loss: 0.2758 | Train mAP: 0.6817 | Val Loss: 0.3428 | Val mAP: 0.5920 | LR: 0.000000 | Elapsed: 23.6s\n",
      "\n",
      "✅ Epoch 101/200 — Train Loss: 0.2681 | Train mAP: 0.6921 | Val Loss: 0.3452 | Val mAP: 0.5889 | LR: 0.000000 | Elapsed: 23.8s\n",
      "\n",
      "✅ Epoch 102/200 — Train Loss: 0.2781 | Train mAP: 0.6797 | Val Loss: 0.3431 | Val mAP: 0.5880 | LR: 0.000000 | Elapsed: 24.0s\n",
      "\n",
      "✅ Epoch 103/200 — Train Loss: 0.2721 | Train mAP: 0.6791 | Val Loss: 0.3443 | Val mAP: 0.5862 | LR: 0.000000 | Elapsed: 24.2s\n",
      "\n",
      "✅ Epoch 104/200 — Train Loss: 0.2789 | Train mAP: 0.6732 | Val Loss: 0.3414 | Val mAP: 0.5899 | LR: 0.000000 | Elapsed: 24.5s\n",
      "\n",
      "✅ Epoch 105/200 — Train Loss: 0.2743 | Train mAP: 0.6800 | Val Loss: 0.3404 | Val mAP: 0.5944 | LR: 0.000000 | Elapsed: 24.7s\n",
      "\n",
      "✅ Epoch 106/200 — Train Loss: 0.2742 | Train mAP: 0.6731 | Val Loss: 0.3443 | Val mAP: 0.5857 | LR: 0.000000 | Elapsed: 24.9s\n",
      "\n",
      "✅ Epoch 107/200 — Train Loss: 0.2827 | Train mAP: 0.6759 | Val Loss: 0.3450 | Val mAP: 0.5880 | LR: 0.000000 | Elapsed: 25.1s\n",
      "\n",
      "✅ Epoch 108/200 — Train Loss: 0.2656 | Train mAP: 0.6862 | Val Loss: 0.3456 | Val mAP: 0.5824 | LR: 0.000000 | Elapsed: 25.3s\n",
      "\n",
      "✅ Epoch 109/200 — Train Loss: 0.2685 | Train mAP: 0.6988 | Val Loss: 0.3459 | Val mAP: 0.5887 | LR: 0.000000 | Elapsed: 25.6s\n",
      "\n",
      "✅ Epoch 110/200 — Train Loss: 0.2767 | Train mAP: 0.6705 | Val Loss: 0.3446 | Val mAP: 0.5829 | LR: 0.000000 | Elapsed: 25.8s\n",
      "\n",
      "✅ Epoch 111/200 — Train Loss: 0.2712 | Train mAP: 0.6870 | Val Loss: 0.3452 | Val mAP: 0.5850 | LR: 0.000000 | Elapsed: 26.0s\n",
      "\n",
      "✅ Epoch 112/200 — Train Loss: 0.2685 | Train mAP: 0.6877 | Val Loss: 0.3441 | Val mAP: 0.5917 | LR: 0.000000 | Elapsed: 26.2s\n",
      "\n",
      "✅ Epoch 113/200 — Train Loss: 0.2770 | Train mAP: 0.6850 | Val Loss: 0.3415 | Val mAP: 0.5893 | LR: 0.000000 | Elapsed: 26.4s\n",
      "\n",
      "🛑 Early stopping at epoch 113. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.5952. Total time: 26.42s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.2835 | Train mAP: 0.0432 | Val Loss: 0.2162 | Val mAP: 0.0997 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1856 | Train mAP: 0.0591 | Val Loss: 0.2025 | Val mAP: 0.0736 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1770 | Train mAP: 0.0747 | Val Loss: 0.1877 | Val mAP: 0.1029 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1705 | Train mAP: 0.0869 | Val Loss: 0.1702 | Val mAP: 0.1164 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1733 | Train mAP: 0.0888 | Val Loss: 0.1818 | Val mAP: 0.1054 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1609 | Train mAP: 0.1074 | Val Loss: 0.1718 | Val mAP: 0.1382 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1664 | Train mAP: 0.0949 | Val Loss: 0.1666 | Val mAP: 0.1237 | LR: 0.001000 | Elapsed: 1.7s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1605 | Train mAP: 0.1258 | Val Loss: 0.1671 | Val mAP: 0.1268 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1551 | Train mAP: 0.1478 | Val Loss: 0.1591 | Val mAP: 0.1491 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1570 | Train mAP: 0.1447 | Val Loss: 0.1653 | Val mAP: 0.1302 | LR: 0.001000 | Elapsed: 2.5s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1519 | Train mAP: 0.1723 | Val Loss: 0.1623 | Val mAP: 0.1286 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1592 | Train mAP: 0.1231 | Val Loss: 0.1660 | Val mAP: 0.1210 | LR: 0.001000 | Elapsed: 3.0s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1548 | Train mAP: 0.1481 | Val Loss: 0.1646 | Val mAP: 0.1313 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1475 | Train mAP: 0.2004 | Val Loss: 0.1627 | Val mAP: 0.1228 | LR: 0.001000 | Elapsed: 3.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1392 | Train mAP: 0.2134 | Val Loss: 0.1741 | Val mAP: 0.1288 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1400 | Train mAP: 0.2116 | Val Loss: 0.1615 | Val mAP: 0.1431 | LR: 0.000500 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1384 | Train mAP: 0.2475 | Val Loss: 0.1631 | Val mAP: 0.1551 | LR: 0.000500 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1430 | Train mAP: 0.2197 | Val Loss: 0.1685 | Val mAP: 0.1346 | LR: 0.000500 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1362 | Train mAP: 0.2399 | Val Loss: 0.1650 | Val mAP: 0.1623 | LR: 0.000500 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1319 | Train mAP: 0.2641 | Val Loss: 0.1652 | Val mAP: 0.1672 | LR: 0.000500 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1337 | Train mAP: 0.2641 | Val Loss: 0.1654 | Val mAP: 0.1816 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1287 | Train mAP: 0.2778 | Val Loss: 0.1658 | Val mAP: 0.1742 | LR: 0.000250 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1286 | Train mAP: 0.3215 | Val Loss: 0.1605 | Val mAP: 0.1790 | LR: 0.000250 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1244 | Train mAP: 0.3246 | Val Loss: 0.1671 | Val mAP: 0.1725 | LR: 0.000250 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1323 | Train mAP: 0.2782 | Val Loss: 0.1601 | Val mAP: 0.1837 | LR: 0.000250 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1270 | Train mAP: 0.2876 | Val Loss: 0.1609 | Val mAP: 0.1749 | LR: 0.000250 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1249 | Train mAP: 0.3633 | Val Loss: 0.1644 | Val mAP: 0.1801 | LR: 0.000250 | Elapsed: 6.4s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1231 | Train mAP: 0.3301 | Val Loss: 0.1608 | Val mAP: 0.1819 | LR: 0.000125 | Elapsed: 6.6s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1255 | Train mAP: 0.3452 | Val Loss: 0.1603 | Val mAP: 0.1866 | LR: 0.000125 | Elapsed: 6.8s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1231 | Train mAP: 0.3519 | Val Loss: 0.1627 | Val mAP: 0.2021 | LR: 0.000125 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1221 | Train mAP: 0.3470 | Val Loss: 0.1622 | Val mAP: 0.2010 | LR: 0.000125 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1176 | Train mAP: 0.3691 | Val Loss: 0.1601 | Val mAP: 0.2024 | LR: 0.000125 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1162 | Train mAP: 0.3960 | Val Loss: 0.1606 | Val mAP: 0.2017 | LR: 0.000125 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1208 | Train mAP: 0.3828 | Val Loss: 0.1590 | Val mAP: 0.2033 | LR: 0.000063 | Elapsed: 7.9s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1286 | Train mAP: 0.3193 | Val Loss: 0.1593 | Val mAP: 0.2039 | LR: 0.000063 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1177 | Train mAP: 0.3615 | Val Loss: 0.1601 | Val mAP: 0.1964 | LR: 0.000063 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1205 | Train mAP: 0.3734 | Val Loss: 0.1626 | Val mAP: 0.1937 | LR: 0.000063 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1201 | Train mAP: 0.3652 | Val Loss: 0.1613 | Val mAP: 0.1978 | LR: 0.000063 | Elapsed: 8.8s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1197 | Train mAP: 0.3759 | Val Loss: 0.1591 | Val mAP: 0.1988 | LR: 0.000063 | Elapsed: 9.0s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1184 | Train mAP: 0.3730 | Val Loss: 0.1598 | Val mAP: 0.2098 | LR: 0.000063 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1210 | Train mAP: 0.3934 | Val Loss: 0.1587 | Val mAP: 0.1904 | LR: 0.000031 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1220 | Train mAP: 0.3741 | Val Loss: 0.1590 | Val mAP: 0.2055 | LR: 0.000031 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1156 | Train mAP: 0.3878 | Val Loss: 0.1607 | Val mAP: 0.1948 | LR: 0.000031 | Elapsed: 9.9s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1080 | Train mAP: 0.4756 | Val Loss: 0.1594 | Val mAP: 0.2055 | LR: 0.000031 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1164 | Train mAP: 0.3781 | Val Loss: 0.1599 | Val mAP: 0.1922 | LR: 0.000031 | Elapsed: 10.4s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1121 | Train mAP: 0.4213 | Val Loss: 0.1600 | Val mAP: 0.1900 | LR: 0.000031 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1183 | Train mAP: 0.3594 | Val Loss: 0.1601 | Val mAP: 0.1897 | LR: 0.000031 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1171 | Train mAP: 0.3938 | Val Loss: 0.1607 | Val mAP: 0.2104 | LR: 0.000016 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1147 | Train mAP: 0.4018 | Val Loss: 0.1590 | Val mAP: 0.1926 | LR: 0.000016 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1125 | Train mAP: 0.4258 | Val Loss: 0.1607 | Val mAP: 0.1958 | LR: 0.000016 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1179 | Train mAP: 0.4083 | Val Loss: 0.1602 | Val mAP: 0.1942 | LR: 0.000016 | Elapsed: 11.7s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1114 | Train mAP: 0.4542 | Val Loss: 0.1602 | Val mAP: 0.1893 | LR: 0.000016 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1172 | Train mAP: 0.4070 | Val Loss: 0.1618 | Val mAP: 0.1897 | LR: 0.000016 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1091 | Train mAP: 0.4544 | Val Loss: 0.1603 | Val mAP: 0.1968 | LR: 0.000008 | Elapsed: 12.4s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1161 | Train mAP: 0.3916 | Val Loss: 0.1591 | Val mAP: 0.1924 | LR: 0.000008 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1174 | Train mAP: 0.3826 | Val Loss: 0.1605 | Val mAP: 0.1951 | LR: 0.000008 | Elapsed: 12.9s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.1201 | Train mAP: 0.3868 | Val Loss: 0.1606 | Val mAP: 0.1921 | LR: 0.000008 | Elapsed: 13.2s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.1144 | Train mAP: 0.4145 | Val Loss: 0.1612 | Val mAP: 0.1930 | LR: 0.000008 | Elapsed: 13.4s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.1168 | Train mAP: 0.4173 | Val Loss: 0.1617 | Val mAP: 0.1937 | LR: 0.000008 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.1187 | Train mAP: 0.3446 | Val Loss: 0.1604 | Val mAP: 0.1911 | LR: 0.000004 | Elapsed: 13.8s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.1123 | Train mAP: 0.4173 | Val Loss: 0.1606 | Val mAP: 0.2136 | LR: 0.000004 | Elapsed: 14.1s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.1128 | Train mAP: 0.4137 | Val Loss: 0.1605 | Val mAP: 0.2141 | LR: 0.000004 | Elapsed: 14.3s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.1117 | Train mAP: 0.4197 | Val Loss: 0.1619 | Val mAP: 0.1932 | LR: 0.000004 | Elapsed: 14.5s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.1126 | Train mAP: 0.4395 | Val Loss: 0.1606 | Val mAP: 0.1950 | LR: 0.000004 | Elapsed: 14.8s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.1144 | Train mAP: 0.4191 | Val Loss: 0.1595 | Val mAP: 0.1939 | LR: 0.000004 | Elapsed: 15.0s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.1100 | Train mAP: 0.4426 | Val Loss: 0.1595 | Val mAP: 0.1969 | LR: 0.000002 | Elapsed: 15.3s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.1134 | Train mAP: 0.4178 | Val Loss: 0.1608 | Val mAP: 0.1948 | LR: 0.000002 | Elapsed: 15.5s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.1190 | Train mAP: 0.3893 | Val Loss: 0.1607 | Val mAP: 0.1918 | LR: 0.000002 | Elapsed: 15.8s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.1135 | Train mAP: 0.4035 | Val Loss: 0.1595 | Val mAP: 0.1968 | LR: 0.000002 | Elapsed: 16.0s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.1160 | Train mAP: 0.4046 | Val Loss: 0.1601 | Val mAP: 0.1935 | LR: 0.000002 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.1174 | Train mAP: 0.4121 | Val Loss: 0.1612 | Val mAP: 0.1933 | LR: 0.000002 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.1185 | Train mAP: 0.4010 | Val Loss: 0.1600 | Val mAP: 0.1959 | LR: 0.000001 | Elapsed: 16.7s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.1152 | Train mAP: 0.3905 | Val Loss: 0.1593 | Val mAP: 0.2074 | LR: 0.000001 | Elapsed: 16.9s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.1156 | Train mAP: 0.4075 | Val Loss: 0.1601 | Val mAP: 0.1930 | LR: 0.000001 | Elapsed: 17.1s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.1157 | Train mAP: 0.4180 | Val Loss: 0.1614 | Val mAP: 0.1985 | LR: 0.000001 | Elapsed: 17.3s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.1186 | Train mAP: 0.3799 | Val Loss: 0.1611 | Val mAP: 0.1950 | LR: 0.000001 | Elapsed: 17.6s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.1155 | Train mAP: 0.4336 | Val Loss: 0.1611 | Val mAP: 0.1953 | LR: 0.000001 | Elapsed: 17.8s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.1124 | Train mAP: 0.4439 | Val Loss: 0.1602 | Val mAP: 0.1944 | LR: 0.000000 | Elapsed: 18.0s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.1158 | Train mAP: 0.3932 | Val Loss: 0.1600 | Val mAP: 0.1976 | LR: 0.000000 | Elapsed: 18.2s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.1178 | Train mAP: 0.3928 | Val Loss: 0.1615 | Val mAP: 0.1948 | LR: 0.000000 | Elapsed: 18.5s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.1192 | Train mAP: 0.3755 | Val Loss: 0.1599 | Val mAP: 0.2049 | LR: 0.000000 | Elapsed: 18.7s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.1155 | Train mAP: 0.4153 | Val Loss: 0.1608 | Val mAP: 0.1925 | LR: 0.000000 | Elapsed: 18.9s\n",
      "\n",
      "🛑 Early stopping at epoch 82. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.2141. Total time: 18.91s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3748 | Train mAP: 0.0663 | Val Loss: 0.2897 | Val mAP: 0.1561 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.2324 | Train mAP: 0.1092 | Val Loss: 0.2312 | Val mAP: 0.2239 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.2301 | Train mAP: 0.1261 | Val Loss: 0.2180 | Val mAP: 0.2004 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.2198 | Train mAP: 0.1356 | Val Loss: 0.2189 | Val mAP: 0.2293 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.2095 | Train mAP: 0.1991 | Val Loss: 0.2076 | Val mAP: 0.2776 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.2042 | Train mAP: 0.2143 | Val Loss: 0.1908 | Val mAP: 0.2598 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1942 | Train mAP: 0.3042 | Val Loss: 0.1867 | Val mAP: 0.3268 | LR: 0.001000 | Elapsed: 1.5s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1927 | Train mAP: 0.2928 | Val Loss: 0.1820 | Val mAP: 0.3227 | LR: 0.001000 | Elapsed: 1.8s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1925 | Train mAP: 0.3055 | Val Loss: 0.1860 | Val mAP: 0.3217 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1874 | Train mAP: 0.3351 | Val Loss: 0.1937 | Val mAP: 0.2886 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1873 | Train mAP: 0.3278 | Val Loss: 0.1859 | Val mAP: 0.3168 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1811 | Train mAP: 0.3673 | Val Loss: 0.1881 | Val mAP: 0.3230 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1800 | Train mAP: 0.3838 | Val Loss: 0.1805 | Val mAP: 0.4069 | LR: 0.001000 | Elapsed: 2.8s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1718 | Train mAP: 0.4163 | Val Loss: 0.1822 | Val mAP: 0.3507 | LR: 0.001000 | Elapsed: 3.1s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1773 | Train mAP: 0.3935 | Val Loss: 0.1830 | Val mAP: 0.3492 | LR: 0.001000 | Elapsed: 3.3s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1744 | Train mAP: 0.4006 | Val Loss: 0.1854 | Val mAP: 0.3732 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1680 | Train mAP: 0.4418 | Val Loss: 0.1744 | Val mAP: 0.3879 | LR: 0.001000 | Elapsed: 3.7s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1695 | Train mAP: 0.4132 | Val Loss: 0.1774 | Val mAP: 0.3709 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1630 | Train mAP: 0.4536 | Val Loss: 0.1743 | Val mAP: 0.3724 | LR: 0.001000 | Elapsed: 4.2s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1675 | Train mAP: 0.4594 | Val Loss: 0.1725 | Val mAP: 0.3923 | LR: 0.001000 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1649 | Train mAP: 0.4695 | Val Loss: 0.1761 | Val mAP: 0.3827 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1551 | Train mAP: 0.5258 | Val Loss: 0.1729 | Val mAP: 0.4216 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1547 | Train mAP: 0.5062 | Val Loss: 0.1743 | Val mAP: 0.3871 | LR: 0.001000 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1639 | Train mAP: 0.4697 | Val Loss: 0.1739 | Val mAP: 0.3991 | LR: 0.001000 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1615 | Train mAP: 0.4787 | Val Loss: 0.1847 | Val mAP: 0.3546 | LR: 0.001000 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1514 | Train mAP: 0.5210 | Val Loss: 0.1818 | Val mAP: 0.3612 | LR: 0.001000 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1538 | Train mAP: 0.5222 | Val Loss: 0.1739 | Val mAP: 0.4008 | LR: 0.000500 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1468 | Train mAP: 0.5466 | Val Loss: 0.1749 | Val mAP: 0.4000 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1437 | Train mAP: 0.5722 | Val Loss: 0.1720 | Val mAP: 0.4196 | LR: 0.000500 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1426 | Train mAP: 0.5640 | Val Loss: 0.1748 | Val mAP: 0.4060 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1362 | Train mAP: 0.5955 | Val Loss: 0.1743 | Val mAP: 0.4091 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1423 | Train mAP: 0.5730 | Val Loss: 0.1737 | Val mAP: 0.4155 | LR: 0.000500 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1382 | Train mAP: 0.5980 | Val Loss: 0.1794 | Val mAP: 0.3950 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1417 | Train mAP: 0.5778 | Val Loss: 0.1719 | Val mAP: 0.4238 | LR: 0.000500 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1426 | Train mAP: 0.5746 | Val Loss: 0.1780 | Val mAP: 0.3947 | LR: 0.000500 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1408 | Train mAP: 0.5792 | Val Loss: 0.1707 | Val mAP: 0.4262 | LR: 0.000500 | Elapsed: 7.9s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1408 | Train mAP: 0.5768 | Val Loss: 0.1767 | Val mAP: 0.4066 | LR: 0.000500 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1373 | Train mAP: 0.6015 | Val Loss: 0.1727 | Val mAP: 0.4187 | LR: 0.000500 | Elapsed: 8.4s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1364 | Train mAP: 0.6006 | Val Loss: 0.1818 | Val mAP: 0.3938 | LR: 0.000500 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1271 | Train mAP: 0.6433 | Val Loss: 0.1846 | Val mAP: 0.3776 | LR: 0.000500 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1341 | Train mAP: 0.6055 | Val Loss: 0.1811 | Val mAP: 0.4188 | LR: 0.000500 | Elapsed: 9.1s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1344 | Train mAP: 0.6029 | Val Loss: 0.1774 | Val mAP: 0.4205 | LR: 0.000500 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.1268 | Train mAP: 0.6423 | Val Loss: 0.1768 | Val mAP: 0.4138 | LR: 0.000250 | Elapsed: 9.6s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.1253 | Train mAP: 0.6500 | Val Loss: 0.1796 | Val mAP: 0.4020 | LR: 0.000250 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.1285 | Train mAP: 0.6335 | Val Loss: 0.1817 | Val mAP: 0.3951 | LR: 0.000250 | Elapsed: 10.1s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.1334 | Train mAP: 0.6200 | Val Loss: 0.1835 | Val mAP: 0.4059 | LR: 0.000250 | Elapsed: 10.3s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.1289 | Train mAP: 0.6327 | Val Loss: 0.1768 | Val mAP: 0.4074 | LR: 0.000250 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.1215 | Train mAP: 0.6645 | Val Loss: 0.1810 | Val mAP: 0.4059 | LR: 0.000250 | Elapsed: 10.7s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.1216 | Train mAP: 0.6706 | Val Loss: 0.1812 | Val mAP: 0.4157 | LR: 0.000125 | Elapsed: 10.9s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.1181 | Train mAP: 0.6816 | Val Loss: 0.1823 | Val mAP: 0.4148 | LR: 0.000125 | Elapsed: 11.1s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.1247 | Train mAP: 0.6512 | Val Loss: 0.1759 | Val mAP: 0.4180 | LR: 0.000125 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.1209 | Train mAP: 0.6737 | Val Loss: 0.1770 | Val mAP: 0.4222 | LR: 0.000125 | Elapsed: 11.6s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.1247 | Train mAP: 0.6508 | Val Loss: 0.1808 | Val mAP: 0.4140 | LR: 0.000125 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.1187 | Train mAP: 0.6740 | Val Loss: 0.1803 | Val mAP: 0.4194 | LR: 0.000125 | Elapsed: 12.0s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.1171 | Train mAP: 0.6890 | Val Loss: 0.1797 | Val mAP: 0.4247 | LR: 0.000063 | Elapsed: 12.2s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.1158 | Train mAP: 0.6944 | Val Loss: 0.1831 | Val mAP: 0.4117 | LR: 0.000063 | Elapsed: 12.4s\n",
      "\n",
      "🛑 Early stopping at epoch 56. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.4262. Total time: 12.41s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3046 | Train mAP: 0.0551 | Val Loss: 0.2539 | Val mAP: 0.1435 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1896 | Train mAP: 0.0883 | Val Loss: 0.1815 | Val mAP: 0.2246 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1850 | Train mAP: 0.0733 | Val Loss: 0.1883 | Val mAP: 0.2275 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1796 | Train mAP: 0.0939 | Val Loss: 0.1647 | Val mAP: 0.2445 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1707 | Train mAP: 0.1174 | Val Loss: 0.1572 | Val mAP: 0.2671 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1722 | Train mAP: 0.1203 | Val Loss: 0.1627 | Val mAP: 0.2945 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1617 | Train mAP: 0.1537 | Val Loss: 0.1551 | Val mAP: 0.2241 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1514 | Train mAP: 0.2118 | Val Loss: 0.1550 | Val mAP: 0.2105 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1555 | Train mAP: 0.1972 | Val Loss: 0.1583 | Val mAP: 0.2059 | LR: 0.001000 | Elapsed: 2.1s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1566 | Train mAP: 0.1947 | Val Loss: 0.1521 | Val mAP: 0.2393 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1491 | Train mAP: 0.2258 | Val Loss: 0.1550 | Val mAP: 0.2640 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1518 | Train mAP: 0.2316 | Val Loss: 0.1526 | Val mAP: 0.2675 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1503 | Train mAP: 0.2476 | Val Loss: 0.1496 | Val mAP: 0.2716 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1485 | Train mAP: 0.2359 | Val Loss: 0.1537 | Val mAP: 0.2190 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1428 | Train mAP: 0.2695 | Val Loss: 0.1561 | Val mAP: 0.1996 | LR: 0.001000 | Elapsed: 3.8s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1440 | Train mAP: 0.2620 | Val Loss: 0.1627 | Val mAP: 0.1766 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1457 | Train mAP: 0.2805 | Val Loss: 0.1502 | Val mAP: 0.2456 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1455 | Train mAP: 0.2899 | Val Loss: 0.1521 | Val mAP: 0.2366 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1410 | Train mAP: 0.3018 | Val Loss: 0.1584 | Val mAP: 0.2020 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1365 | Train mAP: 0.3443 | Val Loss: 0.1513 | Val mAP: 0.2447 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1384 | Train mAP: 0.3449 | Val Loss: 0.1461 | Val mAP: 0.2539 | LR: 0.000500 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1296 | Train mAP: 0.3667 | Val Loss: 0.1527 | Val mAP: 0.2240 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1306 | Train mAP: 0.3794 | Val Loss: 0.1544 | Val mAP: 0.2290 | LR: 0.000500 | Elapsed: 5.7s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1318 | Train mAP: 0.3926 | Val Loss: 0.1621 | Val mAP: 0.2029 | LR: 0.000500 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1242 | Train mAP: 0.4161 | Val Loss: 0.1600 | Val mAP: 0.2278 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1307 | Train mAP: 0.3876 | Val Loss: 0.1658 | Val mAP: 0.2131 | LR: 0.000500 | Elapsed: 6.4s\n",
      "\n",
      "🛑 Early stopping at epoch 26. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.2945. Total time: 6.38s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.2310 | Train mAP: 0.0133 | Val Loss: 0.1504 | Val mAP: 0.0307 | LR: 0.001000 | Elapsed: 0.3s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.0802 | Train mAP: 0.0119 | Val Loss: 0.0898 | Val mAP: 0.0210 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.0736 | Train mAP: 0.0125 | Val Loss: 0.0811 | Val mAP: 0.1292 | LR: 0.001000 | Elapsed: 0.8s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.0734 | Train mAP: 0.0108 | Val Loss: 0.0722 | Val mAP: 0.0329 | LR: 0.001000 | Elapsed: 1.0s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.0679 | Train mAP: 0.0186 | Val Loss: 0.0682 | Val mAP: 0.0204 | LR: 0.001000 | Elapsed: 1.3s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.0638 | Train mAP: 0.0255 | Val Loss: 0.0753 | Val mAP: 0.0185 | LR: 0.001000 | Elapsed: 1.5s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.0635 | Train mAP: 0.0214 | Val Loss: 0.0618 | Val mAP: 0.0244 | LR: 0.001000 | Elapsed: 1.8s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.0645 | Train mAP: 0.0293 | Val Loss: 0.0611 | Val mAP: 0.0236 | LR: 0.001000 | Elapsed: 2.0s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.0666 | Train mAP: 0.0236 | Val Loss: 0.0633 | Val mAP: 0.0208 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.0614 | Train mAP: 0.0232 | Val Loss: 0.0652 | Val mAP: 0.0183 | LR: 0.001000 | Elapsed: 2.5s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.0594 | Train mAP: 0.0412 | Val Loss: 0.0593 | Val mAP: 0.0276 | LR: 0.001000 | Elapsed: 2.7s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.0568 | Train mAP: 0.0597 | Val Loss: 0.0702 | Val mAP: 0.0208 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.0569 | Train mAP: 0.0559 | Val Loss: 0.0613 | Val mAP: 0.0323 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.0593 | Train mAP: 0.0570 | Val Loss: 0.0618 | Val mAP: 0.0279 | LR: 0.001000 | Elapsed: 3.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.0518 | Train mAP: 0.0990 | Val Loss: 0.0631 | Val mAP: 0.0316 | LR: 0.001000 | Elapsed: 3.7s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.0578 | Train mAP: 0.0568 | Val Loss: 0.0621 | Val mAP: 0.0382 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.0532 | Train mAP: 0.1247 | Val Loss: 0.0609 | Val mAP: 0.0399 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.0513 | Train mAP: 0.0818 | Val Loss: 0.0646 | Val mAP: 0.0304 | LR: 0.000500 | Elapsed: 4.4s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.0484 | Train mAP: 0.1948 | Val Loss: 0.0656 | Val mAP: 0.0374 | LR: 0.000500 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.0489 | Train mAP: 0.1699 | Val Loss: 0.0652 | Val mAP: 0.0308 | LR: 0.000500 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.0552 | Train mAP: 0.1408 | Val Loss: 0.0656 | Val mAP: 0.0392 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.0517 | Train mAP: 0.1828 | Val Loss: 0.0651 | Val mAP: 0.0366 | LR: 0.000500 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.0479 | Train mAP: 0.2226 | Val Loss: 0.0644 | Val mAP: 0.0402 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "🛑 Early stopping at epoch 23. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.1292. Total time: 5.46s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3146 | Train mAP: 0.7444 | Val Loss: 0.1815 | Val mAP: 0.8617 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.1803 | Train mAP: 0.8205 | Val Loss: 0.1531 | Val mAP: 0.8513 | LR: 0.001000 | Elapsed: 0.4s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.1590 | Train mAP: 0.8468 | Val Loss: 0.1641 | Val mAP: 0.8324 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.1508 | Train mAP: 0.8592 | Val Loss: 0.1671 | Val mAP: 0.8182 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.1482 | Train mAP: 0.8630 | Val Loss: 0.1661 | Val mAP: 0.7987 | LR: 0.001000 | Elapsed: 1.1s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.1413 | Train mAP: 0.8752 | Val Loss: 0.1495 | Val mAP: 0.8452 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.1310 | Train mAP: 0.8851 | Val Loss: 0.1422 | Val mAP: 0.8572 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.1328 | Train mAP: 0.8846 | Val Loss: 0.1468 | Val mAP: 0.8422 | LR: 0.001000 | Elapsed: 1.9s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.1362 | Train mAP: 0.8782 | Val Loss: 0.1432 | Val mAP: 0.8605 | LR: 0.001000 | Elapsed: 2.1s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.1301 | Train mAP: 0.8895 | Val Loss: 0.1437 | Val mAP: 0.8685 | LR: 0.001000 | Elapsed: 2.4s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1212 | Train mAP: 0.8957 | Val Loss: 0.1392 | Val mAP: 0.8634 | LR: 0.001000 | Elapsed: 2.6s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1267 | Train mAP: 0.8925 | Val Loss: 0.1430 | Val mAP: 0.8506 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1216 | Train mAP: 0.8970 | Val Loss: 0.1418 | Val mAP: 0.8695 | LR: 0.001000 | Elapsed: 3.2s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1229 | Train mAP: 0.8963 | Val Loss: 0.1347 | Val mAP: 0.8756 | LR: 0.001000 | Elapsed: 3.4s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1181 | Train mAP: 0.9019 | Val Loss: 0.1563 | Val mAP: 0.8188 | LR: 0.001000 | Elapsed: 3.6s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1185 | Train mAP: 0.9006 | Val Loss: 0.1372 | Val mAP: 0.8678 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1166 | Train mAP: 0.9039 | Val Loss: 0.1511 | Val mAP: 0.8210 | LR: 0.001000 | Elapsed: 4.1s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1216 | Train mAP: 0.9002 | Val Loss: 0.1679 | Val mAP: 0.8355 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1207 | Train mAP: 0.8978 | Val Loss: 0.1636 | Val mAP: 0.8159 | LR: 0.001000 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1176 | Train mAP: 0.9012 | Val Loss: 0.1365 | Val mAP: 0.8745 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1127 | Train mAP: 0.9070 | Val Loss: 0.1293 | Val mAP: 0.8823 | LR: 0.000500 | Elapsed: 5.0s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1084 | Train mAP: 0.9095 | Val Loss: 0.1341 | Val mAP: 0.8769 | LR: 0.000500 | Elapsed: 5.2s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.0986 | Train mAP: 0.9226 | Val Loss: 0.1396 | Val mAP: 0.8672 | LR: 0.000500 | Elapsed: 5.4s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1005 | Train mAP: 0.9186 | Val Loss: 0.1440 | Val mAP: 0.8465 | LR: 0.000500 | Elapsed: 5.6s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1000 | Train mAP: 0.9224 | Val Loss: 0.1366 | Val mAP: 0.8731 | LR: 0.000500 | Elapsed: 5.9s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1047 | Train mAP: 0.9152 | Val Loss: 0.1367 | Val mAP: 0.8797 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1042 | Train mAP: 0.9138 | Val Loss: 0.1379 | Val mAP: 0.8780 | LR: 0.000500 | Elapsed: 6.3s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1005 | Train mAP: 0.9207 | Val Loss: 0.1370 | Val mAP: 0.8788 | LR: 0.000250 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.0891 | Train mAP: 0.9311 | Val Loss: 0.1360 | Val mAP: 0.8769 | LR: 0.000250 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1062 | Train mAP: 0.9130 | Val Loss: 0.1368 | Val mAP: 0.8589 | LR: 0.000250 | Elapsed: 7.0s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1031 | Train mAP: 0.9147 | Val Loss: 0.1340 | Val mAP: 0.8811 | LR: 0.000250 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.0876 | Train mAP: 0.9378 | Val Loss: 0.1355 | Val mAP: 0.8815 | LR: 0.000250 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.0997 | Train mAP: 0.9188 | Val Loss: 0.1445 | Val mAP: 0.8671 | LR: 0.000250 | Elapsed: 7.7s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.0991 | Train mAP: 0.9190 | Val Loss: 0.1368 | Val mAP: 0.8730 | LR: 0.000125 | Elapsed: 8.0s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.0949 | Train mAP: 0.9276 | Val Loss: 0.1385 | Val mAP: 0.8643 | LR: 0.000125 | Elapsed: 8.2s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.0908 | Train mAP: 0.9301 | Val Loss: 0.1404 | Val mAP: 0.8609 | LR: 0.000125 | Elapsed: 8.5s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.0965 | Train mAP: 0.9235 | Val Loss: 0.1455 | Val mAP: 0.8437 | LR: 0.000125 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.0894 | Train mAP: 0.9300 | Val Loss: 0.1376 | Val mAP: 0.8566 | LR: 0.000125 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.0912 | Train mAP: 0.9276 | Val Loss: 0.1433 | Val mAP: 0.8584 | LR: 0.000125 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.0938 | Train mAP: 0.9277 | Val Loss: 0.1379 | Val mAP: 0.8673 | LR: 0.000063 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.0917 | Train mAP: 0.9298 | Val Loss: 0.1431 | Val mAP: 0.8627 | LR: 0.000063 | Elapsed: 9.6s\n",
      "\n",
      "🛑 Early stopping at epoch 41. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.8823. Total time: 9.61s\n",
      "\n",
      "🖥️  Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/813875559.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.3272 | Train mAP: 0.0874 | Val Loss: 0.2675 | Val mAP: 0.2120 | LR: 0.001000 | Elapsed: 0.2s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.2423 | Train mAP: 0.1124 | Val Loss: 0.2251 | Val mAP: 0.2686 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.2297 | Train mAP: 0.1520 | Val Loss: 0.2079 | Val mAP: 0.2547 | LR: 0.001000 | Elapsed: 0.7s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.2291 | Train mAP: 0.1674 | Val Loss: 0.2121 | Val mAP: 0.2492 | LR: 0.001000 | Elapsed: 0.9s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.2194 | Train mAP: 0.1932 | Val Loss: 0.2070 | Val mAP: 0.2698 | LR: 0.001000 | Elapsed: 1.2s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.2137 | Train mAP: 0.2115 | Val Loss: 0.2001 | Val mAP: 0.2514 | LR: 0.001000 | Elapsed: 1.4s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.2120 | Train mAP: 0.2313 | Val Loss: 0.2032 | Val mAP: 0.2995 | LR: 0.001000 | Elapsed: 1.7s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.2036 | Train mAP: 0.2536 | Val Loss: 0.1963 | Val mAP: 0.3114 | LR: 0.001000 | Elapsed: 2.2s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.2033 | Train mAP: 0.2749 | Val Loss: 0.1986 | Val mAP: 0.2805 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.2003 | Train mAP: 0.2939 | Val Loss: 0.1986 | Val mAP: 0.2807 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.1947 | Train mAP: 0.3224 | Val Loss: 0.2016 | Val mAP: 0.2816 | LR: 0.001000 | Elapsed: 4.5s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.1948 | Train mAP: 0.3169 | Val Loss: 0.2021 | Val mAP: 0.2908 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.1942 | Train mAP: 0.3275 | Val Loss: 0.1986 | Val mAP: 0.3011 | LR: 0.001000 | Elapsed: 5.1s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.1935 | Train mAP: 0.3383 | Val Loss: 0.2032 | Val mAP: 0.2969 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.1900 | Train mAP: 0.3618 | Val Loss: 0.2012 | Val mAP: 0.2940 | LR: 0.000500 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.1890 | Train mAP: 0.3504 | Val Loss: 0.2016 | Val mAP: 0.3280 | LR: 0.000500 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.1794 | Train mAP: 0.3948 | Val Loss: 0.2010 | Val mAP: 0.3325 | LR: 0.000500 | Elapsed: 6.0s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.1799 | Train mAP: 0.4204 | Val Loss: 0.1999 | Val mAP: 0.3305 | LR: 0.000500 | Elapsed: 6.2s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.1765 | Train mAP: 0.4324 | Val Loss: 0.2019 | Val mAP: 0.3198 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.1752 | Train mAP: 0.4532 | Val Loss: 0.1971 | Val mAP: 0.3296 | LR: 0.000500 | Elapsed: 6.7s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.1687 | Train mAP: 0.4811 | Val Loss: 0.1974 | Val mAP: 0.3412 | LR: 0.000250 | Elapsed: 6.9s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.1693 | Train mAP: 0.4648 | Val Loss: 0.1978 | Val mAP: 0.3523 | LR: 0.000250 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.1678 | Train mAP: 0.4969 | Val Loss: 0.1997 | Val mAP: 0.3265 | LR: 0.000250 | Elapsed: 7.4s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.1687 | Train mAP: 0.4893 | Val Loss: 0.1997 | Val mAP: 0.3372 | LR: 0.000250 | Elapsed: 7.6s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.1688 | Train mAP: 0.4890 | Val Loss: 0.1982 | Val mAP: 0.3406 | LR: 0.000250 | Elapsed: 7.8s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.1731 | Train mAP: 0.4567 | Val Loss: 0.2002 | Val mAP: 0.3277 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.1590 | Train mAP: 0.5193 | Val Loss: 0.2013 | Val mAP: 0.3283 | LR: 0.000125 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.1589 | Train mAP: 0.5169 | Val Loss: 0.2007 | Val mAP: 0.3369 | LR: 0.000125 | Elapsed: 8.5s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.1560 | Train mAP: 0.5291 | Val Loss: 0.2010 | Val mAP: 0.3300 | LR: 0.000125 | Elapsed: 8.7s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.1628 | Train mAP: 0.4923 | Val Loss: 0.2049 | Val mAP: 0.3253 | LR: 0.000125 | Elapsed: 8.9s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.1593 | Train mAP: 0.5292 | Val Loss: 0.1997 | Val mAP: 0.3229 | LR: 0.000125 | Elapsed: 9.2s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.1583 | Train mAP: 0.5295 | Val Loss: 0.2052 | Val mAP: 0.3150 | LR: 0.000125 | Elapsed: 9.4s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.1553 | Train mAP: 0.5425 | Val Loss: 0.2078 | Val mAP: 0.3305 | LR: 0.000063 | Elapsed: 9.6s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.1563 | Train mAP: 0.5382 | Val Loss: 0.2074 | Val mAP: 0.3159 | LR: 0.000063 | Elapsed: 9.8s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.1542 | Train mAP: 0.5521 | Val Loss: 0.2032 | Val mAP: 0.3327 | LR: 0.000063 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.1528 | Train mAP: 0.5624 | Val Loss: 0.2005 | Val mAP: 0.3457 | LR: 0.000063 | Elapsed: 10.2s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.1533 | Train mAP: 0.5524 | Val Loss: 0.2050 | Val mAP: 0.3296 | LR: 0.000063 | Elapsed: 10.4s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.1541 | Train mAP: 0.5605 | Val Loss: 0.2033 | Val mAP: 0.3195 | LR: 0.000063 | Elapsed: 10.6s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.1536 | Train mAP: 0.5497 | Val Loss: 0.2053 | Val mAP: 0.3410 | LR: 0.000031 | Elapsed: 10.8s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.1458 | Train mAP: 0.5747 | Val Loss: 0.2068 | Val mAP: 0.3235 | LR: 0.000031 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.1521 | Train mAP: 0.5511 | Val Loss: 0.2046 | Val mAP: 0.3384 | LR: 0.000031 | Elapsed: 11.3s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.1499 | Train mAP: 0.5724 | Val Loss: 0.2037 | Val mAP: 0.3287 | LR: 0.000031 | Elapsed: 11.5s\n",
      "\n",
      "🛑 Early stopping at epoch 42. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.3523. Total time: 11.52s\n"
     ]
    },
    {
     "data": {
      "text/plain": "MultiOutputClassifier(estimator=SklearnMLPClassifier(epochs=200,\n                                                     input_dim=2244))",
      "text/html": "<style>#sk-container-id-6 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-6 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-6 pre {\n  padding: 0;\n}\n\n#sk-container-id-6 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-6 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-6 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-6 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-6 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-6 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-6 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-6 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-6 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-6 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-6 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-6 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-6 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-6 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-6 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-6 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-6 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n#sk-container-id-6 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-6 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-6 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-6 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-6 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-6 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-6 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-6 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-6 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-6 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=SklearnMLPClassifier(epochs=200,\n                                                     input_dim=2244))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultiOutputClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=SklearnMLPClassifier(epochs=200,\n                                                     input_dim=2244))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: SklearnMLPClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SklearnMLPClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244)</pre></div> </div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "base_clf = SklearnMLPClassifier(input_dim=feature_size, n_classes=1, epochs=200, verbose=True)\n",
    "multi_clf = MultiOutputClassifier(base_clf)\n",
    "\n",
    "multi_clf.fit(X_data, y_data)\n",
    "# y_pred = multi_clf.predict(X_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:15:10.327673Z",
     "start_time": "2025-05-20T21:12:38.436197Z"
    }
   },
   "id": "2466cb4a137a317b",
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def create_test_features(path, n_jobs=4):\n",
    "    # Load test data\n",
    "    with open(path, 'r') as f:\n",
    "        test_json = eval(f.read())  # or use json.load(f) if it's valid JSON\n",
    "\n",
    "    keys = list(test_json)\n",
    "\n",
    "    # Parallel feature extraction\n",
    "    X = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(features)(key) for key in keys\n",
    "    )\n",
    "    \n",
    "    # Convert to tensor format\n",
    "    X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
    "\n",
    "    return keys, X\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:20:53.348193Z",
     "start_time": "2025-05-20T21:20:53.326818Z"
    }
   },
   "id": "2549d582405c8936",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m keys, X \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_test_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataroot3\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/test.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m multi_clf\u001B[38;5;241m.\u001B[39mpredict(X\u001B[38;5;241m.\u001B[39mnumpy())  \u001B[38;5;66;03m# MultiOutputClassifier expects NumPy\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# add code to print to predictions3.json. format should be key:prediction converted back using an index to tag \u001B[39;00m\n",
      "Cell \u001B[0;32mIn[111], line 13\u001B[0m, in \u001B[0;36mcreate_test_features\u001B[0;34m(path, n_jobs)\u001B[0m\n\u001B[1;32m     10\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(test_json)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Parallel feature extraction\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Convert to tensor format\u001B[39;00m\n\u001B[1;32m     18\u001B[0m X \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack([torch\u001B[38;5;241m.\u001B[39mtensor(x, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m X])\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/joblib/parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/joblib/parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/joblib/parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "keys, X = create_test_features(dataroot3+\"/test.json\", n_jobs=4)\n",
    "y_pred = multi_clf.predict(X.numpy())  # MultiOutputClassifier expects NumPy\n",
    "\n",
    "# add code to print to predictions3.json. format should be key:prediction converted back using an index to tag "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:21:04.018375Z",
     "start_time": "2025-05-20T21:20:53.778689Z"
    }
   },
   "id": "6c5ddac9afa11896",
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1000, 10)"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:17:21.894868Z",
     "start_time": "2025-05-20T21:17:21.888075Z"
    }
   },
   "id": "ed5d45b049ecedd7",
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions written to predictions3.json\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-hot predictions to tag lists\n",
    "predictions = {}\n",
    "for key, pred_vector in zip(keys, y_pred):\n",
    "    tag_list = [TAGS[i] for i, val in enumerate(pred_vector) if val == 1]\n",
    "    predictions[key] = tag_list\n",
    "\n",
    "# Save predictions to predictions3.json\n",
    "output_path = \"predictions3.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(f\"✅ Predictions written to {output_path}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:17:21.906607Z",
     "start_time": "2025-05-20T21:17:21.891566Z"
    }
   },
   "id": "6ad8599236a7fadf",
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "source": [
    "LGBM classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f41c60c9cd955f0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1765, number of negative: 1835\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.490278 -> initscore=-0.038894\n",
      "[LightGBM] [Info] Start training from score -0.038894\n",
      "[LightGBM] [Info] Number of positive: 147, number of negative: 3453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040833 -> initscore=-3.156566\n",
      "[LightGBM] [Info] Start training from score -3.156566\n",
      "[LightGBM] [Info] Number of positive: 351, number of negative: 3249\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097500 -> initscore=-2.225316\n",
      "[LightGBM] [Info] Start training from score -2.225316\n",
      "[LightGBM] [Info] Number of positive: 617, number of negative: 2983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171389 -> initscore=-1.575816\n",
      "[LightGBM] [Info] Start training from score -1.575816\n",
      "[LightGBM] [Info] Number of positive: 152, number of negative: 3448\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042222 -> initscore=-3.121669\n",
      "[LightGBM] [Info] Start training from score -3.121669\n",
      "[LightGBM] [Info] Number of positive: 220, number of negative: 3380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061111 -> initscore=-2.732003\n",
      "[LightGBM] [Info] Start training from score -2.732003\n",
      "[LightGBM] [Info] Number of positive: 162, number of negative: 3438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.045000 -> initscore=-3.055049\n",
      "[LightGBM] [Info] Start training from score -3.055049\n",
      "[LightGBM] [Info] Number of positive: 40, number of negative: 3560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011111 -> initscore=-4.488636\n",
      "[LightGBM] [Info] Start training from score -4.488636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 476, number of negative: 3124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.132222 -> initscore=-1.881452\n",
      "[LightGBM] [Info] Start training from score -1.881452\n",
      "[LightGBM] [Info] Number of positive: 235, number of negative: 3365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 572220\n",
      "[LightGBM] [Info] Number of data points in the train set: 3600, number of used features: 2244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.065278 -> initscore=-2.661598\n",
      "[LightGBM] [Info] Start training from score -2.661598\n"
     ]
    },
    {
     "data": {
      "text/plain": "MultiOutputClassifier(estimator=LGBMClassifier(objective='binary'))",
      "text/html": "<style>#sk-container-id-11 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-11 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-11 pre {\n  padding: 0;\n}\n\n#sk-container-id-11 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-11 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-11 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-11 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-11 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-11 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-11 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-11 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-11 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-11 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-11 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-11 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-11 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-11 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-11 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-11 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-11 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-11 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-11 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-11 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-11 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-11 div.sk-label label.sk-toggleable__label,\n#sk-container-id-11 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-11 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-11 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-11 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-11 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-11 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-11 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-11 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-11 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-11 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-11 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-11 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=LGBMClassifier(objective=&#x27;binary&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultiOutputClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=LGBMClassifier(objective=&#x27;binary&#x27;))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: LGBMClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Wrap LightGBM in MultiOutputClassifier\n",
    "\n",
    "base_model = LGBMClassifier(\n",
    "    objective='binary',  # each output is binary (0 or 1)\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "model = MultiOutputClassifier(base_model)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:05.418049Z",
     "start_time": "2025-05-20T21:36:24.369594Z"
    }
   },
   "id": "ce1513a0c788986f",
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/645101335.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mehul/Documents/UCSD Classes/ML for Music/MLforMusic/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "keys, X = create_test_features(dataroot3+\"/test.json\", n_jobs=4)\n",
    "y_pred = model.predict(X.numpy())  # MultiOutputClassifier expects NumPy\n",
    "# add code to print to predictions3.json. format should be key:prediction converted back using an index to tag "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:46.567580Z",
     "start_time": "2025-05-20T21:37:30.247708Z"
    }
   },
   "id": "11e71f67d315e29b",
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1000, 10)"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:58.170016Z",
     "start_time": "2025-05-20T21:37:58.158874Z"
    }
   },
   "id": "fe8de6ba403c7a24",
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions written to predictions3.json\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-hot predictions to tag lists\n",
    "predictions = {}\n",
    "for key, pred_vector in zip(keys, y_pred):\n",
    "    tag_list = [TAGS[i] for i, val in enumerate(pred_vector) if val == 1]\n",
    "    predictions[key] = tag_list\n",
    "\n",
    "# Save predictions to predictions3.json\n",
    "output_path = \"predictions3.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(f\"✅ Predictions written to {output_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:37:59.903560Z",
     "start_time": "2025-05-20T21:37:59.887794Z"
    }
   },
   "id": "fd7c694c39322bf9",
   "execution_count": 127
  },
  {
   "cell_type": "markdown",
   "source": [
    "Custom MLP classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2966f61b1cbaa4f1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🖥️  Using device: cpu\n",
      "\n",
      "✅ Epoch 1/200 — Train Loss: 0.5250 | Train mAP: 0.1530 | Val Loss: 0.4342 | Val mAP: 0.2036 | LR: 0.001000 | Elapsed: 0.5s\n",
      "\n",
      "✅ Epoch 2/200 — Train Loss: 0.4214 | Train mAP: 0.1813 | Val Loss: 0.4036 | Val mAP: 0.2281 | LR: 0.001000 | Elapsed: 1.0s\n",
      "\n",
      "✅ Epoch 3/200 — Train Loss: 0.4012 | Train mAP: 0.1990 | Val Loss: 0.3959 | Val mAP: 0.2376 | LR: 0.001000 | Elapsed: 1.6s\n",
      "\n",
      "✅ Epoch 4/200 — Train Loss: 0.3936 | Train mAP: 0.2083 | Val Loss: 0.3894 | Val mAP: 0.2218 | LR: 0.001000 | Elapsed: 2.3s\n",
      "\n",
      "✅ Epoch 5/200 — Train Loss: 0.3784 | Train mAP: 0.2172 | Val Loss: 0.3935 | Val mAP: 0.2382 | LR: 0.001000 | Elapsed: 2.9s\n",
      "\n",
      "✅ Epoch 6/200 — Train Loss: 0.3792 | Train mAP: 0.2176 | Val Loss: 0.3840 | Val mAP: 0.2377 | LR: 0.001000 | Elapsed: 3.5s\n",
      "\n",
      "✅ Epoch 7/200 — Train Loss: 0.3736 | Train mAP: 0.2238 | Val Loss: 0.3800 | Val mAP: 0.2400 | LR: 0.001000 | Elapsed: 3.9s\n",
      "\n",
      "✅ Epoch 8/200 — Train Loss: 0.3654 | Train mAP: 0.2324 | Val Loss: 0.3781 | Val mAP: 0.2459 | LR: 0.001000 | Elapsed: 4.3s\n",
      "\n",
      "✅ Epoch 9/200 — Train Loss: 0.3635 | Train mAP: 0.2352 | Val Loss: 0.3798 | Val mAP: 0.2456 | LR: 0.001000 | Elapsed: 4.6s\n",
      "\n",
      "✅ Epoch 10/200 — Train Loss: 0.3645 | Train mAP: 0.2397 | Val Loss: 0.3835 | Val mAP: 0.2478 | LR: 0.001000 | Elapsed: 4.8s\n",
      "\n",
      "✅ Epoch 11/200 — Train Loss: 0.3565 | Train mAP: 0.2392 | Val Loss: 0.3806 | Val mAP: 0.2411 | LR: 0.001000 | Elapsed: 5.1s\n",
      "\n",
      "✅ Epoch 12/200 — Train Loss: 0.3570 | Train mAP: 0.2429 | Val Loss: 0.3833 | Val mAP: 0.2551 | LR: 0.001000 | Elapsed: 5.3s\n",
      "\n",
      "✅ Epoch 13/200 — Train Loss: 0.3534 | Train mAP: 0.2448 | Val Loss: 0.3847 | Val mAP: 0.2479 | LR: 0.001000 | Elapsed: 5.5s\n",
      "\n",
      "✅ Epoch 14/200 — Train Loss: 0.3501 | Train mAP: 0.2579 | Val Loss: 0.3820 | Val mAP: 0.2500 | LR: 0.001000 | Elapsed: 5.8s\n",
      "\n",
      "✅ Epoch 15/200 — Train Loss: 0.3493 | Train mAP: 0.2557 | Val Loss: 0.3822 | Val mAP: 0.2508 | LR: 0.000500 | Elapsed: 6.1s\n",
      "\n",
      "✅ Epoch 16/200 — Train Loss: 0.3426 | Train mAP: 0.2553 | Val Loss: 0.3804 | Val mAP: 0.2486 | LR: 0.000500 | Elapsed: 6.5s\n",
      "\n",
      "✅ Epoch 17/200 — Train Loss: 0.3406 | Train mAP: 0.2596 | Val Loss: 0.3842 | Val mAP: 0.2476 | LR: 0.000500 | Elapsed: 6.8s\n",
      "\n",
      "✅ Epoch 18/200 — Train Loss: 0.3392 | Train mAP: 0.2641 | Val Loss: 0.3787 | Val mAP: 0.2446 | LR: 0.000500 | Elapsed: 7.2s\n",
      "\n",
      "✅ Epoch 19/200 — Train Loss: 0.3293 | Train mAP: 0.2728 | Val Loss: 0.3798 | Val mAP: 0.2533 | LR: 0.000500 | Elapsed: 7.6s\n",
      "\n",
      "✅ Epoch 20/200 — Train Loss: 0.3317 | Train mAP: 0.2719 | Val Loss: 0.3806 | Val mAP: 0.2611 | LR: 0.000500 | Elapsed: 7.8s\n",
      "\n",
      "✅ Epoch 21/200 — Train Loss: 0.3302 | Train mAP: 0.2783 | Val Loss: 0.3843 | Val mAP: 0.2427 | LR: 0.000250 | Elapsed: 8.1s\n",
      "\n",
      "✅ Epoch 22/200 — Train Loss: 0.3246 | Train mAP: 0.2766 | Val Loss: 0.3779 | Val mAP: 0.2539 | LR: 0.000250 | Elapsed: 8.3s\n",
      "\n",
      "✅ Epoch 23/200 — Train Loss: 0.3263 | Train mAP: 0.2848 | Val Loss: 0.3774 | Val mAP: 0.2552 | LR: 0.000250 | Elapsed: 8.6s\n",
      "\n",
      "✅ Epoch 24/200 — Train Loss: 0.3263 | Train mAP: 0.2801 | Val Loss: 0.3819 | Val mAP: 0.2585 | LR: 0.000250 | Elapsed: 8.8s\n",
      "\n",
      "✅ Epoch 25/200 — Train Loss: 0.3226 | Train mAP: 0.2948 | Val Loss: 0.3787 | Val mAP: 0.2567 | LR: 0.000250 | Elapsed: 9.0s\n",
      "\n",
      "✅ Epoch 26/200 — Train Loss: 0.3177 | Train mAP: 0.2859 | Val Loss: 0.3812 | Val mAP: 0.2582 | LR: 0.000250 | Elapsed: 9.3s\n",
      "\n",
      "✅ Epoch 27/200 — Train Loss: 0.3253 | Train mAP: 0.2922 | Val Loss: 0.3812 | Val mAP: 0.2613 | LR: 0.000250 | Elapsed: 9.5s\n",
      "\n",
      "✅ Epoch 28/200 — Train Loss: 0.3221 | Train mAP: 0.2819 | Val Loss: 0.3802 | Val mAP: 0.2521 | LR: 0.000250 | Elapsed: 9.7s\n",
      "\n",
      "✅ Epoch 29/200 — Train Loss: 0.3173 | Train mAP: 0.2935 | Val Loss: 0.3789 | Val mAP: 0.2565 | LR: 0.000250 | Elapsed: 10.0s\n",
      "\n",
      "✅ Epoch 30/200 — Train Loss: 0.3149 | Train mAP: 0.2874 | Val Loss: 0.3789 | Val mAP: 0.2574 | LR: 0.000125 | Elapsed: 10.2s\n",
      "\n",
      "✅ Epoch 31/200 — Train Loss: 0.3128 | Train mAP: 0.3099 | Val Loss: 0.3800 | Val mAP: 0.2589 | LR: 0.000125 | Elapsed: 10.5s\n",
      "\n",
      "✅ Epoch 32/200 — Train Loss: 0.3130 | Train mAP: 0.2974 | Val Loss: 0.3813 | Val mAP: 0.2603 | LR: 0.000125 | Elapsed: 10.7s\n",
      "\n",
      "✅ Epoch 33/200 — Train Loss: 0.3088 | Train mAP: 0.2982 | Val Loss: 0.3780 | Val mAP: 0.2568 | LR: 0.000125 | Elapsed: 11.0s\n",
      "\n",
      "✅ Epoch 34/200 — Train Loss: 0.3109 | Train mAP: 0.3045 | Val Loss: 0.3798 | Val mAP: 0.2561 | LR: 0.000125 | Elapsed: 11.2s\n",
      "\n",
      "✅ Epoch 35/200 — Train Loss: 0.3142 | Train mAP: 0.2987 | Val Loss: 0.3841 | Val mAP: 0.2552 | LR: 0.000125 | Elapsed: 11.5s\n",
      "\n",
      "✅ Epoch 36/200 — Train Loss: 0.3055 | Train mAP: 0.3120 | Val Loss: 0.3809 | Val mAP: 0.2575 | LR: 0.000063 | Elapsed: 11.8s\n",
      "\n",
      "✅ Epoch 37/200 — Train Loss: 0.3062 | Train mAP: 0.3119 | Val Loss: 0.3812 | Val mAP: 0.2565 | LR: 0.000063 | Elapsed: 12.1s\n",
      "\n",
      "✅ Epoch 38/200 — Train Loss: 0.3101 | Train mAP: 0.3111 | Val Loss: 0.3827 | Val mAP: 0.2509 | LR: 0.000063 | Elapsed: 12.3s\n",
      "\n",
      "✅ Epoch 39/200 — Train Loss: 0.3059 | Train mAP: 0.3055 | Val Loss: 0.3792 | Val mAP: 0.2589 | LR: 0.000063 | Elapsed: 12.7s\n",
      "\n",
      "✅ Epoch 40/200 — Train Loss: 0.3052 | Train mAP: 0.3089 | Val Loss: 0.3811 | Val mAP: 0.2594 | LR: 0.000063 | Elapsed: 13.1s\n",
      "\n",
      "✅ Epoch 41/200 — Train Loss: 0.3024 | Train mAP: 0.3091 | Val Loss: 0.3802 | Val mAP: 0.2601 | LR: 0.000063 | Elapsed: 13.6s\n",
      "\n",
      "✅ Epoch 42/200 — Train Loss: 0.3070 | Train mAP: 0.3080 | Val Loss: 0.3797 | Val mAP: 0.2634 | LR: 0.000031 | Elapsed: 14.0s\n",
      "\n",
      "✅ Epoch 43/200 — Train Loss: 0.3061 | Train mAP: 0.3108 | Val Loss: 0.3812 | Val mAP: 0.2576 | LR: 0.000031 | Elapsed: 14.3s\n",
      "\n",
      "✅ Epoch 44/200 — Train Loss: 0.3040 | Train mAP: 0.3102 | Val Loss: 0.3790 | Val mAP: 0.2605 | LR: 0.000031 | Elapsed: 14.7s\n",
      "\n",
      "✅ Epoch 45/200 — Train Loss: 0.3030 | Train mAP: 0.3176 | Val Loss: 0.3813 | Val mAP: 0.2560 | LR: 0.000031 | Elapsed: 15.5s\n",
      "\n",
      "✅ Epoch 46/200 — Train Loss: 0.3008 | Train mAP: 0.3172 | Val Loss: 0.3818 | Val mAP: 0.2615 | LR: 0.000031 | Elapsed: 15.9s\n",
      "\n",
      "✅ Epoch 47/200 — Train Loss: 0.3003 | Train mAP: 0.3195 | Val Loss: 0.3822 | Val mAP: 0.2666 | LR: 0.000031 | Elapsed: 16.2s\n",
      "\n",
      "✅ Epoch 48/200 — Train Loss: 0.3016 | Train mAP: 0.3181 | Val Loss: 0.3822 | Val mAP: 0.2620 | LR: 0.000016 | Elapsed: 16.4s\n",
      "\n",
      "✅ Epoch 49/200 — Train Loss: 0.3042 | Train mAP: 0.3073 | Val Loss: 0.3805 | Val mAP: 0.2589 | LR: 0.000016 | Elapsed: 16.7s\n",
      "\n",
      "✅ Epoch 50/200 — Train Loss: 0.3039 | Train mAP: 0.3080 | Val Loss: 0.3822 | Val mAP: 0.2663 | LR: 0.000016 | Elapsed: 17.0s\n",
      "\n",
      "✅ Epoch 51/200 — Train Loss: 0.3030 | Train mAP: 0.3105 | Val Loss: 0.3826 | Val mAP: 0.2620 | LR: 0.000016 | Elapsed: 17.2s\n",
      "\n",
      "✅ Epoch 52/200 — Train Loss: 0.3015 | Train mAP: 0.3195 | Val Loss: 0.3786 | Val mAP: 0.2520 | LR: 0.000016 | Elapsed: 17.5s\n",
      "\n",
      "✅ Epoch 53/200 — Train Loss: 0.3002 | Train mAP: 0.3147 | Val Loss: 0.3831 | Val mAP: 0.2662 | LR: 0.000016 | Elapsed: 17.8s\n",
      "\n",
      "✅ Epoch 54/200 — Train Loss: 0.3041 | Train mAP: 0.3220 | Val Loss: 0.3804 | Val mAP: 0.2598 | LR: 0.000008 | Elapsed: 18.0s\n",
      "\n",
      "✅ Epoch 55/200 — Train Loss: 0.3052 | Train mAP: 0.3161 | Val Loss: 0.3797 | Val mAP: 0.2583 | LR: 0.000008 | Elapsed: 18.3s\n",
      "\n",
      "✅ Epoch 56/200 — Train Loss: 0.3018 | Train mAP: 0.3127 | Val Loss: 0.3821 | Val mAP: 0.2660 | LR: 0.000008 | Elapsed: 18.5s\n",
      "\n",
      "✅ Epoch 57/200 — Train Loss: 0.2997 | Train mAP: 0.3204 | Val Loss: 0.3803 | Val mAP: 0.2661 | LR: 0.000008 | Elapsed: 18.8s\n",
      "\n",
      "✅ Epoch 58/200 — Train Loss: 0.3016 | Train mAP: 0.3118 | Val Loss: 0.3836 | Val mAP: 0.2623 | LR: 0.000008 | Elapsed: 19.1s\n",
      "\n",
      "✅ Epoch 59/200 — Train Loss: 0.2958 | Train mAP: 0.3124 | Val Loss: 0.3807 | Val mAP: 0.2680 | LR: 0.000008 | Elapsed: 19.3s\n",
      "\n",
      "✅ Epoch 60/200 — Train Loss: 0.2977 | Train mAP: 0.3245 | Val Loss: 0.3794 | Val mAP: 0.2585 | LR: 0.000004 | Elapsed: 19.6s\n",
      "\n",
      "✅ Epoch 61/200 — Train Loss: 0.3016 | Train mAP: 0.3229 | Val Loss: 0.3844 | Val mAP: 0.2605 | LR: 0.000004 | Elapsed: 19.9s\n",
      "\n",
      "✅ Epoch 62/200 — Train Loss: 0.2998 | Train mAP: 0.3258 | Val Loss: 0.3818 | Val mAP: 0.2619 | LR: 0.000004 | Elapsed: 20.1s\n",
      "\n",
      "✅ Epoch 63/200 — Train Loss: 0.3046 | Train mAP: 0.3143 | Val Loss: 0.3836 | Val mAP: 0.2599 | LR: 0.000004 | Elapsed: 20.4s\n",
      "\n",
      "✅ Epoch 64/200 — Train Loss: 0.3015 | Train mAP: 0.3140 | Val Loss: 0.3825 | Val mAP: 0.2597 | LR: 0.000004 | Elapsed: 20.6s\n",
      "\n",
      "✅ Epoch 65/200 — Train Loss: 0.3009 | Train mAP: 0.3128 | Val Loss: 0.3791 | Val mAP: 0.2599 | LR: 0.000004 | Elapsed: 20.9s\n",
      "\n",
      "✅ Epoch 66/200 — Train Loss: 0.2979 | Train mAP: 0.3226 | Val Loss: 0.3803 | Val mAP: 0.2567 | LR: 0.000002 | Elapsed: 21.2s\n",
      "\n",
      "✅ Epoch 67/200 — Train Loss: 0.2985 | Train mAP: 0.3204 | Val Loss: 0.3802 | Val mAP: 0.2613 | LR: 0.000002 | Elapsed: 21.4s\n",
      "\n",
      "✅ Epoch 68/200 — Train Loss: 0.3013 | Train mAP: 0.3265 | Val Loss: 0.3809 | Val mAP: 0.2702 | LR: 0.000002 | Elapsed: 21.7s\n",
      "\n",
      "✅ Epoch 69/200 — Train Loss: 0.3003 | Train mAP: 0.3155 | Val Loss: 0.3813 | Val mAP: 0.2584 | LR: 0.000002 | Elapsed: 22.0s\n",
      "\n",
      "✅ Epoch 70/200 — Train Loss: 0.3023 | Train mAP: 0.3214 | Val Loss: 0.3821 | Val mAP: 0.2756 | LR: 0.000002 | Elapsed: 22.3s\n",
      "\n",
      "✅ Epoch 71/200 — Train Loss: 0.3029 | Train mAP: 0.3188 | Val Loss: 0.3822 | Val mAP: 0.2634 | LR: 0.000002 | Elapsed: 22.7s\n",
      "\n",
      "✅ Epoch 72/200 — Train Loss: 0.3044 | Train mAP: 0.3106 | Val Loss: 0.3852 | Val mAP: 0.2658 | LR: 0.000001 | Elapsed: 23.1s\n",
      "\n",
      "✅ Epoch 73/200 — Train Loss: 0.3023 | Train mAP: 0.3073 | Val Loss: 0.3782 | Val mAP: 0.2589 | LR: 0.000001 | Elapsed: 23.4s\n",
      "\n",
      "✅ Epoch 74/200 — Train Loss: 0.3020 | Train mAP: 0.3260 | Val Loss: 0.3829 | Val mAP: 0.2587 | LR: 0.000001 | Elapsed: 23.7s\n",
      "\n",
      "✅ Epoch 75/200 — Train Loss: 0.2993 | Train mAP: 0.3121 | Val Loss: 0.3807 | Val mAP: 0.2630 | LR: 0.000001 | Elapsed: 24.0s\n",
      "\n",
      "✅ Epoch 76/200 — Train Loss: 0.2991 | Train mAP: 0.3214 | Val Loss: 0.3829 | Val mAP: 0.2676 | LR: 0.000001 | Elapsed: 24.3s\n",
      "\n",
      "✅ Epoch 77/200 — Train Loss: 0.2986 | Train mAP: 0.3260 | Val Loss: 0.3825 | Val mAP: 0.2631 | LR: 0.000001 | Elapsed: 24.5s\n",
      "\n",
      "✅ Epoch 78/200 — Train Loss: 0.3035 | Train mAP: 0.3137 | Val Loss: 0.3792 | Val mAP: 0.2566 | LR: 0.000000 | Elapsed: 24.8s\n",
      "\n",
      "✅ Epoch 79/200 — Train Loss: 0.3012 | Train mAP: 0.3198 | Val Loss: 0.3812 | Val mAP: 0.2674 | LR: 0.000000 | Elapsed: 25.0s\n",
      "\n",
      "✅ Epoch 80/200 — Train Loss: 0.3006 | Train mAP: 0.3143 | Val Loss: 0.3804 | Val mAP: 0.2618 | LR: 0.000000 | Elapsed: 25.3s\n",
      "\n",
      "✅ Epoch 81/200 — Train Loss: 0.3000 | Train mAP: 0.3371 | Val Loss: 0.3806 | Val mAP: 0.2677 | LR: 0.000000 | Elapsed: 25.6s\n",
      "\n",
      "✅ Epoch 82/200 — Train Loss: 0.3047 | Train mAP: 0.3120 | Val Loss: 0.3794 | Val mAP: 0.2576 | LR: 0.000000 | Elapsed: 25.8s\n",
      "\n",
      "✅ Epoch 83/200 — Train Loss: 0.3033 | Train mAP: 0.3197 | Val Loss: 0.3817 | Val mAP: 0.2612 | LR: 0.000000 | Elapsed: 26.1s\n",
      "\n",
      "✅ Epoch 84/200 — Train Loss: 0.3028 | Train mAP: 0.3143 | Val Loss: 0.3815 | Val mAP: 0.2627 | LR: 0.000000 | Elapsed: 26.4s\n",
      "\n",
      "✅ Epoch 85/200 — Train Loss: 0.3001 | Train mAP: 0.3193 | Val Loss: 0.3820 | Val mAP: 0.2609 | LR: 0.000000 | Elapsed: 26.6s\n",
      "\n",
      "✅ Epoch 86/200 — Train Loss: 0.3013 | Train mAP: 0.3088 | Val Loss: 0.3807 | Val mAP: 0.2640 | LR: 0.000000 | Elapsed: 26.9s\n",
      "\n",
      "✅ Epoch 87/200 — Train Loss: 0.3038 | Train mAP: 0.3209 | Val Loss: 0.3773 | Val mAP: 0.2653 | LR: 0.000000 | Elapsed: 27.2s\n",
      "\n",
      "✅ Epoch 88/200 — Train Loss: 0.3032 | Train mAP: 0.3173 | Val Loss: 0.3812 | Val mAP: 0.2600 | LR: 0.000000 | Elapsed: 27.6s\n",
      "\n",
      "✅ Epoch 89/200 — Train Loss: 0.3010 | Train mAP: 0.3144 | Val Loss: 0.3801 | Val mAP: 0.2644 | LR: 0.000000 | Elapsed: 27.9s\n",
      "\n",
      "✅ Epoch 90/200 — Train Loss: 0.3004 | Train mAP: 0.3294 | Val Loss: 0.3824 | Val mAP: 0.2603 | LR: 0.000000 | Elapsed: 28.3s\n",
      "\n",
      "🛑 Early stopping at epoch 90. No val mAP improvement for 20 epochs.\n",
      "\n",
      "🏁 Training complete. Best val mAP: 0.2756. Total time: 28.25s\n"
     ]
    },
    {
     "data": {
      "text/plain": "SklearnMLPClassifier(epochs=200, input_dim=2244, n_classes=10)",
      "text/html": "<style>#sk-container-id-10 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-10 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-10 pre {\n  padding: 0;\n}\n\n#sk-container-id-10 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-10 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-10 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-10 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-10 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-10 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-10 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-10 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-10 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-10 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-10 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-10 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-10 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-10 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-10 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-10 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-10 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n#sk-container-id-10 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-10 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-10 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-10 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-10 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-10 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-10 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-10 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-10 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-10 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244, n_classes=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SklearnMLPClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SklearnMLPClassifier(epochs=200, input_dim=2244, n_classes=10)</pre></div> </div></div></div></div>"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SklearnMLPClassifier(input_dim=feature_size, n_classes=10, epochs=200, verbose=True)\n",
    "model.fit(X_train, y_train, X_val_global, y_val_global)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:32:58.408671Z",
     "start_time": "2025-05-20T21:32:30.088059Z"
    }
   },
   "id": "23109bf6b3229ce1",
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape: torch.Size([1000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_9431/645101335.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([torch.tensor(x, dtype=torch.float32) for x in X])\n"
     ]
    }
   ],
   "source": [
    "keys, X = create_test_features(dataroot3+\"/test.json\", n_jobs=4)\n",
    "y_pred = model.predict(X.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:27:14.362957Z",
     "start_time": "2025-05-20T21:26:59.916371Z"
    }
   },
   "id": "e5b6d7eef38a80c1",
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions written to predictions3.json\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-hot predictions to tag lists\n",
    "predictions = {}\n",
    "for key, pred_vector in zip(keys, y_pred):\n",
    "    tag_list = [TAGS[i] for i, val in enumerate(pred_vector) if val == 1]\n",
    "    predictions[key] = tag_list\n",
    "\n",
    "# Save predictions to predictions3.json\n",
    "output_path = \"predictions3.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(f\"✅ Predictions written to {output_path}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:27:14.371153Z",
     "start_time": "2025-05-20T21:27:14.363255Z"
    }
   },
   "id": "590367e8204d86c2",
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5e6c23bac1f47994"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
