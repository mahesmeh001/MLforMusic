{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Simple is best. Using sklearns mutlioutput classifier on a basic CNN model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3bbcef4058eeaa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, basics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7003d21c5eb123d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:38.470289Z",
     "start_time": "2025-05-20T18:24:37.353018Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance', 'blues', 'punk', 'chill', 'electronic', 'country']\n",
    "tag_to_index = {tag: i for i, tag in enumerate(TAGS)}\n",
    "\n",
    "\n",
    "# do multi-hot encoding\n",
    "\n",
    "def multi_hot_encode(tags):\n",
    "    \"\"\"\n",
    "    Given a list of tag strings, return a multi-hot encoded tensor.\n",
    "    Example input: ['jazz', 'pop']\n",
    "    Output: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
    "    \"\"\"\n",
    "    vec = torch.zeros(len(TAGS), dtype=torch.float32)\n",
    "    for tag in tags:\n",
    "        if tag in tag_to_index:\n",
    "            vec[tag_to_index[tag]] = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tag: {tag}\")\n",
    "    return vec\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:38.474059Z",
     "start_time": "2025-05-20T18:24:38.471690Z"
    }
   },
   "id": "74bdb69fcd21e847",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataroot3 = \"data/student_files/task3_audio_classification/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:38.476149Z",
     "start_time": "2025-05-20T18:24:38.474016Z"
    }
   },
   "id": "cb814ecda6aa6e7b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib  # for sklearn models\n",
    "import os\n",
    "\n",
    "def save_model(model, filepath='sol_3_2.pt'):\n",
    "    \"\"\"Save a PyTorch or scikit-learn model to a file\"\"\"\n",
    "    if 'torch' in str(type(model)):\n",
    "        torch.save(model.state_dict(), filepath)\n",
    "        print(f\"PyTorch model saved to {filepath}\")\n",
    "    else:\n",
    "        joblib.dump(model, filepath)\n",
    "        print(f\"scikit-learn model saved to {filepath}\")\n",
    "\n",
    "def load_model(model_class_or_type, filepath='sol_3_2.pt', *args, **kwargs):\n",
    "    \"\"\"Load a PyTorch or scikit-learn model from a file\"\"\"\n",
    "    ext = os.path.splitext(filepath)[1]\n",
    "    \n",
    "    if ext in ['.pt', '.pth']:\n",
    "        model = model_class_or_type(*args, **kwargs)  # instantiate PyTorch model\n",
    "        model.load_state_dict(torch.load(filepath))\n",
    "        model.eval()\n",
    "        print(f\"PyTorch model loaded from {filepath}\")\n",
    "    else:\n",
    "        model = joblib.load(filepath)\n",
    "        print(f\"scikit-learn model loaded from {filepath}\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:38.480051Z",
     "start_time": "2025-05-20T18:24:38.478115Z"
    }
   },
   "id": "cfc9e827ce380cd9",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "I already have the data i want, just load it in"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c44052751aada0d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load data\n",
    "with open(\"task3_train_data.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_data = data['x']\n",
    "y_data = data['y']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:49.947968Z",
     "start_time": "2025-05-20T18:24:38.480195Z"
    }
   },
   "id": "50333c037601c91f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4000, 3, 128, 512]), torch.Size([4000, 10]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape, y_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:49.977721Z",
     "start_time": "2025-05-20T18:24:49.975125Z"
    }
   },
   "id": "c4c47771154afa81",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:49.981847Z",
     "start_time": "2025-05-20T18:24:49.978410Z"
    }
   },
   "id": "1eb5dffaad13a83b",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Format for skLearn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187f5c1e70c87a56"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split the 3 channels into separate tensors\n",
    "mel = X_data[:, 0].unsqueeze(1)   # (4000, 1, 128, 512)\n",
    "mfcc = X_data[:, 1].unsqueeze(1)  # (4000, 1, 128, 512)\n",
    "q = X_data[:, 2].unsqueeze(1)     # (4000, 1, 128, 512)\n",
    "\n",
    "# No change needed for y_data\n",
    "y = y_data  # (4000, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:24:49.993153Z",
     "start_time": "2025-05-20T18:24:49.989921Z"
    }
   },
   "id": "daec911b07edd09b",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Real input arrays (shape: [4000, 1, 128, 512])\n",
    "mel_np = mel.numpy()\n",
    "mfcc_np = mfcc.numpy()\n",
    "q_np = q.numpy()\n",
    "y_np = y.numpy()\n",
    "\n",
    "# Step 1: Create dummy X just for index-based splitting\n",
    "X_dummy = np.zeros((y_np.shape[0], 1))  # shape: (4000, 1)\n",
    "\n",
    "# Step 2: Perform iterative split on dummy X\n",
    "X_dummy_train, y_train, X_dummy_val, y_val = iterative_train_test_split(X_dummy, y_np, test_size=0.1)\n",
    "\n",
    "# Step 3: Get indices back by comparing values (X_dummy has 0s)\n",
    "train_idx = np.where(X_dummy_train[:, 0] == 0)[0]\n",
    "val_idx = np.where(X_dummy_val[:, 0] == 0)[0]\n",
    "\n",
    "# Step 4: Use those indices to slice real inputs\n",
    "mel_train, mfcc_train, q_train = mel_np[train_idx], mfcc_np[train_idx], q_np[train_idx]\n",
    "mel_val, mfcc_val, q_val = mel_np[val_idx], mfcc_np[val_idx], q_np[val_idx]\n",
    "\n",
    "# Final output format\n",
    "X_train = (mel_train, mfcc_train, q_train)\n",
    "X_val = (mel_val, mfcc_val, q_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:25:01.615393Z",
     "start_time": "2025-05-20T18:24:49.999210Z"
    }
   },
   "id": "5ee302c75c03eefd",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Class Imbalances"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45dd101b6605bd1c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def verify_data(y_train, mel=None, mfcc=None, num_classes=10):\n",
    "    \"\"\"Check label distribution and optional input stats. \n",
    "       Returns pos_weight tensor for BCEWithLogitsLoss to handle class imbalance.\n",
    "\n",
    "       Parameters:\n",
    "       - y_train (Tensor or ndarray): shape (N, num_classes), binary multi-label.\n",
    "       - mel (Tensor or ndarray): optional, for range checking.\n",
    "       - mfcc (Tensor or ndarray): optional, for range checking.\n",
    "    \"\"\"\n",
    "    if isinstance(y_train, np.ndarray):\n",
    "        y_train = torch.tensor(y_train)\n",
    "    \n",
    "    label_counter = Counter()\n",
    "    total_assignments = 0\n",
    "    sample_count = y_train.size(0)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        class_count = (y_train[:, i] == 1).sum().item()\n",
    "        label_counter[i] += class_count\n",
    "        total_assignments += class_count\n",
    "\n",
    "    print(f\"Total samples: {sample_count}\")\n",
    "    print(f\"Total class assignments (1s): {total_assignments}\\n\")\n",
    "\n",
    "    print(\"Class frequency distribution:\")\n",
    "    counts = []\n",
    "    for i in range(num_classes):\n",
    "        count = label_counter[i]\n",
    "        counts.append(count)\n",
    "        print(f\"  Class {i}: {count} assignments ({count / total_assignments:.2%})\")\n",
    "\n",
    "    # Compute pos_weight = (N - count) / count\n",
    "    label_counts_tensor = torch.tensor(counts, dtype=torch.float)\n",
    "    pos_weight = (sample_count - label_counts_tensor) / label_counts_tensor\n",
    "\n",
    "    print(\"\\nComputed pos_weight (for BCEWithLogitsLoss):\")\n",
    "    for i, w in enumerate(pos_weight):\n",
    "        print(f\"  Class {i}: {w.item():.4f}\")\n",
    "\n",
    "    # Optional: check mel / mfcc value ranges\n",
    "    if mel is not None and isinstance(mel, np.ndarray):\n",
    "        mel = torch.tensor(mel)\n",
    "    if mfcc is not None and isinstance(mfcc, np.ndarray):\n",
    "        mfcc = torch.tensor(mfcc)\n",
    "\n",
    "    if mel is not None:\n",
    "        if torch.isnan(mel).any() or torch.isinf(mel).any():\n",
    "            print(\"WARNING: NaN or Inf values found in mel data!\")\n",
    "        else:\n",
    "            print(f\"mel range: [{mel.min().item():.4f}, {mel.max().item():.4f}]\")\n",
    "\n",
    "    if mfcc is not None:\n",
    "        if torch.isnan(mfcc).any() or torch.isinf(mfcc).any():\n",
    "            print(\"WARNING: NaN or Inf values found in mfcc data!\")\n",
    "        else:\n",
    "            print(f\"mfcc range: [{mfcc.min().item():.4f}, {mfcc.max().item():.4f}]\")\n",
    "\n",
    "    return pos_weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:25:01.652062Z",
     "start_time": "2025-05-20T18:25:01.628224Z"
    }
   },
   "id": "310252e33064af22",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3602\n",
      "Total class assignments (1s): 4166\n",
      "\n",
      "Class frequency distribution:\n",
      "  Class 0: 1765 assignments (42.37%)\n",
      "  Class 1: 147 assignments (3.53%)\n",
      "  Class 2: 351 assignments (8.43%)\n",
      "  Class 3: 617 assignments (14.81%)\n",
      "  Class 4: 152 assignments (3.65%)\n",
      "  Class 5: 220 assignments (5.28%)\n",
      "  Class 6: 162 assignments (3.89%)\n",
      "  Class 7: 41 assignments (0.98%)\n",
      "  Class 8: 476 assignments (11.43%)\n",
      "  Class 9: 235 assignments (5.64%)\n",
      "\n",
      "Computed pos_weight (for BCEWithLogitsLoss):\n",
      "  Class 0: 1.0408\n",
      "  Class 1: 23.5034\n",
      "  Class 2: 9.2621\n",
      "  Class 3: 4.8379\n",
      "  Class 4: 22.6974\n",
      "  Class 5: 15.3727\n",
      "  Class 6: 21.2346\n",
      "  Class 7: 86.8537\n",
      "  Class 8: 6.5672\n",
      "  Class 9: 14.3277\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.7133, 3.1988, 2.3285, 1.7644, 3.1654, 2.7956, 3.1016, 4.4757, 2.0238,\n        2.7297])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = verify_data(y_train)\n",
    "pos_weight = torch.log1p(pos_weight) \n",
    "pos_weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:25:02.034112Z",
     "start_time": "2025-05-20T18:25:01.632673Z"
    }
   },
   "id": "785adbf8d8caf283",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model stuff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f25191f11b95456e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),   # (B, 16, 128, 512)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                           # (B, 16, 64, 256)\n",
    "        \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # (B, 32, 64, 256)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                           # (B, 32, 32, 128)\n",
    "        \n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # (B, 32, 32, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                           # (B, 32, 16, 64)\n",
    "        \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (B, 64, 16, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                           # (B, 64, 8, 32)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_branch = nn.Sequential(\n",
    "            nn.Flatten(),                                # (B, 96 * 8 * 32 = 24,576)\n",
    "            nn.Linear(64 * 8 * 32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_branch(x)  # (B, 512)\n",
    "        return x\n",
    "\n",
    "class MultiInputCNNClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.branch_mel = CNNBranch()\n",
    "        self.branch_mfcc = CNNBranch()\n",
    "        self.branch_q = CNNBranch()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 3, 256),  # (B, 1536)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel, mfcc, q):  # Each input: (B, 1, 128, 512)\n",
    "        f_mel = self.branch_mel(mel)\n",
    "        f_mfcc = self.branch_mfcc(mfcc)\n",
    "        f_q = self.branch_q(q)\n",
    "\n",
    "        x = torch.cat([f_mel, f_mfcc, f_q], dim=1)  # (B, 1536)\n",
    "        return self.classifier(x)                   # (B, n_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:25:02.034378Z",
     "start_time": "2025-05-20T18:25:01.923125Z"
    }
   },
   "id": "5fe69ef4c013eb38",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=10, input_shape=(1, 128, 512)):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Compute flattened feature size dynamically\n",
    "        self.feature_dim = self._get_feature_dim(input_shape)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.feature_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def _get_feature_dim(self, input_shape):\n",
    "        x = torch.zeros(1, *input_shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):  # x: (B, 1, 128, 512)\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (B, 16, 64, 256)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B, 32, 32, 128)\n",
    "        x = x.view(x.size(0), -1)             # Flatten\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return torch.sigmoid(self.fc2(x))     # For multilabel classification\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:33:05.385326Z",
     "start_time": "2025-05-20T18:33:05.336816Z"
    }
   },
   "id": "dd33ad4dd443590a",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleInputCNNClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=10, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),         # (B, 16, 64, 256)\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),         # (B, 32, 32, 128)\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),         # (B, 64, 16, 64)\n",
    "            nn.Dropout2d(0.15)\n",
    "        )\n",
    "        \n",
    "        # Use adaptive pooling to avoid giant flattening\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))  # Output: (B, 64, 4, 4)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),              # (B, 64 * 4 * 4 = 1024)\n",
    "            nn.Linear(64 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):  # x: (B, 1, 128, 512)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x  # (B, n_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:25:02.034502Z",
     "start_time": "2025-05-20T18:25:01.930780Z"
    }
   },
   "id": "7d736d584bc61a6",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wrapper for sklearns mutli-output classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f0cde09f29931d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "# Assumes MultiInputCNNClassifier is defined elsewhere and imported\n",
    "# from your_model import MultiInputCNNClassifier\n",
    "\n",
    "class SklearnCNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, epochs=500, lr=1e-3, batch_size=24, device='cpu', n_classes=1):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.n_classes = n_classes\n",
    "        self._build_model()\n",
    "        self.history = []\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        # self.model = MultiInputCNNClassifier(n_classes=self.n_classes).to(self.device)\n",
    "        # self.model = SingleInputCNNClassifier(n_classes=self.n_classes).to(self.device)\n",
    "        self.model = CNNClassifier(n_classes=10).to(self.device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(self.device))\n",
    "        self. optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "    def overfit(self, X, y, max_epochs=100, patience=10, restore_best=True):\n",
    "        import time\n",
    "        import torch\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "    \n",
    "        device = torch.device(self.device)\n",
    "        print(f\"\\n🖥️  Using device: {device}\")\n",
    "    \n",
    "        mel, mfcc, q = [torch.tensor(arr, dtype=torch.float32) for arr in X]\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "        dataset = TensorDataset(mel, mfcc, q, y)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "    \n",
    "        start_time = time.time()\n",
    "        print(f\"🔥 Overfitting CNN classifier (output dim={self.n_classes}) to maximize mAP...\")\n",
    "    \n",
    "        best_map = -1\n",
    "        best_epoch = 0\n",
    "        best_state = None\n",
    "    \n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "    \n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "    \n",
    "            for i, (mel_b, mfcc_b, q_b, y_b) in enumerate(loader):\n",
    "            \n",
    "                mel_b = mel_b.to(device)\n",
    "                mfcc_b = mfcc_b.to(device)\n",
    "                q_b = q_b.to(device)\n",
    "                y_b = y_b.to(device)\n",
    "    \n",
    "                self.optimizer.zero_grad()\n",
    "                # outputs = self.model(mel_b, mfcc_b, q_b)\n",
    "                outputs = self.model(mel_b)\n",
    "                loss = self.criterion(outputs, y_b)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "    \n",
    "                preds = torch.sigmoid(outputs).detach().cpu()\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(y_b.detach().cpu())\n",
    "    \n",
    "                if i == 0:\n",
    "                    print(f\"\\nEpoch {epoch}, Batch {i}:\")\n",
    "                    print(f\"  Targets     : {y_b.cpu().numpy()[:1]}\")\n",
    "                    print(f\"  Predictions : {preds.numpy()[:1]}\")\n",
    "    \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            all_preds = torch.cat(all_preds).numpy()\n",
    "            all_targets = torch.cat(all_targets).numpy()\n",
    "    \n",
    "            try:\n",
    "                map_score = average_precision_score(all_targets, all_preds, average=\"macro\")\n",
    "            except ValueError as e:\n",
    "                map_score = float(\"nan\")\n",
    "                print(f\"⚠️  mAP computation failed: {e}\")\n",
    "    \n",
    "            print(f\"✅ Epoch {epoch}/{max_epochs} — Avg Loss: {avg_loss:.4f} | mAP: {map_score:.4f}\")\n",
    "    \n",
    "            # Track best model\n",
    "            if map_score > best_map:\n",
    "                best_map = map_score\n",
    "                best_epoch = epoch\n",
    "            elif epoch - best_epoch >= patience:\n",
    "                print(f\"\\n⏹️ Early stopping: mAP has not improved for {patience} epochs (Best: {best_map:.4f} @ epoch {best_epoch})\")\n",
    "                break\n",
    "    \n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "        if restore_best and best_state is not None:\n",
    "            self.model.load_state_dict(best_state)\n",
    "            print(f\"\\n🧠 Restored best model state from epoch {best_epoch} with mAP={best_map:.4f}\")\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"🏁 Overfitting complete. Best mAP: {best_map:.4f}. Time elapsed: {elapsed:.2f}s\\n\")\n",
    "    \n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        import time\n",
    "        import copy\n",
    "        import torch\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        from sklearn.metrics import average_precision_score\n",
    "        from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    \n",
    "        device = torch.device(self.device)\n",
    "        print(f\"\\n🖥️  Using device: {device}\")\n",
    "        if device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "        self.history = []\n",
    "        patience = 500\n",
    "        best_val_map = float('-inf')\n",
    "        best_model_state = None\n",
    "        epochs_without_improvement = 0\n",
    "    \n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "        def make_loader(X, y, shuffle):\n",
    "            mel, mfcc, q = [torch.tensor(arr, dtype=torch.float32, device=self.device) for arr in X]\n",
    "            y = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "            dataset = TensorDataset(mel, mfcc, q, y)\n",
    "            return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
    "    \n",
    "        train_loader = make_loader(X_train, y_train, shuffle=True)\n",
    "        val_loader = make_loader(X_val, y_val, shuffle=False) if X_val is not None and y_val is not None else None\n",
    "    \n",
    "        self.model.to(device)\n",
    "        start_time = time.time()\n",
    "        print(f\"🚀 Training CNN classifier (output dim={self.n_classes}) for up to {self.epochs} epochs...\")\n",
    "    \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            all_preds_train = []\n",
    "            all_targets_train = []\n",
    "    \n",
    "            for mel_b, mfcc_b, q_b, y_b in train_loader:\n",
    "                mel_b, mfcc_b, q_b, y_b = mel_b.to(device), mfcc_b.to(device), q_b.to(device), y_b.to(device)\n",
    "    \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(mel_b)  # or (mel_b, mfcc_b, q_b)\n",
    "                loss = self.criterion(outputs, y_b)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "    \n",
    "                preds = torch.sigmoid(outputs).detach().cpu()\n",
    "                all_preds_train.append(preds)\n",
    "                all_targets_train.append(y_b.detach().cpu())\n",
    "    \n",
    "            avg_train_loss = epoch_loss / num_batches\n",
    "            all_preds_train = torch.cat(all_preds_train).numpy()\n",
    "            all_targets_train = torch.cat(all_targets_train).numpy()\n",
    "    \n",
    "            try:\n",
    "                train_map = average_precision_score(all_targets_train, all_preds_train, average=\"macro\")\n",
    "            except ValueError as e:\n",
    "                train_map = float(\"nan\")\n",
    "                print(f\"⚠️  Train mAP computation failed: {e}\")\n",
    "    \n",
    "            # Validation\n",
    "            avg_val_loss = None\n",
    "            val_map = None\n",
    "    \n",
    "            if val_loader is not None:\n",
    "                self.model.eval()\n",
    "                all_preds_val, all_targets_val = [], []\n",
    "                val_loss, val_batches = 0.0, 0\n",
    "    \n",
    "                with torch.no_grad():\n",
    "                    for mel_b, mfcc_b, q_b, y_b in val_loader:\n",
    "                        mel_b, mfcc_b, q_b, y_b = mel_b.to(device), mfcc_b.to(device), q_b.to(device), y_b.to(device)\n",
    "                        outputs = self.model(mel_b)\n",
    "                        preds = torch.sigmoid(outputs)\n",
    "                        loss = self.criterion(outputs, y_b)\n",
    "    \n",
    "                        val_loss += loss.item()\n",
    "                        val_batches += 1\n",
    "                        all_preds_val.append(preds.cpu())\n",
    "                        all_targets_val.append(y_b.cpu())\n",
    "    \n",
    "                avg_val_loss = val_loss / val_batches\n",
    "                all_preds_val = torch.cat(all_preds_val).numpy()\n",
    "                all_targets_val = torch.cat(all_targets_val).numpy()\n",
    "    \n",
    "                try:\n",
    "                    val_map = average_precision_score(all_targets_val, all_preds_val, average=\"macro\")\n",
    "                except ValueError as e:\n",
    "                    val_map = float(\"nan\")\n",
    "                    print(f\"⚠️  Val mAP computation failed: {e}\")\n",
    "    \n",
    "            # Logging\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            elapsed = time.time() - start_time\n",
    "    \n",
    "            print(f\"\\n✅ Epoch {epoch}/{self.epochs} — \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "                  f\"| Train mAP: {train_map:.4f} \"\n",
    "                  f\"{f'| Val Loss: {avg_val_loss:.4f} | Val mAP: {val_map:.4f}' if val_map is not None else ''} \"\n",
    "                  f\"| LR: {current_lr:.6f} | Elapsed: {elapsed:.1f}s\")\n",
    "    \n",
    "            # History\n",
    "            self.history.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_map\": train_map,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_map\": val_map\n",
    "            })\n",
    "    \n",
    "            # LR Scheduler\n",
    "            if avg_val_loss is not None:\n",
    "                scheduler.step(avg_val_loss)\n",
    "    \n",
    "            # Early stopping\n",
    "            if val_map is not None:\n",
    "                if val_map > best_val_map:\n",
    "                    best_val_map = val_map\n",
    "                    best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                    save_model(self.model)\n",
    "                    epochs_without_improvement = 0\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "                    if epochs_without_improvement >= patience:\n",
    "                        print(f\"\\n🛑 Early stopping at epoch {epoch} (no val mAP improvement for {patience} epochs).\")\n",
    "                        break\n",
    "    \n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "        # Restore best\n",
    "        if best_model_state:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n🏁 Training complete. Best val mAP: {best_val_map:.4f}. Total time: {elapsed:.2f}s\")\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        import torch\n",
    "    \n",
    "        self.model.eval()\n",
    "        device = torch.device(self.device)\n",
    "    \n",
    "        mel, mfcc, q = [torch.tensor(arr, dtype=torch.float32, device=device) for arr in X]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            # outputs = self.model(mel, mfcc, q)\n",
    "            outputs = self.model(mel)\n",
    "\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            return (preds > 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        from sklearn.metrics import average_precision_score\n",
    "        import torch\n",
    "    \n",
    "        self.model.eval()\n",
    "        device = torch.device(self.device)\n",
    "    \n",
    "        # Unpack and convert inputs to torch tensors on CPU\n",
    "        mel, mfcc, q = [torch.tensor(arr, dtype=torch.float32) for arr in X]\n",
    "        y_true = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "    \n",
    "        preds = []\n",
    "    \n",
    "        # Predict in batches to avoid OOM\n",
    "        batch_size = self.batch_size\n",
    "        for i in range(0, len(y_true), batch_size):\n",
    "            mel_b = mel[i:i+batch_size].to(device)\n",
    "            mfcc_b = mfcc[i:i+batch_size].to(device)\n",
    "            q_b = q[i:i+batch_size].to(device)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                # outputs = self.model(mel_b, mfcc_b, q_b)\n",
    "                outputs = self.model(mel_b)\n",
    "\n",
    "                preds.append(outputs.cpu())\n",
    "    \n",
    "        y_pred = torch.cat(preds).numpy()\n",
    "        y_true = y_true.numpy()\n",
    "    \n",
    "        # Compute mean Average Precision (mAP)\n",
    "        return average_precision_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'epochs': self.epochs,\n",
    "            'lr': self.lr,\n",
    "            'batch_size': self.batch_size,\n",
    "            'device': self.device,\n",
    "            'n_classes': self.n_classes\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        self._build_model()\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:33:07.866420Z",
     "start_time": "2025-05-20T18:33:07.859322Z"
    }
   },
   "id": "6337b23713e60f01",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try to overfit on small data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8aed1bdd86d83672"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# clf = MultiOutputClassifier(SklearnCNNClassifier(device='mps', n_classes=1, epochs=10), n_jobs=None)\n",
    "# clf = SklearnCNNClassifier(n_classes=10)\n",
    "# X_subset = tuple(x[:30] for x in X_train)  # Correctly slice each of mel, mfcc, q\n",
    "# clf.overfit(X_subset, y_train[:30])\n",
    "# clf.fit(X_subset, y_train[:30])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:25:02.270563Z",
     "start_time": "2025-05-20T18:25:01.997945Z"
    }
   },
   "id": "dc754e266c0af55d",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🖥️  Using device: mps\n",
      "🚀 Training CNN classifier (output dim=10) for up to 50 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "clf = SklearnCNNClassifier(device='mps', n_classes=10, epochs=50, lr=0.001)\n",
    "clf.fit(X_train, y_train, X_val, y_val)\n",
    "# clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T18:37:40.906353Z",
     "start_time": "2025-05-20T18:33:10.007215Z"
    }
   },
   "id": "179ce493ca7ede47",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = [h[\"epoch\"] for h in history]\n",
    "    train_loss = [h[\"train_loss\"] for h in history]\n",
    "    val_loss = [h[\"val_loss\"] for h in history]\n",
    "    train_map = [h[\"train_map\"] for h in history]\n",
    "    val_map = [h[\"val_map\"] for h in history]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # mAP\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_map, label=\"Train mAP\")\n",
    "    plt.plot(epochs, val_map, label=\"Val mAP\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(\"mAP over Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-20T18:29:51.474017Z"
    }
   },
   "id": "86ac273a65bdeaab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# After training:\n",
    "plot_training_history(clf.history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-20T18:29:51.482457Z"
    }
   },
   "id": "4fae9e971bdd4af9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-20T18:29:51.482829Z"
    }
   },
   "id": "37bc97d7af0b9949",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
